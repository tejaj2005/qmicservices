{
  "version": 3,
  "sources": ["../../node_modules/minipass/src/index.ts", "../../node_modules/@isaacs/fs-minipass/src/index.ts", "../../src/options.ts", "../../src/make-command.ts", "../../node_modules/minizlib/src/constants.ts", "../../node_modules/minizlib/src/index.ts", "../../src/large-numbers.ts", "../../src/types.ts", "../../src/header.ts", "../../src/pax.ts", "../../src/normalize-windows-path.ts", "../../src/read-entry.ts", "../../src/warn-method.ts", "../../src/parse.ts", "../../src/strip-trailing-slashes.ts", "../../src/list.ts", "../../src/mode-fix.ts", "../../src/strip-absolute-path.ts", "../../src/winchars.ts", "../../src/write-entry.ts", "../../node_modules/yallist/src/index.ts", "../../src/pack.ts", "../../src/create.ts", "../../src/get-write-flag.ts", "../../node_modules/chownr/src/index.ts", "../../src/cwd-error.ts", "../../src/symlink-error.ts", "../../src/mkdir.ts", "../../src/normalize-unicode.ts", "../../src/path-reservations.ts", "../../src/process-umask.ts", "../../src/unpack.ts", "../../src/extract.ts", "../../src/replace.ts", "../../src/update.ts", "../../src/index.ts"],
  "sourcesContent": ["const proc =\n  typeof process === 'object' && process\n    ? process\n    : {\n        stdout: null,\n        stderr: null,\n      }\nimport { EventEmitter } from 'node:events'\nimport Stream from 'node:stream'\nimport { StringDecoder } from 'node:string_decoder'\n\n/**\n * Same as StringDecoder, but exposing the `lastNeed` flag on the type\n */\ntype SD = StringDecoder & { lastNeed: boolean }\n\nexport type { SD, Pipe, PipeProxyErrors }\n\n/**\n * Return true if the argument is a Minipass stream, Node stream, or something\n * else that Minipass can interact with.\n */\nexport const isStream = (\n  s: any\n): s is Minipass.Readable | Minipass.Writable =>\n  !!s &&\n  typeof s === 'object' &&\n  (s instanceof Minipass ||\n    s instanceof Stream ||\n    isReadable(s) ||\n    isWritable(s))\n\n/**\n * Return true if the argument is a valid {@link Minipass.Readable}\n */\nexport const isReadable = (s: any): s is Minipass.Readable =>\n  !!s &&\n  typeof s === 'object' &&\n  s instanceof EventEmitter &&\n  typeof (s as Minipass.Readable).pipe === 'function' &&\n  // node core Writable streams have a pipe() method, but it throws\n  (s as Minipass.Readable).pipe !== Stream.Writable.prototype.pipe\n\n/**\n * Return true if the argument is a valid {@link Minipass.Writable}\n */\nexport const isWritable = (s: any): s is Minipass.Readable =>\n  !!s &&\n  typeof s === 'object' &&\n  s instanceof EventEmitter &&\n  typeof (s as Minipass.Writable).write === 'function' &&\n  typeof (s as Minipass.Writable).end === 'function'\n\nconst EOF = Symbol('EOF')\nconst MAYBE_EMIT_END = Symbol('maybeEmitEnd')\nconst EMITTED_END = Symbol('emittedEnd')\nconst EMITTING_END = Symbol('emittingEnd')\nconst EMITTED_ERROR = Symbol('emittedError')\nconst CLOSED = Symbol('closed')\nconst READ = Symbol('read')\nconst FLUSH = Symbol('flush')\nconst FLUSHCHUNK = Symbol('flushChunk')\nconst ENCODING = Symbol('encoding')\nconst DECODER = Symbol('decoder')\nconst FLOWING = Symbol('flowing')\nconst PAUSED = Symbol('paused')\nconst RESUME = Symbol('resume')\nconst BUFFER = Symbol('buffer')\nconst PIPES = Symbol('pipes')\nconst BUFFERLENGTH = Symbol('bufferLength')\nconst BUFFERPUSH = Symbol('bufferPush')\nconst BUFFERSHIFT = Symbol('bufferShift')\nconst OBJECTMODE = Symbol('objectMode')\n// internal event when stream is destroyed\nconst DESTROYED = Symbol('destroyed')\n// internal event when stream has an error\nconst ERROR = Symbol('error')\nconst EMITDATA = Symbol('emitData')\nconst EMITEND = Symbol('emitEnd')\nconst EMITEND2 = Symbol('emitEnd2')\nconst ASYNC = Symbol('async')\nconst ABORT = Symbol('abort')\nconst ABORTED = Symbol('aborted')\nconst SIGNAL = Symbol('signal')\nconst DATALISTENERS = Symbol('dataListeners')\nconst DISCARDED = Symbol('discarded')\n\nconst defer = (fn: (...a: any[]) => any) => Promise.resolve().then(fn)\nconst nodefer = (fn: (...a: any[]) => any) => fn()\n\n// events that mean 'the stream is over'\n// these are treated specially, and re-emitted\n// if they are listened for after emitting.\ntype EndishEvent = 'end' | 'finish' | 'prefinish'\nconst isEndish = (ev: any): ev is EndishEvent =>\n  ev === 'end' || ev === 'finish' || ev === 'prefinish'\n\nconst isArrayBufferLike = (b: any): b is ArrayBufferLike =>\n  b instanceof ArrayBuffer ||\n  (!!b &&\n    typeof b === 'object' &&\n    b.constructor &&\n    b.constructor.name === 'ArrayBuffer' &&\n    b.byteLength >= 0)\n\nconst isArrayBufferView = (b: any): b is ArrayBufferView =>\n  !Buffer.isBuffer(b) && ArrayBuffer.isView(b)\n\n/**\n * Options that may be passed to stream.pipe()\n */\nexport interface PipeOptions {\n  /**\n   * end the destination stream when the source stream ends\n   */\n  end?: boolean\n  /**\n   * proxy errors from the source stream to the destination stream\n   */\n  proxyErrors?: boolean\n}\n\n/**\n * Internal class representing a pipe to a destination stream.\n *\n * @internal\n */\nclass Pipe<T extends unknown> {\n  src: Minipass<T>\n  dest: Minipass<any, T>\n  opts: PipeOptions\n  ondrain: () => any\n  constructor(\n    src: Minipass<T>,\n    dest: Minipass.Writable,\n    opts: PipeOptions\n  ) {\n    this.src = src\n    this.dest = dest as Minipass<any, T>\n    this.opts = opts\n    this.ondrain = () => src[RESUME]()\n    this.dest.on('drain', this.ondrain)\n  }\n  unpipe() {\n    this.dest.removeListener('drain', this.ondrain)\n  }\n  // only here for the prototype\n  /* c8 ignore start */\n  proxyErrors(_er: any) {}\n  /* c8 ignore stop */\n  end() {\n    this.unpipe()\n    if (this.opts.end) this.dest.end()\n  }\n}\n\n/**\n * Internal class representing a pipe to a destination stream where\n * errors are proxied.\n *\n * @internal\n */\nclass PipeProxyErrors<T> extends Pipe<T> {\n  unpipe() {\n    this.src.removeListener('error', this.proxyErrors)\n    super.unpipe()\n  }\n  constructor(\n    src: Minipass<T>,\n    dest: Minipass.Writable,\n    opts: PipeOptions\n  ) {\n    super(src, dest, opts)\n    this.proxyErrors = er => dest.emit('error', er)\n    src.on('error', this.proxyErrors)\n  }\n}\n\nexport namespace Minipass {\n  /**\n   * Encoding used to create a stream that outputs strings rather than\n   * Buffer objects.\n   */\n  export type Encoding = BufferEncoding | 'buffer' | null\n\n  /**\n   * Any stream that Minipass can pipe into\n   */\n  export type Writable =\n    | Minipass<any, any, any>\n    | NodeJS.WriteStream\n    | (NodeJS.WriteStream & { fd: number })\n    | (EventEmitter & {\n        end(): any\n        write(chunk: any, ...args: any[]): any\n      })\n\n  /**\n   * Any stream that can be read from\n   */\n  export type Readable =\n    | Minipass<any, any, any>\n    | NodeJS.ReadStream\n    | (NodeJS.ReadStream & { fd: number })\n    | (EventEmitter & {\n        pause(): any\n        resume(): any\n        pipe(...destArgs: any[]): any\n      })\n\n  /**\n   * Utility type that can be iterated sync or async\n   */\n  export type DualIterable<T> = Iterable<T> & AsyncIterable<T>\n\n  type EventArguments = Record<string | symbol, unknown[]>\n\n  /**\n   * The listing of events that a Minipass class can emit.\n   * Extend this when extending the Minipass class, and pass as\n   * the third template argument.  The key is the name of the event,\n   * and the value is the argument list.\n   *\n   * Any undeclared events will still be allowed, but the handler will get\n   * arguments as `unknown[]`.\n   */\n  export interface Events<RType extends any = Buffer>\n    extends EventArguments {\n    readable: []\n    data: [chunk: RType]\n    error: [er: unknown]\n    abort: [reason: unknown]\n    drain: []\n    resume: []\n    end: []\n    finish: []\n    prefinish: []\n    close: []\n    [DESTROYED]: [er?: unknown]\n    [ERROR]: [er: unknown]\n  }\n\n  /**\n   * String or buffer-like data that can be joined and sliced\n   */\n  export type ContiguousData =\n    | Buffer\n    | ArrayBufferLike\n    | ArrayBufferView\n    | string\n  export type BufferOrString = Buffer | string\n\n  /**\n   * Options passed to the Minipass constructor.\n   */\n  export type SharedOptions = {\n    /**\n     * Defer all data emission and other events until the end of the\n     * current tick, similar to Node core streams\n     */\n    async?: boolean\n    /**\n     * A signal which will abort the stream\n     */\n    signal?: AbortSignal\n    /**\n     * Output string encoding. Set to `null` or `'buffer'` (or omit) to\n     * emit Buffer objects rather than strings.\n     *\n     * Conflicts with `objectMode`\n     */\n    encoding?: BufferEncoding | null | 'buffer'\n    /**\n     * Output data exactly as it was written, supporting non-buffer/string\n     * data (such as arbitrary objects, falsey values, etc.)\n     *\n     * Conflicts with `encoding`\n     */\n    objectMode?: boolean\n  }\n\n  /**\n   * Options for a string encoded output\n   */\n  export type EncodingOptions = SharedOptions & {\n    encoding: BufferEncoding\n    objectMode?: false\n  }\n\n  /**\n   * Options for contiguous data buffer output\n   */\n  export type BufferOptions = SharedOptions & {\n    encoding?: null | 'buffer'\n    objectMode?: false\n  }\n\n  /**\n   * Options for objectMode arbitrary output\n   */\n  export type ObjectModeOptions = SharedOptions & {\n    objectMode: true\n    encoding?: null\n  }\n\n  /**\n   * Utility type to determine allowed options based on read type\n   */\n  export type Options<T> =\n    | ObjectModeOptions\n    | (T extends string\n        ? EncodingOptions\n        : T extends Buffer\n        ? BufferOptions\n        : SharedOptions)\n}\n\nconst isObjectModeOptions = (\n  o: Minipass.SharedOptions\n): o is Minipass.ObjectModeOptions => !!o.objectMode\n\nconst isEncodingOptions = (\n  o: Minipass.SharedOptions\n): o is Minipass.EncodingOptions =>\n  !o.objectMode && !!o.encoding && o.encoding !== 'buffer'\n\n/**\n * Main export, the Minipass class\n *\n * `RType` is the type of data emitted, defaults to Buffer\n *\n * `WType` is the type of data to be written, if RType is buffer or string,\n * then any {@link Minipass.ContiguousData} is allowed.\n *\n * `Events` is the set of event handler signatures that this object\n * will emit, see {@link Minipass.Events}\n */\nexport class Minipass<\n    RType extends unknown = Buffer,\n    WType extends unknown = RType extends Minipass.BufferOrString\n      ? Minipass.ContiguousData\n      : RType,\n    Events extends Minipass.Events<RType> = Minipass.Events<RType>\n  >\n  extends EventEmitter\n  implements Minipass.DualIterable<RType>\n{\n  [FLOWING]: boolean = false;\n  [PAUSED]: boolean = false;\n  [PIPES]: Pipe<RType>[] = [];\n  [BUFFER]: RType[] = [];\n  [OBJECTMODE]: boolean;\n  [ENCODING]: BufferEncoding | null;\n  [ASYNC]: boolean;\n  [DECODER]: SD | null;\n  [EOF]: boolean = false;\n  [EMITTED_END]: boolean = false;\n  [EMITTING_END]: boolean = false;\n  [CLOSED]: boolean = false;\n  [EMITTED_ERROR]: unknown = null;\n  [BUFFERLENGTH]: number = 0;\n  [DESTROYED]: boolean = false;\n  [SIGNAL]?: AbortSignal;\n  [ABORTED]: boolean = false;\n  [DATALISTENERS]: number = 0;\n  [DISCARDED]: boolean = false\n\n  /**\n   * true if the stream can be written\n   */\n  writable: boolean = true\n  /**\n   * true if the stream can be read\n   */\n  readable: boolean = true\n\n  /**\n   * If `RType` is Buffer, then options do not need to be provided.\n   * Otherwise, an options object must be provided to specify either\n   * {@link Minipass.SharedOptions.objectMode} or\n   * {@link Minipass.SharedOptions.encoding}, as appropriate.\n   */\n  constructor(\n    ...args:\n      | [Minipass.ObjectModeOptions]\n      | (RType extends Buffer\n          ? [] | [Minipass.Options<RType>]\n          : [Minipass.Options<RType>])\n  ) {\n    const options: Minipass.Options<RType> = (args[0] ||\n      {}) as Minipass.Options<RType>\n    super()\n    if (options.objectMode && typeof options.encoding === 'string') {\n      throw new TypeError(\n        'Encoding and objectMode may not be used together'\n      )\n    }\n    if (isObjectModeOptions(options)) {\n      this[OBJECTMODE] = true\n      this[ENCODING] = null\n    } else if (isEncodingOptions(options)) {\n      this[ENCODING] = options.encoding\n      this[OBJECTMODE] = false\n    } else {\n      this[OBJECTMODE] = false\n      this[ENCODING] = null\n    }\n    this[ASYNC] = !!options.async\n    this[DECODER] = this[ENCODING]\n      ? (new StringDecoder(this[ENCODING]) as SD)\n      : null\n\n    //@ts-ignore - private option for debugging and testing\n    if (options && options.debugExposeBuffer === true) {\n      Object.defineProperty(this, 'buffer', { get: () => this[BUFFER] })\n    }\n    //@ts-ignore - private option for debugging and testing\n    if (options && options.debugExposePipes === true) {\n      Object.defineProperty(this, 'pipes', { get: () => this[PIPES] })\n    }\n\n    const { signal } = options\n    if (signal) {\n      this[SIGNAL] = signal\n      if (signal.aborted) {\n        this[ABORT]()\n      } else {\n        signal.addEventListener('abort', () => this[ABORT]())\n      }\n    }\n  }\n\n  /**\n   * The amount of data stored in the buffer waiting to be read.\n   *\n   * For Buffer strings, this will be the total byte length.\n   * For string encoding streams, this will be the string character length,\n   * according to JavaScript's `string.length` logic.\n   * For objectMode streams, this is a count of the items waiting to be\n   * emitted.\n   */\n  get bufferLength() {\n    return this[BUFFERLENGTH]\n  }\n\n  /**\n   * The `BufferEncoding` currently in use, or `null`\n   */\n  get encoding() {\n    return this[ENCODING]\n  }\n\n  /**\n   * @deprecated - This is a read only property\n   */\n  set encoding(_enc) {\n    throw new Error('Encoding must be set at instantiation time')\n  }\n\n  /**\n   * @deprecated - Encoding may only be set at instantiation time\n   */\n  setEncoding(_enc: Minipass.Encoding) {\n    throw new Error('Encoding must be set at instantiation time')\n  }\n\n  /**\n   * True if this is an objectMode stream\n   */\n  get objectMode() {\n    return this[OBJECTMODE]\n  }\n\n  /**\n   * @deprecated - This is a read-only property\n   */\n  set objectMode(_om) {\n    throw new Error('objectMode must be set at instantiation time')\n  }\n\n  /**\n   * true if this is an async stream\n   */\n  get ['async'](): boolean {\n    return this[ASYNC]\n  }\n  /**\n   * Set to true to make this stream async.\n   *\n   * Once set, it cannot be unset, as this would potentially cause incorrect\n   * behavior.  Ie, a sync stream can be made async, but an async stream\n   * cannot be safely made sync.\n   */\n  set ['async'](a: boolean) {\n    this[ASYNC] = this[ASYNC] || !!a\n  }\n\n  // drop everything and get out of the flow completely\n  [ABORT]() {\n    this[ABORTED] = true\n    this.emit('abort', this[SIGNAL]?.reason)\n    this.destroy(this[SIGNAL]?.reason)\n  }\n\n  /**\n   * True if the stream has been aborted.\n   */\n  get aborted() {\n    return this[ABORTED]\n  }\n  /**\n   * No-op setter. Stream aborted status is set via the AbortSignal provided\n   * in the constructor options.\n   */\n  set aborted(_) {}\n\n  /**\n   * Write data into the stream\n   *\n   * If the chunk written is a string, and encoding is not specified, then\n   * `utf8` will be assumed. If the stream encoding matches the encoding of\n   * a written string, and the state of the string decoder allows it, then\n   * the string will be passed through to either the output or the internal\n   * buffer without any processing. Otherwise, it will be turned into a\n   * Buffer object for processing into the desired encoding.\n   *\n   * If provided, `cb` function is called immediately before return for\n   * sync streams, or on next tick for async streams, because for this\n   * base class, a chunk is considered \"processed\" once it is accepted\n   * and either emitted or buffered. That is, the callback does not indicate\n   * that the chunk has been eventually emitted, though of course child\n   * classes can override this function to do whatever processing is required\n   * and call `super.write(...)` only once processing is completed.\n   */\n  write(chunk: WType, cb?: () => void): boolean\n  write(\n    chunk: WType,\n    encoding?: Minipass.Encoding,\n    cb?: () => void\n  ): boolean\n  write(\n    chunk: WType,\n    encoding?: Minipass.Encoding | (() => void),\n    cb?: () => void\n  ): boolean {\n    if (this[ABORTED]) return false\n    if (this[EOF]) throw new Error('write after end')\n\n    if (this[DESTROYED]) {\n      this.emit(\n        'error',\n        Object.assign(\n          new Error('Cannot call write after a stream was destroyed'),\n          { code: 'ERR_STREAM_DESTROYED' }\n        )\n      )\n      return true\n    }\n\n    if (typeof encoding === 'function') {\n      cb = encoding\n      encoding = 'utf8'\n    }\n\n    if (!encoding) encoding = 'utf8'\n\n    const fn = this[ASYNC] ? defer : nodefer\n\n    // convert array buffers and typed array views into buffers\n    // at some point in the future, we may want to do the opposite!\n    // leave strings and buffers as-is\n    // anything is only allowed if in object mode, so throw\n    if (!this[OBJECTMODE] && !Buffer.isBuffer(chunk)) {\n      if (isArrayBufferView(chunk)) {\n        //@ts-ignore - sinful unsafe type changing\n        chunk = Buffer.from(\n          chunk.buffer,\n          chunk.byteOffset,\n          chunk.byteLength\n        )\n      } else if (isArrayBufferLike(chunk)) {\n        //@ts-ignore - sinful unsafe type changing\n        chunk = Buffer.from(chunk)\n      } else if (typeof chunk !== 'string') {\n        throw new Error(\n          'Non-contiguous data written to non-objectMode stream'\n        )\n      }\n    }\n\n    // handle object mode up front, since it's simpler\n    // this yields better performance, fewer checks later.\n    if (this[OBJECTMODE]) {\n      // maybe impossible?\n      /* c8 ignore start */\n      if (this[FLOWING] && this[BUFFERLENGTH] !== 0) this[FLUSH](true)\n      /* c8 ignore stop */\n\n      if (this[FLOWING]) this.emit('data', chunk as unknown as RType)\n      else this[BUFFERPUSH](chunk as unknown as RType)\n\n      if (this[BUFFERLENGTH] !== 0) this.emit('readable')\n\n      if (cb) fn(cb)\n\n      return this[FLOWING]\n    }\n\n    // at this point the chunk is a buffer or string\n    // don't buffer it up or send it to the decoder\n    if (!(chunk as Minipass.BufferOrString).length) {\n      if (this[BUFFERLENGTH] !== 0) this.emit('readable')\n      if (cb) fn(cb)\n      return this[FLOWING]\n    }\n\n    // fast-path writing strings of same encoding to a stream with\n    // an empty buffer, skipping the buffer/decoder dance\n    if (\n      typeof chunk === 'string' &&\n      // unless it is a string already ready for us to use\n      !(encoding === this[ENCODING] && !this[DECODER]?.lastNeed)\n    ) {\n      //@ts-ignore - sinful unsafe type change\n      chunk = Buffer.from(chunk, encoding)\n    }\n\n    if (Buffer.isBuffer(chunk) && this[ENCODING]) {\n      //@ts-ignore - sinful unsafe type change\n      chunk = this[DECODER].write(chunk)\n    }\n\n    // Note: flushing CAN potentially switch us into not-flowing mode\n    if (this[FLOWING] && this[BUFFERLENGTH] !== 0) this[FLUSH](true)\n\n    if (this[FLOWING]) this.emit('data', chunk as unknown as RType)\n    else this[BUFFERPUSH](chunk as unknown as RType)\n\n    if (this[BUFFERLENGTH] !== 0) this.emit('readable')\n\n    if (cb) fn(cb)\n\n    return this[FLOWING]\n  }\n\n  /**\n   * Low-level explicit read method.\n   *\n   * In objectMode, the argument is ignored, and one item is returned if\n   * available.\n   *\n   * `n` is the number of bytes (or in the case of encoding streams,\n   * characters) to consume. If `n` is not provided, then the entire buffer\n   * is returned, or `null` is returned if no data is available.\n   *\n   * If `n` is greater that the amount of data in the internal buffer,\n   * then `null` is returned.\n   */\n  read(n?: number | null): RType | null {\n    if (this[DESTROYED]) return null\n    this[DISCARDED] = false\n\n    if (\n      this[BUFFERLENGTH] === 0 ||\n      n === 0 ||\n      (n && n > this[BUFFERLENGTH])\n    ) {\n      this[MAYBE_EMIT_END]()\n      return null\n    }\n\n    if (this[OBJECTMODE]) n = null\n\n    if (this[BUFFER].length > 1 && !this[OBJECTMODE]) {\n      // not object mode, so if we have an encoding, then RType is string\n      // otherwise, must be Buffer\n      this[BUFFER] = [\n        (this[ENCODING]\n          ? this[BUFFER].join('')\n          : Buffer.concat(\n              this[BUFFER] as Buffer[],\n              this[BUFFERLENGTH]\n            )) as RType,\n      ]\n    }\n\n    const ret = this[READ](n || null, this[BUFFER][0] as RType)\n    this[MAYBE_EMIT_END]()\n    return ret\n  }\n\n  [READ](n: number | null, chunk: RType) {\n    if (this[OBJECTMODE]) this[BUFFERSHIFT]()\n    else {\n      const c = chunk as Minipass.BufferOrString\n      if (n === c.length || n === null) this[BUFFERSHIFT]()\n      else if (typeof c === 'string') {\n        this[BUFFER][0] = c.slice(n) as RType\n        chunk = c.slice(0, n) as RType\n        this[BUFFERLENGTH] -= n\n      } else {\n        this[BUFFER][0] = c.subarray(n) as RType\n        chunk = c.subarray(0, n) as RType\n        this[BUFFERLENGTH] -= n\n      }\n    }\n\n    this.emit('data', chunk)\n\n    if (!this[BUFFER].length && !this[EOF]) this.emit('drain')\n\n    return chunk\n  }\n\n  /**\n   * End the stream, optionally providing a final write.\n   *\n   * See {@link Minipass#write} for argument descriptions\n   */\n  end(cb?: () => void): this\n  end(chunk: WType, cb?: () => void): this\n  end(chunk: WType, encoding?: Minipass.Encoding, cb?: () => void): this\n  end(\n    chunk?: WType | (() => void),\n    encoding?: Minipass.Encoding | (() => void),\n    cb?: () => void\n  ): this {\n    if (typeof chunk === 'function') {\n      cb = chunk as () => void\n      chunk = undefined\n    }\n    if (typeof encoding === 'function') {\n      cb = encoding\n      encoding = 'utf8'\n    }\n    if (chunk !== undefined) this.write(chunk, encoding)\n    if (cb) this.once('end', cb)\n    this[EOF] = true\n    this.writable = false\n\n    // if we haven't written anything, then go ahead and emit,\n    // even if we're not reading.\n    // we'll re-emit if a new 'end' listener is added anyway.\n    // This makes MP more suitable to write-only use cases.\n    if (this[FLOWING] || !this[PAUSED]) this[MAYBE_EMIT_END]()\n    return this\n  }\n\n  // don't let the internal resume be overwritten\n  [RESUME]() {\n    if (this[DESTROYED]) return\n\n    if (!this[DATALISTENERS] && !this[PIPES].length) {\n      this[DISCARDED] = true\n    }\n    this[PAUSED] = false\n    this[FLOWING] = true\n    this.emit('resume')\n    if (this[BUFFER].length) this[FLUSH]()\n    else if (this[EOF]) this[MAYBE_EMIT_END]()\n    else this.emit('drain')\n  }\n\n  /**\n   * Resume the stream if it is currently in a paused state\n   *\n   * If called when there are no pipe destinations or `data` event listeners,\n   * this will place the stream in a \"discarded\" state, where all data will\n   * be thrown away. The discarded state is removed if a pipe destination or\n   * data handler is added, if pause() is called, or if any synchronous or\n   * asynchronous iteration is started.\n   */\n  resume() {\n    return this[RESUME]()\n  }\n\n  /**\n   * Pause the stream\n   */\n  pause() {\n    this[FLOWING] = false\n    this[PAUSED] = true\n    this[DISCARDED] = false\n  }\n\n  /**\n   * true if the stream has been forcibly destroyed\n   */\n  get destroyed() {\n    return this[DESTROYED]\n  }\n\n  /**\n   * true if the stream is currently in a flowing state, meaning that\n   * any writes will be immediately emitted.\n   */\n  get flowing() {\n    return this[FLOWING]\n  }\n\n  /**\n   * true if the stream is currently in a paused state\n   */\n  get paused() {\n    return this[PAUSED]\n  }\n\n  [BUFFERPUSH](chunk: RType) {\n    if (this[OBJECTMODE]) this[BUFFERLENGTH] += 1\n    else this[BUFFERLENGTH] += (chunk as Minipass.BufferOrString).length\n    this[BUFFER].push(chunk)\n  }\n\n  [BUFFERSHIFT](): RType {\n    if (this[OBJECTMODE]) this[BUFFERLENGTH] -= 1\n    else\n      this[BUFFERLENGTH] -= (\n        this[BUFFER][0] as Minipass.BufferOrString\n      ).length\n    return this[BUFFER].shift() as RType\n  }\n\n  [FLUSH](noDrain: boolean = false) {\n    do {} while (\n      this[FLUSHCHUNK](this[BUFFERSHIFT]()) &&\n      this[BUFFER].length\n    )\n\n    if (!noDrain && !this[BUFFER].length && !this[EOF]) this.emit('drain')\n  }\n\n  [FLUSHCHUNK](chunk: RType) {\n    this.emit('data', chunk)\n    return this[FLOWING]\n  }\n\n  /**\n   * Pipe all data emitted by this stream into the destination provided.\n   *\n   * Triggers the flow of data.\n   */\n  pipe<W extends Minipass.Writable>(dest: W, opts?: PipeOptions): W {\n    if (this[DESTROYED]) return dest\n    this[DISCARDED] = false\n\n    const ended = this[EMITTED_END]\n    opts = opts || {}\n    if (dest === proc.stdout || dest === proc.stderr) opts.end = false\n    else opts.end = opts.end !== false\n    opts.proxyErrors = !!opts.proxyErrors\n\n    // piping an ended stream ends immediately\n    if (ended) {\n      if (opts.end) dest.end()\n    } else {\n      // \"as\" here just ignores the WType, which pipes don't care about,\n      // since they're only consuming from us, and writing to the dest\n      this[PIPES].push(\n        !opts.proxyErrors\n          ? new Pipe<RType>(this as Minipass<RType>, dest, opts)\n          : new PipeProxyErrors<RType>(this as Minipass<RType>, dest, opts)\n      )\n      if (this[ASYNC]) defer(() => this[RESUME]())\n      else this[RESUME]()\n    }\n\n    return dest\n  }\n\n  /**\n   * Fully unhook a piped destination stream.\n   *\n   * If the destination stream was the only consumer of this stream (ie,\n   * there are no other piped destinations or `'data'` event listeners)\n   * then the flow of data will stop until there is another consumer or\n   * {@link Minipass#resume} is explicitly called.\n   */\n  unpipe<W extends Minipass.Writable>(dest: W) {\n    const p = this[PIPES].find(p => p.dest === dest)\n    if (p) {\n      if (this[PIPES].length === 1) {\n        if (this[FLOWING] && this[DATALISTENERS] === 0) {\n          this[FLOWING] = false\n        }\n        this[PIPES] = []\n      } else this[PIPES].splice(this[PIPES].indexOf(p), 1)\n      p.unpipe()\n    }\n  }\n\n  /**\n   * Alias for {@link Minipass#on}\n   */\n  addListener<Event extends keyof Events>(\n    ev: Event,\n    handler: (...args: Events[Event]) => any\n  ): this {\n    return this.on(ev, handler)\n  }\n\n  /**\n   * Mostly identical to `EventEmitter.on`, with the following\n   * behavior differences to prevent data loss and unnecessary hangs:\n   *\n   * - Adding a 'data' event handler will trigger the flow of data\n   *\n   * - Adding a 'readable' event handler when there is data waiting to be read\n   *   will cause 'readable' to be emitted immediately.\n   *\n   * - Adding an 'endish' event handler ('end', 'finish', etc.) which has\n   *   already passed will cause the event to be emitted immediately and all\n   *   handlers removed.\n   *\n   * - Adding an 'error' event handler after an error has been emitted will\n   *   cause the event to be re-emitted immediately with the error previously\n   *   raised.\n   */\n  on<Event extends keyof Events>(\n    ev: Event,\n    handler: (...args: Events[Event]) => any\n  ): this {\n    const ret = super.on(\n      ev as string | symbol,\n      handler as (...a: any[]) => any\n    )\n    if (ev === 'data') {\n      this[DISCARDED] = false\n      this[DATALISTENERS]++\n      if (!this[PIPES].length && !this[FLOWING]) {\n        this[RESUME]()\n      }\n    } else if (ev === 'readable' && this[BUFFERLENGTH] !== 0) {\n      super.emit('readable')\n    } else if (isEndish(ev) && this[EMITTED_END]) {\n      super.emit(ev)\n      this.removeAllListeners(ev)\n    } else if (ev === 'error' && this[EMITTED_ERROR]) {\n      const h = handler as (...a: Events['error']) => any\n      if (this[ASYNC]) defer(() => h.call(this, this[EMITTED_ERROR]))\n      else h.call(this, this[EMITTED_ERROR])\n    }\n    return ret\n  }\n\n  /**\n   * Alias for {@link Minipass#off}\n   */\n  removeListener<Event extends keyof Events>(\n    ev: Event,\n    handler: (...args: Events[Event]) => any\n  ) {\n    return this.off(ev, handler)\n  }\n\n  /**\n   * Mostly identical to `EventEmitter.off`\n   *\n   * If a 'data' event handler is removed, and it was the last consumer\n   * (ie, there are no pipe destinations or other 'data' event listeners),\n   * then the flow of data will stop until there is another consumer or\n   * {@link Minipass#resume} is explicitly called.\n   */\n  off<Event extends keyof Events>(\n    ev: Event,\n    handler: (...args: Events[Event]) => any\n  ) {\n    const ret = super.off(\n      ev as string | symbol,\n      handler as (...a: any[]) => any\n    )\n    // if we previously had listeners, and now we don't, and we don't\n    // have any pipes, then stop the flow, unless it's been explicitly\n    // put in a discarded flowing state via stream.resume().\n    if (ev === 'data') {\n      this[DATALISTENERS] = this.listeners('data').length\n      if (\n        this[DATALISTENERS] === 0 &&\n        !this[DISCARDED] &&\n        !this[PIPES].length\n      ) {\n        this[FLOWING] = false\n      }\n    }\n    return ret\n  }\n\n  /**\n   * Mostly identical to `EventEmitter.removeAllListeners`\n   *\n   * If all 'data' event handlers are removed, and they were the last consumer\n   * (ie, there are no pipe destinations), then the flow of data will stop\n   * until there is another consumer or {@link Minipass#resume} is explicitly\n   * called.\n   */\n  removeAllListeners<Event extends keyof Events>(ev?: Event) {\n    const ret = super.removeAllListeners(ev as string | symbol | undefined)\n    if (ev === 'data' || ev === undefined) {\n      this[DATALISTENERS] = 0\n      if (!this[DISCARDED] && !this[PIPES].length) {\n        this[FLOWING] = false\n      }\n    }\n    return ret\n  }\n\n  /**\n   * true if the 'end' event has been emitted\n   */\n  get emittedEnd() {\n    return this[EMITTED_END]\n  }\n\n  [MAYBE_EMIT_END]() {\n    if (\n      !this[EMITTING_END] &&\n      !this[EMITTED_END] &&\n      !this[DESTROYED] &&\n      this[BUFFER].length === 0 &&\n      this[EOF]\n    ) {\n      this[EMITTING_END] = true\n      this.emit('end')\n      this.emit('prefinish')\n      this.emit('finish')\n      if (this[CLOSED]) this.emit('close')\n      this[EMITTING_END] = false\n    }\n  }\n\n  /**\n   * Mostly identical to `EventEmitter.emit`, with the following\n   * behavior differences to prevent data loss and unnecessary hangs:\n   *\n   * If the stream has been destroyed, and the event is something other\n   * than 'close' or 'error', then `false` is returned and no handlers\n   * are called.\n   *\n   * If the event is 'end', and has already been emitted, then the event\n   * is ignored. If the stream is in a paused or non-flowing state, then\n   * the event will be deferred until data flow resumes. If the stream is\n   * async, then handlers will be called on the next tick rather than\n   * immediately.\n   *\n   * If the event is 'close', and 'end' has not yet been emitted, then\n   * the event will be deferred until after 'end' is emitted.\n   *\n   * If the event is 'error', and an AbortSignal was provided for the stream,\n   * and there are no listeners, then the event is ignored, matching the\n   * behavior of node core streams in the presense of an AbortSignal.\n   *\n   * If the event is 'finish' or 'prefinish', then all listeners will be\n   * removed after emitting the event, to prevent double-firing.\n   */\n  emit<Event extends keyof Events>(\n    ev: Event,\n    ...args: Events[Event]\n  ): boolean {\n    const data = args[0]\n    // error and close are only events allowed after calling destroy()\n    if (\n      ev !== 'error' &&\n      ev !== 'close' &&\n      ev !== DESTROYED &&\n      this[DESTROYED]\n    ) {\n      return false\n    } else if (ev === 'data') {\n      return !this[OBJECTMODE] && !data\n        ? false\n        : this[ASYNC]\n        ? (defer(() => this[EMITDATA](data as RType)), true)\n        : this[EMITDATA](data as RType)\n    } else if (ev === 'end') {\n      return this[EMITEND]()\n    } else if (ev === 'close') {\n      this[CLOSED] = true\n      // don't emit close before 'end' and 'finish'\n      if (!this[EMITTED_END] && !this[DESTROYED]) return false\n      const ret = super.emit('close')\n      this.removeAllListeners('close')\n      return ret\n    } else if (ev === 'error') {\n      this[EMITTED_ERROR] = data\n      super.emit(ERROR, data)\n      const ret =\n        !this[SIGNAL] || this.listeners('error').length\n          ? super.emit('error', data)\n          : false\n      this[MAYBE_EMIT_END]()\n      return ret\n    } else if (ev === 'resume') {\n      const ret = super.emit('resume')\n      this[MAYBE_EMIT_END]()\n      return ret\n    } else if (ev === 'finish' || ev === 'prefinish') {\n      const ret = super.emit(ev)\n      this.removeAllListeners(ev)\n      return ret\n    }\n\n    // Some other unknown event\n    const ret = super.emit(ev as string, ...args)\n    this[MAYBE_EMIT_END]()\n    return ret\n  }\n\n  [EMITDATA](data: RType) {\n    for (const p of this[PIPES]) {\n      if (p.dest.write(data as RType) === false) this.pause()\n    }\n    const ret = this[DISCARDED] ? false : super.emit('data', data)\n    this[MAYBE_EMIT_END]()\n    return ret\n  }\n\n  [EMITEND]() {\n    if (this[EMITTED_END]) return false\n\n    this[EMITTED_END] = true\n    this.readable = false\n    return this[ASYNC]\n      ? (defer(() => this[EMITEND2]()), true)\n      : this[EMITEND2]()\n  }\n\n  [EMITEND2]() {\n    if (this[DECODER]) {\n      const data = this[DECODER].end()\n      if (data) {\n        for (const p of this[PIPES]) {\n          p.dest.write(data as RType)\n        }\n        if (!this[DISCARDED]) super.emit('data', data)\n      }\n    }\n\n    for (const p of this[PIPES]) {\n      p.end()\n    }\n    const ret = super.emit('end')\n    this.removeAllListeners('end')\n    return ret\n  }\n\n  /**\n   * Return a Promise that resolves to an array of all emitted data once\n   * the stream ends.\n   */\n  async collect(): Promise<RType[] & { dataLength: number }> {\n    const buf: RType[] & { dataLength: number } = Object.assign([], {\n      dataLength: 0,\n    })\n    if (!this[OBJECTMODE]) buf.dataLength = 0\n    // set the promise first, in case an error is raised\n    // by triggering the flow here.\n    const p = this.promise()\n    this.on('data', c => {\n      buf.push(c)\n      if (!this[OBJECTMODE])\n        buf.dataLength += (c as Minipass.BufferOrString).length\n    })\n    await p\n    return buf\n  }\n\n  /**\n   * Return a Promise that resolves to the concatenation of all emitted data\n   * once the stream ends.\n   *\n   * Not allowed on objectMode streams.\n   */\n  async concat(): Promise<RType> {\n    if (this[OBJECTMODE]) {\n      throw new Error('cannot concat in objectMode')\n    }\n    const buf = await this.collect()\n    return (\n      this[ENCODING]\n        ? buf.join('')\n        : Buffer.concat(buf as Buffer[], buf.dataLength)\n    ) as RType\n  }\n\n  /**\n   * Return a void Promise that resolves once the stream ends.\n   */\n  async promise(): Promise<void> {\n    return new Promise<void>((resolve, reject) => {\n      this.on(DESTROYED, () => reject(new Error('stream destroyed')))\n      this.on('error', er => reject(er))\n      this.on('end', () => resolve())\n    })\n  }\n\n  /**\n   * Asynchronous `for await of` iteration.\n   *\n   * This will continue emitting all chunks until the stream terminates.\n   */\n  [Symbol.asyncIterator](): AsyncGenerator<RType, void, void> {\n    // set this up front, in case the consumer doesn't call next()\n    // right away.\n    this[DISCARDED] = false\n    let stopped = false\n    const stop = async (): Promise<IteratorReturnResult<void>> => {\n      this.pause()\n      stopped = true\n      return { value: undefined, done: true }\n    }\n    const next = (): Promise<IteratorResult<RType, void>> => {\n      if (stopped) return stop()\n      const res = this.read()\n      if (res !== null) return Promise.resolve({ done: false, value: res })\n\n      if (this[EOF]) return stop()\n\n      let resolve!: (res: IteratorResult<RType>) => void\n      let reject!: (er: unknown) => void\n      const onerr = (er: unknown) => {\n        this.off('data', ondata)\n        this.off('end', onend)\n        this.off(DESTROYED, ondestroy)\n        stop()\n        reject(er)\n      }\n      const ondata = (value: RType) => {\n        this.off('error', onerr)\n        this.off('end', onend)\n        this.off(DESTROYED, ondestroy)\n        this.pause()\n        resolve({ value, done: !!this[EOF] })\n      }\n      const onend = () => {\n        this.off('error', onerr)\n        this.off('data', ondata)\n        this.off(DESTROYED, ondestroy)\n        stop()\n        resolve({ done: true, value: undefined })\n      }\n      const ondestroy = () => onerr(new Error('stream destroyed'))\n      return new Promise<IteratorResult<RType>>((res, rej) => {\n        reject = rej\n        resolve = res\n        this.once(DESTROYED, ondestroy)\n        this.once('error', onerr)\n        this.once('end', onend)\n        this.once('data', ondata)\n      })\n    }\n\n    return {\n      next,\n      throw: stop,\n      return: stop,\n      [Symbol.asyncIterator]() {\n        return this\n      },\n    }\n  }\n\n  /**\n   * Synchronous `for of` iteration.\n   *\n   * The iteration will terminate when the internal buffer runs out, even\n   * if the stream has not yet terminated.\n   */\n  [Symbol.iterator](): Generator<RType, void, void> {\n    // set this up front, in case the consumer doesn't call next()\n    // right away.\n    this[DISCARDED] = false\n    let stopped = false\n    const stop = (): IteratorReturnResult<void> => {\n      this.pause()\n      this.off(ERROR, stop)\n      this.off(DESTROYED, stop)\n      this.off('end', stop)\n      stopped = true\n      return { done: true, value: undefined }\n    }\n\n    const next = (): IteratorResult<RType, void> => {\n      if (stopped) return stop()\n      const value = this.read()\n      return value === null ? stop() : { done: false, value }\n    }\n\n    this.once('end', stop)\n    this.once(ERROR, stop)\n    this.once(DESTROYED, stop)\n\n    return {\n      next,\n      throw: stop,\n      return: stop,\n      [Symbol.iterator]() {\n        return this\n      },\n    }\n  }\n\n  /**\n   * Destroy a stream, preventing it from being used for any further purpose.\n   *\n   * If the stream has a `close()` method, then it will be called on\n   * destruction.\n   *\n   * After destruction, any attempt to write data, read data, or emit most\n   * events will be ignored.\n   *\n   * If an error argument is provided, then it will be emitted in an\n   * 'error' event.\n   */\n  destroy(er?: unknown) {\n    if (this[DESTROYED]) {\n      if (er) this.emit('error', er)\n      else this.emit(DESTROYED)\n      return this\n    }\n\n    this[DESTROYED] = true\n    this[DISCARDED] = true\n\n    // throw away all buffered data, it's never coming out\n    this[BUFFER].length = 0\n    this[BUFFERLENGTH] = 0\n\n    const wc = this as Minipass<RType, WType, Events> & {\n      close?: () => void\n    }\n    if (typeof wc.close === 'function' && !this[CLOSED]) wc.close()\n\n    if (er) this.emit('error', er)\n    // if no error to emit, still reject pending promises\n    else this.emit(DESTROYED)\n\n    return this\n  }\n\n  /**\n   * Alias for {@link isStream}\n   *\n   * Former export location, maintained for backwards compatibility.\n   *\n   * @deprecated\n   */\n  static get isStream() {\n    return isStream\n  }\n}\n", "import EE from 'events'\nimport fs from 'fs'\nimport { Minipass } from 'minipass'\n\nconst writev = fs.writev\n\nconst _autoClose = Symbol('_autoClose')\nconst _close = Symbol('_close')\nconst _ended = Symbol('_ended')\nconst _fd = Symbol('_fd')\nconst _finished = Symbol('_finished')\nconst _flags = Symbol('_flags')\nconst _flush = Symbol('_flush')\nconst _handleChunk = Symbol('_handleChunk')\nconst _makeBuf = Symbol('_makeBuf')\nconst _mode = Symbol('_mode')\nconst _needDrain = Symbol('_needDrain')\nconst _onerror = Symbol('_onerror')\nconst _onopen = Symbol('_onopen')\nconst _onread = Symbol('_onread')\nconst _onwrite = Symbol('_onwrite')\nconst _open = Symbol('_open')\nconst _path = Symbol('_path')\nconst _pos = Symbol('_pos')\nconst _queue = Symbol('_queue')\nconst _read = Symbol('_read')\nconst _readSize = Symbol('_readSize')\nconst _reading = Symbol('_reading')\nconst _remain = Symbol('_remain')\nconst _size = Symbol('_size')\nconst _write = Symbol('_write')\nconst _writing = Symbol('_writing')\nconst _defaultFlag = Symbol('_defaultFlag')\nconst _errored = Symbol('_errored')\n\nexport type ReadStreamOptions =\n  Minipass.Options<Minipass.ContiguousData> & {\n    fd?: number\n    readSize?: number\n    size?: number\n    autoClose?: boolean\n  }\n\nexport type ReadStreamEvents = Minipass.Events<Minipass.ContiguousData> & {\n  open: [fd: number]\n}\n\nexport class ReadStream extends Minipass<\n  Minipass.ContiguousData,\n  Buffer,\n  ReadStreamEvents\n> {\n  [_errored]: boolean = false;\n  [_fd]?: number;\n  [_path]: string;\n  [_readSize]: number;\n  [_reading]: boolean = false;\n  [_size]: number;\n  [_remain]: number;\n  [_autoClose]: boolean\n\n  constructor(path: string, opt: ReadStreamOptions) {\n    opt = opt || {}\n    super(opt)\n\n    this.readable = true\n    this.writable = false\n\n    if (typeof path !== 'string') {\n      throw new TypeError('path must be a string')\n    }\n\n    this[_errored] = false\n    this[_fd] = typeof opt.fd === 'number' ? opt.fd : undefined\n    this[_path] = path\n    this[_readSize] = opt.readSize || 16 * 1024 * 1024\n    this[_reading] = false\n    this[_size] = typeof opt.size === 'number' ? opt.size : Infinity\n    this[_remain] = this[_size]\n    this[_autoClose] =\n      typeof opt.autoClose === 'boolean' ? opt.autoClose : true\n\n    if (typeof this[_fd] === 'number') {\n      this[_read]()\n    } else {\n      this[_open]()\n    }\n  }\n\n  get fd() {\n    return this[_fd]\n  }\n\n  get path() {\n    return this[_path]\n  }\n\n  //@ts-ignore\n  write() {\n    throw new TypeError('this is a readable stream')\n  }\n\n  //@ts-ignore\n  end() {\n    throw new TypeError('this is a readable stream')\n  }\n\n  [_open]() {\n    fs.open(this[_path], 'r', (er, fd) => this[_onopen](er, fd))\n  }\n\n  [_onopen](er?: NodeJS.ErrnoException | null, fd?: number) {\n    if (er) {\n      this[_onerror](er)\n    } else {\n      this[_fd] = fd\n      this.emit('open', fd as number)\n      this[_read]()\n    }\n  }\n\n  [_makeBuf]() {\n    return Buffer.allocUnsafe(Math.min(this[_readSize], this[_remain]))\n  }\n\n  [_read]() {\n    if (!this[_reading]) {\n      this[_reading] = true\n      const buf = this[_makeBuf]()\n      /* c8 ignore start */\n      if (buf.length === 0) {\n        return process.nextTick(() => this[_onread](null, 0, buf))\n      }\n      /* c8 ignore stop */\n      fs.read(this[_fd] as number, buf, 0, buf.length, null, (er, br, b) =>\n        this[_onread](er, br, b),\n      )\n    }\n  }\n\n  [_onread](er?: NodeJS.ErrnoException | null, br?: number, buf?: Buffer) {\n    this[_reading] = false\n    if (er) {\n      this[_onerror](er)\n    } else if (this[_handleChunk](br as number, buf as Buffer)) {\n      this[_read]()\n    }\n  }\n\n  [_close]() {\n    if (this[_autoClose] && typeof this[_fd] === 'number') {\n      const fd = this[_fd]\n      this[_fd] = undefined\n      fs.close(fd, er =>\n        er ? this.emit('error', er) : this.emit('close'),\n      )\n    }\n  }\n\n  [_onerror](er: NodeJS.ErrnoException) {\n    this[_reading] = true\n    this[_close]()\n    this.emit('error', er)\n  }\n\n  [_handleChunk](br: number, buf: Buffer) {\n    let ret = false\n    // no effect if infinite\n    this[_remain] -= br\n    if (br > 0) {\n      ret = super.write(br < buf.length ? buf.subarray(0, br) : buf)\n    }\n\n    if (br === 0 || this[_remain] <= 0) {\n      ret = false\n      this[_close]()\n      super.end()\n    }\n\n    return ret\n  }\n\n  emit<Event extends keyof ReadStreamEvents>(\n    ev: Event,\n    ...args: ReadStreamEvents[Event]\n  ): boolean {\n    switch (ev) {\n      case 'prefinish':\n      case 'finish':\n        return false\n\n      case 'drain':\n        if (typeof this[_fd] === 'number') {\n          this[_read]()\n        }\n        return false\n\n      case 'error':\n        if (this[_errored]) {\n          return false\n        }\n        this[_errored] = true\n        return super.emit(ev, ...args)\n\n      default:\n        return super.emit(ev, ...args)\n    }\n  }\n}\n\nexport class ReadStreamSync extends ReadStream {\n  [_open]() {\n    let threw = true\n    try {\n      this[_onopen](null, fs.openSync(this[_path], 'r'))\n      threw = false\n    } finally {\n      if (threw) {\n        this[_close]()\n      }\n    }\n  }\n\n  [_read]() {\n    let threw = true\n    try {\n      if (!this[_reading]) {\n        this[_reading] = true\n        do {\n          const buf = this[_makeBuf]()\n          /* c8 ignore start */\n          const br =\n            buf.length === 0\n              ? 0\n              : fs.readSync(this[_fd] as number, buf, 0, buf.length, null)\n          /* c8 ignore stop */\n          if (!this[_handleChunk](br, buf)) {\n            break\n          }\n        } while (true)\n        this[_reading] = false\n      }\n      threw = false\n    } finally {\n      if (threw) {\n        this[_close]()\n      }\n    }\n  }\n\n  [_close]() {\n    if (this[_autoClose] && typeof this[_fd] === 'number') {\n      const fd = this[_fd]\n      this[_fd] = undefined\n      fs.closeSync(fd)\n      this.emit('close')\n    }\n  }\n}\n\nexport type WriteStreamOptions = {\n  fd?: number\n  autoClose?: boolean\n  mode?: number\n  captureRejections?: boolean\n  start?: number\n  flags?: string\n}\n\nexport class WriteStream extends EE {\n  readable: false = false\n  writable: boolean = true;\n  [_errored]: boolean = false;\n  [_writing]: boolean = false;\n  [_ended]: boolean = false;\n  [_queue]: Buffer[] = [];\n  [_needDrain]: boolean = false;\n  [_path]: string;\n  [_mode]: number;\n  [_autoClose]: boolean;\n  [_fd]?: number;\n  [_defaultFlag]: boolean;\n  [_flags]: string;\n  [_finished]: boolean = false;\n  [_pos]?: number\n\n  constructor(path: string, opt: WriteStreamOptions) {\n    opt = opt || {}\n    super(opt)\n    this[_path] = path\n    this[_fd] = typeof opt.fd === 'number' ? opt.fd : undefined\n    this[_mode] = opt.mode === undefined ? 0o666 : opt.mode\n    this[_pos] = typeof opt.start === 'number' ? opt.start : undefined\n    this[_autoClose] =\n      typeof opt.autoClose === 'boolean' ? opt.autoClose : true\n\n    // truncating makes no sense when writing into the middle\n    const defaultFlag = this[_pos] !== undefined ? 'r+' : 'w'\n    this[_defaultFlag] = opt.flags === undefined\n    this[_flags] = opt.flags === undefined ? defaultFlag : opt.flags\n\n    if (this[_fd] === undefined) {\n      this[_open]()\n    }\n  }\n\n  emit(ev: string, ...args: any[]) {\n    if (ev === 'error') {\n      if (this[_errored]) {\n        return false\n      }\n      this[_errored] = true\n    }\n    return super.emit(ev, ...args)\n  }\n\n  get fd() {\n    return this[_fd]\n  }\n\n  get path() {\n    return this[_path]\n  }\n\n  [_onerror](er: NodeJS.ErrnoException) {\n    this[_close]()\n    this[_writing] = true\n    this.emit('error', er)\n  }\n\n  [_open]() {\n    fs.open(this[_path], this[_flags], this[_mode], (er, fd) =>\n      this[_onopen](er, fd),\n    )\n  }\n\n  [_onopen](er?: null | NodeJS.ErrnoException, fd?: number) {\n    if (\n      this[_defaultFlag] &&\n      this[_flags] === 'r+' &&\n      er &&\n      er.code === 'ENOENT'\n    ) {\n      this[_flags] = 'w'\n      this[_open]()\n    } else if (er) {\n      this[_onerror](er)\n    } else {\n      this[_fd] = fd\n      this.emit('open', fd)\n      if (!this[_writing]) {\n        this[_flush]()\n      }\n    }\n  }\n\n  end(buf: string, enc?: BufferEncoding): this\n  end(buf?: Buffer, enc?: undefined): this\n  end(buf?: Buffer | string, enc?: BufferEncoding): this {\n    if (buf) {\n      //@ts-ignore\n      this.write(buf, enc)\n    }\n\n    this[_ended] = true\n\n    // synthetic after-write logic, where drain/finish live\n    if (\n      !this[_writing] &&\n      !this[_queue].length &&\n      typeof this[_fd] === 'number'\n    ) {\n      this[_onwrite](null, 0)\n    }\n    return this\n  }\n\n  write(buf: string, enc?: BufferEncoding): boolean\n  write(buf: Buffer, enc?: undefined): boolean\n  write(buf: Buffer | string, enc?: BufferEncoding): boolean {\n    if (typeof buf === 'string') {\n      buf = Buffer.from(buf, enc)\n    }\n\n    if (this[_ended]) {\n      this.emit('error', new Error('write() after end()'))\n      return false\n    }\n\n    if (this[_fd] === undefined || this[_writing] || this[_queue].length) {\n      this[_queue].push(buf)\n      this[_needDrain] = true\n      return false\n    }\n\n    this[_writing] = true\n    this[_write](buf)\n    return true\n  }\n\n  [_write](buf: Buffer) {\n    fs.write(\n      this[_fd] as number,\n      buf,\n      0,\n      buf.length,\n      this[_pos],\n      (er, bw) => this[_onwrite](er, bw),\n    )\n  }\n\n  [_onwrite](er?: null | NodeJS.ErrnoException, bw?: number) {\n    if (er) {\n      this[_onerror](er)\n    } else {\n      if (this[_pos] !== undefined && typeof bw === 'number') {\n        this[_pos] += bw\n      }\n      if (this[_queue].length) {\n        this[_flush]()\n      } else {\n        this[_writing] = false\n\n        if (this[_ended] && !this[_finished]) {\n          this[_finished] = true\n          this[_close]()\n          this.emit('finish')\n        } else if (this[_needDrain]) {\n          this[_needDrain] = false\n          this.emit('drain')\n        }\n      }\n    }\n  }\n\n  [_flush]() {\n    if (this[_queue].length === 0) {\n      if (this[_ended]) {\n        this[_onwrite](null, 0)\n      }\n    } else if (this[_queue].length === 1) {\n      this[_write](this[_queue].pop() as Buffer)\n    } else {\n      const iovec = this[_queue]\n      this[_queue] = []\n      writev(this[_fd] as number, iovec, this[_pos] as number, (er, bw) =>\n        this[_onwrite](er, bw),\n      )\n    }\n  }\n\n  [_close]() {\n    if (this[_autoClose] && typeof this[_fd] === 'number') {\n      const fd = this[_fd]\n      this[_fd] = undefined\n      fs.close(fd, er =>\n        er ? this.emit('error', er) : this.emit('close'),\n      )\n    }\n  }\n}\n\nexport class WriteStreamSync extends WriteStream {\n  [_open](): void {\n    let fd\n    // only wrap in a try{} block if we know we'll retry, to avoid\n    // the rethrow obscuring the error's source frame in most cases.\n    if (this[_defaultFlag] && this[_flags] === 'r+') {\n      try {\n        fd = fs.openSync(this[_path], this[_flags], this[_mode])\n      } catch (er) {\n        if ((er as NodeJS.ErrnoException)?.code === 'ENOENT') {\n          this[_flags] = 'w'\n          return this[_open]()\n        } else {\n          throw er\n        }\n      }\n    } else {\n      fd = fs.openSync(this[_path], this[_flags], this[_mode])\n    }\n\n    this[_onopen](null, fd)\n  }\n\n  [_close]() {\n    if (this[_autoClose] && typeof this[_fd] === 'number') {\n      const fd = this[_fd]\n      this[_fd] = undefined\n      fs.closeSync(fd)\n      this.emit('close')\n    }\n  }\n\n  [_write](buf: Buffer) {\n    // throw the original, but try to close if it fails\n    let threw = true\n    try {\n      this[_onwrite](\n        null,\n        fs.writeSync(this[_fd] as number, buf, 0, buf.length, this[_pos]),\n      )\n      threw = false\n    } finally {\n      if (threw) {\n        try {\n          this[_close]()\n        } catch {\n          // ok error\n        }\n      }\n    }\n  }\n}\n", "// turn tar(1) style args like `C` into the more verbose things like `cwd`\n\nimport { type GzipOptions, type ZlibOptions } from 'minizlib'\nimport { type Stats } from 'node:fs'\nimport { type ReadEntry } from './read-entry.js'\nimport { type WarnData } from './warn-method.js'\nimport { WriteEntry } from './write-entry.js'\n\nconst argmap = new Map<keyof TarOptionsWithAliases, keyof TarOptions>(\n  [\n    ['C', 'cwd'],\n    ['f', 'file'],\n    ['z', 'gzip'],\n    ['P', 'preservePaths'],\n    ['U', 'unlink'],\n    ['strip-components', 'strip'],\n    ['stripComponents', 'strip'],\n    ['keep-newer', 'newer'],\n    ['keepNewer', 'newer'],\n    ['keep-newer-files', 'newer'],\n    ['keepNewerFiles', 'newer'],\n    ['k', 'keep'],\n    ['keep-existing', 'keep'],\n    ['keepExisting', 'keep'],\n    ['m', 'noMtime'],\n    ['no-mtime', 'noMtime'],\n    ['p', 'preserveOwner'],\n    ['L', 'follow'],\n    ['h', 'follow'],\n    ['onentry', 'onReadEntry'],\n  ],\n)\n\n/**\n * The options that can be provided to tar commands.\n *\n * Note that some of these are only relevant for certain commands, since\n * they are specific to reading or writing.\n *\n * Aliases are provided in the {@link TarOptionsWithAliases} type.\n */\nexport interface TarOptions {\n  //////////////////////////\n  // shared options\n\n  /**\n   * Perform all I/O operations synchronously. If the stream is ended\n   * immediately, then it will be processed entirely synchronously.\n   */\n  sync?: boolean\n\n  /**\n   * The tar file to be read and/or written. When this is set, a stream\n   * is not returned. Asynchronous commands will return a promise indicating\n   * when the operation is completed, and synchronous commands will return\n   * immediately.\n   */\n  file?: string\n\n  /**\n   * Treat warnings as crash-worthy errors. Defaults false.\n   */\n  strict?: boolean\n\n  /**\n   * The effective current working directory for this tar command\n   */\n  cwd?: string\n\n  /**\n   * When creating a tar archive, this can be used to compress it as well.\n   * Set to `true` to use the default gzip options, or customize them as\n   * needed.\n   *\n   * When reading, if this is unset, then the compression status will be\n   * inferred from the archive data. This is generally best, unless you are\n   * sure of the compression settings in use to create the archive, and want to\n   * fail if the archive doesn't match expectations.\n   */\n  gzip?: boolean | GzipOptions\n\n  /**\n   * When creating archives, preserve absolute and `..` paths in the archive,\n   * rather than sanitizing them under the cwd.\n   *\n   * When extracting, allow absolute paths, paths containing `..`, and\n   * extracting through symbolic links. By default, the root `/` is stripped\n   * from absolute paths (eg, turning `/x/y/z` into `x/y/z`), paths containing\n   * `..` are not extracted, and any file whose location would be modified by a\n   * symbolic link is not extracted.\n   *\n   * **WARNING** This is almost always unsafe, and must NEVER be used on\n   * archives from untrusted sources, such as user input, and every entry must\n   * be validated to ensure it is safe to write. Even if the input is not\n   * malicious, mistakes can cause a lot of damage!\n   */\n  preservePaths?: boolean\n\n  /**\n   * When extracting, do not set the `mtime` value for extracted entries to\n   * match the `mtime` in the archive.\n   *\n   * When creating archives, do not store the `mtime` value in the entry. Note\n   * that this prevents properly using other mtime-based features (such as\n   * `tar.update` or the `newer` option) with the resulting archive.\n   */\n  noMtime?: boolean\n\n  /**\n   * Set to `true` or an object with settings for `zlib.BrotliCompress()` to\n   * create a brotli-compressed archive\n   *\n   * When extracting, this will cause the archive to be treated as a\n   * brotli-compressed file if set to `true` or a ZlibOptions object.\n   *\n   * If set `false`, then brotli options will not be used.\n   *\n   * If this, the `gzip`, and `zstd` options are left `undefined`, then tar\n   * will attempt to infer the brotli compression status, but can only do so\n   * based on the filename. If the filename ends in `.tbr` or `.tar.br`, and\n   * the first 512 bytes are not a valid tar header, then brotli decompression\n   * will be attempted.\n   */\n  brotli?: boolean | ZlibOptions\n\n  /**\n   * Set to `true` or an object with settings for `zstd.compress()` to\n   * create a zstd-compressed archive\n   *\n   * When extracting, this will cause the archive to be treated as a\n   * zstd-compressed file if set to `true` or a ZlibOptions object.\n   *\n   * If set `false`, then zstd options will not be used.\n   *\n   * If this, the `gzip`, and `brotli` options are left `undefined`, then tar\n   * will attempt to infer the zstd compression status, but can only do so\n   * based on the filename. If the filename ends in `.tzst` or `.tar.zst`, and\n   * the first 512 bytes are not a valid tar header, then zstd decompression\n   * will be attempted.\n   */\n  zstd?: boolean | ZlibOptions\n\n  /**\n   * A function that is called with `(path, stat)` when creating an archive, or\n   * `(path, entry)` when extracting. Return true to process the file/entry, or\n   * false to exclude it.\n   */\n  filter?: (path: string, entry: Stats | ReadEntry) => boolean\n\n  /**\n   * A function that gets called for any warning encountered.\n   *\n   * Note: if `strict` is set, then the warning will throw, and this method\n   * will not be called.\n   */\n  onwarn?: (code: string, message: string, data: WarnData) => any\n\n  //////////////////////////\n  // extraction options\n\n  /**\n   * When extracting, unlink files before creating them. Without this option,\n   * tar overwrites existing files, which preserves existing hardlinks. With\n   * this option, existing hardlinks will be broken, as will any symlink that\n   * would affect the location of an extracted file.\n   */\n  unlink?: boolean\n\n  /**\n   * When extracting, strip the specified number of path portions from the\n   * entry path. For example, with `{strip: 2}`, the entry `a/b/c/d` would be\n   * extracted to `{cwd}/c/d`.\n   *\n   * Any entry whose entire path is stripped will be excluded.\n   */\n  strip?: number\n\n  /**\n   * When extracting, keep the existing file on disk if it's newer than the\n   * file in the archive.\n   */\n  newer?: boolean\n\n  /**\n   * When extracting, do not overwrite existing files at all.\n   */\n  keep?: boolean\n\n  /**\n   * When extracting, set the `uid` and `gid` of extracted entries to the `uid`\n   * and `gid` fields in the archive. Defaults to true when run as root, and\n   * false otherwise.\n   *\n   * If false, then files and directories will be set with the owner and group\n   * of the user running the process. This is similar to `-p` in `tar(1)`, but\n   * ACLs and other system-specific data is never unpacked in this\n   * implementation, and modes are set by default already.\n   */\n  preserveOwner?: boolean\n\n  /**\n   * The maximum depth of subfolders to extract into. This defaults to 1024.\n   * Anything deeper than the limit will raise a warning and skip the entry.\n   * Set to `Infinity` to remove the limitation.\n   */\n  maxDepth?: number\n\n  /**\n   * When extracting, force all created files and directories, and all\n   * implicitly created directories, to be owned by the specified user id,\n   * regardless of the `uid` field in the archive.\n   *\n   * Cannot be used along with `preserveOwner`. Requires also setting the `gid`\n   * option.\n   */\n  uid?: number\n\n  /**\n   * When extracting, force all created files and directories, and all\n   * implicitly created directories, to be owned by the specified group id,\n   * regardless of the `gid` field in the archive.\n   *\n   * Cannot be used along with `preserveOwner`. Requires also setting the `uid`\n   * option.\n   */\n  gid?: number\n\n  /**\n   * When extracting, provide a function that takes an `entry` object, and\n   * returns a stream, or any falsey value. If a stream is provided, then that\n   * stream's data will be written instead of the contents of the archive\n   * entry. If a falsey value is provided, then the entry is written to disk as\n   * normal.\n   *\n   * To exclude items from extraction, use the `filter` option.\n   *\n   * Note that using an asynchronous stream type with the `transform` option\n   * will cause undefined behavior in synchronous extractions.\n   * [MiniPass](http://npm.im/minipass)-based streams are designed for this use\n   * case.\n   */\n  transform?: (entry: ReadEntry) => any\n\n  /**\n   * Call `chmod()` to ensure that extracted files match the entry's mode\n   * field. Without this field set, all mode fields in archive entries are a\n   * best effort attempt only.\n   *\n   * Setting this necessitates a call to the deprecated `process.umask()`\n   * method to determine the default umask value, unless a `processUmask`\n   * config is provided as well.\n   *\n   * If not set, tar will attempt to create file system entries with whatever\n   * mode is provided, and let the implicit process `umask` apply normally, but\n   * if a file already exists to be written to, then its existing mode will not\n   * be modified.\n   *\n   * When setting `chmod: true`, it is highly recommend to set the\n   * {@link TarOptions#processUmask} option as well, to avoid the call to the\n   * deprecated (and thread-unsafe) `process.umask()` method.\n   */\n  chmod?: boolean\n\n  /**\n   * When setting the {@link TarOptions#chmod} option to `true`, you may\n   * provide a value here to avoid having to call the deprecated and\n   * thread-unsafe `process.umask()` method.\n   *\n   * This has no effect with `chmod` is not set to true, as mode values are not\n   * set explicitly anyway. If `chmod` is set to `true`, and a value is not\n   * provided here, then `process.umask()` must be called, which will result in\n   * deprecation warnings.\n   *\n   * The most common values for this are `0o22` (resulting in directories\n   * created with mode `0o755` and files with `0o644` by default) and `0o2`\n   * (resulting in directores created with mode `0o775` and files `0o664`, so\n   * they are group-writable).\n   */\n  processUmask?: number\n\n  //////////////////////////\n  // archive creation options\n\n  /**\n   * When parsing/listing archives, `entry` streams are by default resumed\n   * (set into \"flowing\" mode) immediately after the call to `onReadEntry()`.\n   * Set `noResume: true` to suppress this behavior.\n   *\n   * Note that when this is set, the stream will never complete until the\n   * data is consumed somehow.\n   *\n   * Set automatically in extract operations, since the entry is piped to\n   * a file system entry right away. Only relevant when parsing.\n   */\n  noResume?: boolean\n\n  /**\n   * When creating, updating, or replacing within archives, this method will\n   * be called with each WriteEntry that is created.\n   */\n  onWriteEntry?: (entry: WriteEntry) => any\n\n  /**\n   * When extracting or listing archives, this method will be called with\n   * each entry that is not excluded by a `filter`.\n   *\n   * Important when listing archives synchronously from a file, because there\n   * is otherwise no way to interact with the data!\n   */\n  onReadEntry?: (entry: ReadEntry) => any\n\n  /**\n   * Pack the targets of symbolic links rather than the link itself.\n   */\n  follow?: boolean\n\n  /**\n   * When creating archives, omit any metadata that is system-specific:\n   * `ctime`, `atime`, `uid`, `gid`, `uname`, `gname`, `dev`, `ino`, and\n   * `nlink`. Note that `mtime` is still included, because this is necessary\n   * for other time-based operations such as `tar.update`. Additionally, `mode`\n   * is set to a \"reasonable default\" for mose unix systems, based on an\n   * effective `umask` of `0o22`.\n   *\n   * This also defaults the `portable` option in the gzip configs when creating\n   * a compressed archive, in order to produce deterministic archives that are\n   * not operating-system specific.\n   */\n  portable?: boolean\n\n  /**\n   * When creating archives, do not recursively archive the contents of\n   * directories. By default, archiving a directory archives all of its\n   * contents as well.\n   */\n  noDirRecurse?: boolean\n\n  /**\n   * Suppress Pax extended headers when creating archives. Note that this means\n   * long paths and linkpaths will be truncated, and large or negative numeric\n   * values may be interpreted incorrectly.\n   */\n  noPax?: boolean\n\n  /**\n   * Set to a `Date` object to force a specific `mtime` value for everything\n   * written to an archive.\n   *\n   * This is useful when creating archives that are intended to be\n   * deterministic based on their contents, irrespective of the file's last\n   * modification time.\n   *\n   * Overridden by `noMtime`.\n   */\n  mtime?: Date\n\n  /**\n   * A path portion to prefix onto the entries added to an archive.\n   */\n  prefix?: string\n\n  /**\n   * The mode to set on any created file archive, defaults to 0o666\n   * masked by the process umask, often resulting in 0o644.\n   *\n   * This does *not* affect the mode fields of individual entries, or the\n   * mode status of extracted entries on the filesystem.\n   */\n  mode?: number\n\n  //////////////////////////\n  // internal options\n\n  /**\n   * A cache of mtime values, to avoid having to stat the same file repeatedly.\n   *\n   * @internal\n   */\n  mtimeCache?: Map<string, Date>\n\n  /**\n   * maximum buffer size for `fs.read()` operations.\n   *\n   * @internal\n   */\n  maxReadSize?: number\n\n  /**\n   * Filter modes of entries being unpacked, like `process.umask()`\n   *\n   * @internal\n   */\n  umask?: number\n\n  /**\n   * Default mode for directories. Used for all implicitly created directories,\n   * and any directories in the archive that do not have a mode field.\n   *\n   * @internal\n   */\n  dmode?: number\n\n  /**\n   * default mode for files\n   *\n   * @internal\n   */\n  fmode?: number\n\n  /**\n   * Map that tracks which directories already exist, for extraction\n   *\n   * @internal\n   */\n  dirCache?: Map<string, boolean>\n  /**\n   * maximum supported size of meta entries. Defaults to 1MB\n   *\n   * @internal\n   */\n  maxMetaEntrySize?: number\n\n  /**\n   * A Map object containing the device and inode value for any file whose\n   * `nlink` value is greater than 1, to identify hard links when creating\n   * archives.\n   *\n   * @internal\n   */\n  linkCache?: Map<LinkCacheKey, string>\n\n  /**\n   * A map object containing the results of `fs.readdir()` calls.\n   *\n   * @internal\n   */\n  readdirCache?: Map<string, string[]>\n\n  /**\n   * A cache of all `lstat` results, for use in creating archives.\n   *\n   * @internal\n   */\n  statCache?: Map<string, Stats>\n\n  /**\n   * Number of concurrent jobs to run when creating archives.\n   *\n   * Defaults to 4.\n   *\n   * @internal\n   */\n  jobs?: number\n\n  /**\n   * Automatically set to true on Windows systems.\n   *\n   * When extracting, causes behavior where filenames containing `<|>?:`\n   * characters are converted to windows-compatible escape sequences in the\n   * created filesystem entries.\n   *\n   * When packing, causes behavior where paths replace `\\` with `/`, and\n   * filenames containing the windows-compatible escaped forms of `<|>?:` are\n   * converted to actual `<|>?:` characters in the archive.\n   *\n   * @internal\n   */\n  win32?: boolean\n\n  /**\n   * For `WriteEntry` objects, the absolute path to the entry on the\n   * filesystem. By default, this is `resolve(cwd, entry.path)`, but it can be\n   * overridden explicitly.\n   *\n   * @internal\n   */\n  absolute?: string\n\n  /**\n   * Used with Parser stream interface, to attach and take over when the\n   * stream is completely parsed. If this is set, then the prefinish,\n   * finish, and end events will not fire, and are the responsibility of\n   * the ondone method to emit properly.\n   *\n   * @internal\n   */\n  ondone?: () => void\n\n  /**\n   * Mostly for testing, but potentially useful in some cases.\n   * Forcibly trigger a chown on every entry, no matter what.\n   */\n  forceChown?: boolean\n\n  /**\n   * ambiguous deprecated name for {@link onReadEntry}\n   *\n   * @deprecated\n   */\n  onentry?: (entry: ReadEntry) => any\n}\n\nexport type TarOptionsSync = TarOptions & { sync: true }\nexport type TarOptionsAsync = TarOptions & { sync?: false }\nexport type TarOptionsFile = TarOptions & { file: string }\nexport type TarOptionsNoFile = TarOptions & { file?: undefined }\nexport type TarOptionsSyncFile = TarOptionsSync & TarOptionsFile\nexport type TarOptionsAsyncFile = TarOptionsAsync & TarOptionsFile\nexport type TarOptionsSyncNoFile = TarOptionsSync & TarOptionsNoFile\nexport type TarOptionsAsyncNoFile = TarOptionsAsync & TarOptionsNoFile\n\nexport type LinkCacheKey = `${number}:${number}`\n\nexport interface TarOptionsWithAliases extends TarOptions {\n  /**\n   * The effective current working directory for this tar command\n   */\n  C?: TarOptions['cwd']\n  /**\n   * The tar file to be read and/or written. When this is set, a stream\n   * is not returned. Asynchronous commands will return a promise indicating\n   * when the operation is completed, and synchronous commands will return\n   * immediately.\n   */\n  f?: TarOptions['file']\n  /**\n   * When creating a tar archive, this can be used to compress it as well.\n   * Set to `true` to use the default gzip options, or customize them as\n   * needed.\n   *\n   * When reading, if this is unset, then the compression status will be\n   * inferred from the archive data. This is generally best, unless you are\n   * sure of the compression settings in use to create the archive, and want to\n   * fail if the archive doesn't match expectations.\n   */\n  z?: TarOptions['gzip']\n  /**\n   * When creating archives, preserve absolute and `..` paths in the archive,\n   * rather than sanitizing them under the cwd.\n   *\n   * When extracting, allow absolute paths, paths containing `..`, and\n   * extracting through symbolic links. By default, the root `/` is stripped\n   * from absolute paths (eg, turning `/x/y/z` into `x/y/z`), paths containing\n   * `..` are not extracted, and any file whose location would be modified by a\n   * symbolic link is not extracted.\n   *\n   * **WARNING** This is almost always unsafe, and must NEVER be used on\n   * archives from untrusted sources, such as user input, and every entry must\n   * be validated to ensure it is safe to write. Even if the input is not\n   * malicious, mistakes can cause a lot of damage!\n   */\n  P?: TarOptions['preservePaths']\n  /**\n   * When extracting, unlink files before creating them. Without this option,\n   * tar overwrites existing files, which preserves existing hardlinks. With\n   * this option, existing hardlinks will be broken, as will any symlink that\n   * would affect the location of an extracted file.\n   */\n  U?: TarOptions['unlink']\n  /**\n   * When extracting, strip the specified number of path portions from the\n   * entry path. For example, with `{strip: 2}`, the entry `a/b/c/d` would be\n   * extracted to `{cwd}/c/d`.\n   */\n  'strip-components'?: TarOptions['strip']\n  /**\n   * When extracting, strip the specified number of path portions from the\n   * entry path. For example, with `{strip: 2}`, the entry `a/b/c/d` would be\n   * extracted to `{cwd}/c/d`.\n   */\n  stripComponents?: TarOptions['strip']\n  /**\n   * When extracting, keep the existing file on disk if it's newer than the\n   * file in the archive.\n   */\n  'keep-newer'?: TarOptions['newer']\n  /**\n   * When extracting, keep the existing file on disk if it's newer than the\n   * file in the archive.\n   */\n  keepNewer?: TarOptions['newer']\n  /**\n   * When extracting, keep the existing file on disk if it's newer than the\n   * file in the archive.\n   */\n  'keep-newer-files'?: TarOptions['newer']\n  /**\n   * When extracting, keep the existing file on disk if it's newer than the\n   * file in the archive.\n   */\n  keepNewerFiles?: TarOptions['newer']\n  /**\n   * When extracting, do not overwrite existing files at all.\n   */\n  k?: TarOptions['keep']\n  /**\n   * When extracting, do not overwrite existing files at all.\n   */\n  'keep-existing'?: TarOptions['keep']\n  /**\n   * When extracting, do not overwrite existing files at all.\n   */\n  keepExisting?: TarOptions['keep']\n  /**\n   * When extracting, do not set the `mtime` value for extracted entries to\n   * match the `mtime` in the archive.\n   *\n   * When creating archives, do not store the `mtime` value in the entry. Note\n   * that this prevents properly using other mtime-based features (such as\n   * `tar.update` or the `newer` option) with the resulting archive.\n   */\n  m?: TarOptions['noMtime']\n  /**\n   * When extracting, do not set the `mtime` value for extracted entries to\n   * match the `mtime` in the archive.\n   *\n   * When creating archives, do not store the `mtime` value in the entry. Note\n   * that this prevents properly using other mtime-based features (such as\n   * `tar.update` or the `newer` option) with the resulting archive.\n   */\n  'no-mtime'?: TarOptions['noMtime']\n  /**\n   * When extracting, set the `uid` and `gid` of extracted entries to the `uid`\n   * and `gid` fields in the archive. Defaults to true when run as root, and\n   * false otherwise.\n   *\n   * If false, then files and directories will be set with the owner and group\n   * of the user running the process. This is similar to `-p` in `tar(1)`, but\n   * ACLs and other system-specific data is never unpacked in this\n   * implementation, and modes are set by default already.\n   */\n  p?: TarOptions['preserveOwner']\n  /**\n   * Pack the targets of symbolic links rather than the link itself.\n   */\n  L?: TarOptions['follow']\n  /**\n   * Pack the targets of symbolic links rather than the link itself.\n   */\n  h?: TarOptions['follow']\n\n  /**\n   * Deprecated option. Set explicitly false to set `chmod: true`. Ignored\n   * if {@link TarOptions#chmod} is set to any boolean value.\n   *\n   * @deprecated\n   */\n  noChmod?: boolean\n}\n\nexport type TarOptionsWithAliasesSync = TarOptionsWithAliases & {\n  sync: true\n}\nexport type TarOptionsWithAliasesAsync = TarOptionsWithAliases & {\n  sync?: false\n}\nexport type TarOptionsWithAliasesFile =\n  | (TarOptionsWithAliases & {\n      file: string\n    })\n  | (TarOptionsWithAliases & { f: string })\nexport type TarOptionsWithAliasesSyncFile =\n  TarOptionsWithAliasesSync & TarOptionsWithAliasesFile\nexport type TarOptionsWithAliasesAsyncFile =\n  TarOptionsWithAliasesAsync & TarOptionsWithAliasesFile\n\nexport type TarOptionsWithAliasesNoFile = TarOptionsWithAliases & {\n  f?: undefined\n  file?: undefined\n}\n\nexport type TarOptionsWithAliasesSyncNoFile =\n  TarOptionsWithAliasesSync & TarOptionsWithAliasesNoFile\nexport type TarOptionsWithAliasesAsyncNoFile =\n  TarOptionsWithAliasesAsync & TarOptionsWithAliasesNoFile\n\nexport const isSyncFile = <O extends TarOptions>(\n  o: O,\n): o is O & TarOptionsSyncFile => !!o.sync && !!o.file\nexport const isAsyncFile = <O extends TarOptions>(\n  o: O,\n): o is O & TarOptionsAsyncFile => !o.sync && !!o.file\nexport const isSyncNoFile = <O extends TarOptions>(\n  o: O,\n): o is O & TarOptionsSyncNoFile => !!o.sync && !o.file\nexport const isAsyncNoFile = <O extends TarOptions>(\n  o: O,\n): o is O & TarOptionsAsyncNoFile => !o.sync && !o.file\nexport const isSync = <O extends TarOptions>(\n  o: O,\n): o is O & TarOptionsSync => !!o.sync\nexport const isAsync = <O extends TarOptions>(\n  o: O,\n): o is O & TarOptionsAsync => !o.sync\nexport const isFile = <O extends TarOptions>(\n  o: O,\n): o is O & TarOptionsFile => !!o.file\nexport const isNoFile = <O extends TarOptions>(\n  o: O,\n): o is O & TarOptionsNoFile => !o.file\n\nconst dealiasKey = (\n  k: keyof TarOptionsWithAliases,\n): keyof TarOptions => {\n  const d = argmap.get(k)\n  if (d) return d\n  return k as keyof TarOptions\n}\n\nexport const dealias = (\n  opt: TarOptionsWithAliases = {},\n): TarOptions => {\n  if (!opt) return {}\n  const result: Record<string, any> = {}\n  for (const [key, v] of Object.entries(opt) as [\n    keyof TarOptionsWithAliases,\n    any,\n  ][]) {\n    // TS doesn't know that aliases are going to always be the same type\n    const k = dealiasKey(key)\n    result[k] = v\n  }\n  // affordance for deprecated noChmod -> chmod\n  if (result.chmod === undefined && result.noChmod === false) {\n    result.chmod = true\n  }\n  delete result.noChmod\n  return result as TarOptions\n}\n", "import {\n  dealias,\n  isAsyncFile,\n  isAsyncNoFile,\n  isSyncFile,\n  isSyncNoFile,\n  TarOptions,\n  TarOptionsAsyncFile,\n  TarOptionsAsyncNoFile,\n  TarOptionsSyncFile,\n  TarOptionsSyncNoFile,\n  TarOptionsWithAliases,\n  TarOptionsWithAliasesAsync,\n  TarOptionsWithAliasesAsyncFile,\n  TarOptionsWithAliasesAsyncNoFile,\n  TarOptionsWithAliasesFile,\n  TarOptionsWithAliasesNoFile,\n  TarOptionsWithAliasesSync,\n  TarOptionsWithAliasesSyncFile,\n  TarOptionsWithAliasesSyncNoFile,\n} from './options.js'\n\nexport type CB = (er?: Error) => any\n\nexport type TarCommand<\n  AsyncClass,\n  SyncClass extends { sync: true },\n> = {\n  // async and no file specified\n  (): AsyncClass\n  (opt: TarOptionsWithAliasesAsyncNoFile): AsyncClass\n  (entries: string[]): AsyncClass\n  (\n    opt: TarOptionsWithAliasesAsyncNoFile,\n    entries: string[],\n  ): AsyncClass\n} & {\n  // sync and no file\n  (opt: TarOptionsWithAliasesSyncNoFile): SyncClass\n  (opt: TarOptionsWithAliasesSyncNoFile, entries: string[]): SyncClass\n} & {\n  // async and file\n  (opt: TarOptionsWithAliasesAsyncFile): Promise<void>\n  (\n    opt: TarOptionsWithAliasesAsyncFile,\n    entries: string[],\n  ): Promise<void>\n  (opt: TarOptionsWithAliasesAsyncFile, cb: CB): Promise<void>\n  (\n    opt: TarOptionsWithAliasesAsyncFile,\n    entries: string[],\n    cb: CB,\n  ): Promise<void>\n} & {\n  // sync and file\n  (opt: TarOptionsWithAliasesSyncFile): void\n  (opt: TarOptionsWithAliasesSyncFile, entries: string[]): void\n} & {\n  // sync, maybe file\n  (opt: TarOptionsWithAliasesSync): typeof opt extends (\n    TarOptionsWithAliasesFile\n  ) ?\n    void\n  : typeof opt extends TarOptionsWithAliasesNoFile ? SyncClass\n  : void | SyncClass\n  (\n    opt: TarOptionsWithAliasesSync,\n    entries: string[],\n  ): typeof opt extends TarOptionsWithAliasesFile ? void\n  : typeof opt extends TarOptionsWithAliasesNoFile ? SyncClass\n  : void | SyncClass\n} & {\n  // async, maybe file\n  (opt: TarOptionsWithAliasesAsync): typeof opt extends (\n    TarOptionsWithAliasesFile\n  ) ?\n    Promise<void>\n  : typeof opt extends TarOptionsWithAliasesNoFile ? AsyncClass\n  : Promise<void> | AsyncClass\n  (\n    opt: TarOptionsWithAliasesAsync,\n    entries: string[],\n  ): typeof opt extends TarOptionsWithAliasesFile ? Promise<void>\n  : typeof opt extends TarOptionsWithAliasesNoFile ? AsyncClass\n  : Promise<void> | AsyncClass\n  (opt: TarOptionsWithAliasesAsync, cb: CB): Promise<void>\n  (\n    opt: TarOptionsWithAliasesAsync,\n    entries: string[],\n    cb: CB,\n  ): typeof opt extends TarOptionsWithAliasesFile ? Promise<void>\n  : typeof opt extends TarOptionsWithAliasesNoFile ? never\n  : Promise<void>\n} & {\n  // maybe sync, file\n  (opt: TarOptionsWithAliasesFile): Promise<void> | void\n  (\n    opt: TarOptionsWithAliasesFile,\n    entries: string[],\n  ): typeof opt extends TarOptionsWithAliasesSync ? void\n  : typeof opt extends TarOptionsWithAliasesAsync ? Promise<void>\n  : Promise<void> | void\n  (opt: TarOptionsWithAliasesFile, cb: CB): Promise<void>\n  (\n    opt: TarOptionsWithAliasesFile,\n    entries: string[],\n    cb: CB,\n  ): typeof opt extends TarOptionsWithAliasesSync ? never\n  : typeof opt extends TarOptionsWithAliasesAsync ? Promise<void>\n  : Promise<void>\n} & {\n  // maybe sync, no file\n  (opt: TarOptionsWithAliasesNoFile): typeof opt extends (\n    TarOptionsWithAliasesSync\n  ) ?\n    SyncClass\n  : typeof opt extends TarOptionsWithAliasesAsync ? AsyncClass\n  : SyncClass | AsyncClass\n  (\n    opt: TarOptionsWithAliasesNoFile,\n    entries: string[],\n  ): typeof opt extends TarOptionsWithAliasesSync ? SyncClass\n  : typeof opt extends TarOptionsWithAliasesAsync ? AsyncClass\n  : SyncClass | AsyncClass\n} & {\n  // maybe sync, maybe file\n  (opt: TarOptionsWithAliases): typeof opt extends (\n    TarOptionsWithAliasesFile\n  ) ?\n    typeof opt extends TarOptionsWithAliasesSync ? void\n    : typeof opt extends TarOptionsWithAliasesAsync ? Promise<void>\n    : void | Promise<void>\n  : typeof opt extends TarOptionsWithAliasesNoFile ?\n    typeof opt extends TarOptionsWithAliasesSync ? SyncClass\n    : typeof opt extends TarOptionsWithAliasesAsync ? AsyncClass\n    : SyncClass | AsyncClass\n  : typeof opt extends TarOptionsWithAliasesSync ? SyncClass | void\n  : typeof opt extends TarOptionsWithAliasesAsync ?\n    AsyncClass | Promise<void>\n  : SyncClass | void | AsyncClass | Promise<void>\n} & {\n  // extras\n  syncFile: (opt: TarOptionsSyncFile, entries: string[]) => void\n  asyncFile: (\n    opt: TarOptionsAsyncFile,\n    entries: string[],\n    cb?: CB,\n  ) => Promise<void>\n  syncNoFile: (\n    opt: TarOptionsSyncNoFile,\n    entries: string[],\n  ) => SyncClass\n  asyncNoFile: (\n    opt: TarOptionsAsyncNoFile,\n    entries: string[],\n  ) => AsyncClass\n  validate?: (opt: TarOptions, entries?: string[]) => void\n}\n\nexport const makeCommand = <\n  AsyncClass,\n  SyncClass extends { sync: true },\n>(\n  syncFile: (opt: TarOptionsSyncFile, entries: string[]) => void,\n  asyncFile: (\n    opt: TarOptionsAsyncFile,\n    entries: string[],\n    cb?: CB,\n  ) => Promise<void>,\n  syncNoFile: (\n    opt: TarOptionsSyncNoFile,\n    entries: string[],\n  ) => SyncClass,\n  asyncNoFile: (\n    opt: TarOptionsAsyncNoFile,\n    entries: string[],\n  ) => AsyncClass,\n  validate?: (opt: TarOptions, entries?: string[]) => void,\n): TarCommand<AsyncClass, SyncClass> => {\n  return Object.assign(\n    (\n      opt_: TarOptionsWithAliases | string[] = [],\n      entries?: string[] | CB,\n      cb?: CB,\n    ) => {\n      if (Array.isArray(opt_)) {\n        entries = opt_\n        opt_ = {}\n      }\n\n      if (typeof entries === 'function') {\n        cb = entries\n        entries = undefined\n      }\n\n      if (!entries) {\n        entries = []\n      } else {\n        entries = Array.from(entries)\n      }\n\n      const opt = dealias(opt_)\n\n      validate?.(opt, entries)\n\n      if (isSyncFile(opt)) {\n        if (typeof cb === 'function') {\n          throw new TypeError(\n            'callback not supported for sync tar functions',\n          )\n        }\n        return syncFile(opt, entries)\n      } else if (isAsyncFile(opt)) {\n        const p = asyncFile(opt, entries)\n        // weirdness to make TS happy\n        const c = cb ? cb : undefined\n        return c ? p.then(() => c(), c) : p\n      } else if (isSyncNoFile(opt)) {\n        if (typeof cb === 'function') {\n          throw new TypeError(\n            'callback not supported for sync tar functions',\n          )\n        }\n        return syncNoFile(opt, entries)\n      } else if (isAsyncNoFile(opt)) {\n        if (typeof cb === 'function') {\n          throw new TypeError(\n            'callback only supported with file option',\n          )\n        }\n        return asyncNoFile(opt, entries)\n        /* c8 ignore start */\n      } else {\n        throw new Error('impossible options??')\n      }\n      /* c8 ignore stop */\n    },\n    {\n      syncFile,\n      asyncFile,\n      syncNoFile,\n      asyncNoFile,\n      validate,\n    },\n  ) as TarCommand<AsyncClass, SyncClass>\n}\n", "// Update with any zlib constants that are added or changed in the future.\n// Node v6 didn't export this, so we just hard code the version and rely\n// on all the other hard-coded values from zlib v4736.  When node v6\n// support drops, we can just export the realZlibConstants object.\nimport realZlib from 'zlib'\n/* c8 ignore start */\nconst realZlibConstants = realZlib.constants || { ZLIB_VERNUM: 4736 }\n/* c8 ignore stop */\n\nexport const constants = Object.freeze(\n  Object.assign(\n    Object.create(null),\n    {\n      Z_NO_FLUSH: 0,\n      Z_PARTIAL_FLUSH: 1,\n      Z_SYNC_FLUSH: 2,\n      Z_FULL_FLUSH: 3,\n      Z_FINISH: 4,\n      Z_BLOCK: 5,\n      Z_OK: 0,\n      Z_STREAM_END: 1,\n      Z_NEED_DICT: 2,\n      Z_ERRNO: -1,\n      Z_STREAM_ERROR: -2,\n      Z_DATA_ERROR: -3,\n      Z_MEM_ERROR: -4,\n      Z_BUF_ERROR: -5,\n      Z_VERSION_ERROR: -6,\n      Z_NO_COMPRESSION: 0,\n      Z_BEST_SPEED: 1,\n      Z_BEST_COMPRESSION: 9,\n      Z_DEFAULT_COMPRESSION: -1,\n      Z_FILTERED: 1,\n      Z_HUFFMAN_ONLY: 2,\n      Z_RLE: 3,\n      Z_FIXED: 4,\n      Z_DEFAULT_STRATEGY: 0,\n      DEFLATE: 1,\n      INFLATE: 2,\n      GZIP: 3,\n      GUNZIP: 4,\n      DEFLATERAW: 5,\n      INFLATERAW: 6,\n      UNZIP: 7,\n      BROTLI_DECODE: 8,\n      BROTLI_ENCODE: 9,\n      Z_MIN_WINDOWBITS: 8,\n      Z_MAX_WINDOWBITS: 15,\n      Z_DEFAULT_WINDOWBITS: 15,\n      Z_MIN_CHUNK: 64,\n      Z_MAX_CHUNK: Infinity,\n      Z_DEFAULT_CHUNK: 16384,\n      Z_MIN_MEMLEVEL: 1,\n      Z_MAX_MEMLEVEL: 9,\n      Z_DEFAULT_MEMLEVEL: 8,\n      Z_MIN_LEVEL: -1,\n      Z_MAX_LEVEL: 9,\n      Z_DEFAULT_LEVEL: -1,\n      BROTLI_OPERATION_PROCESS: 0,\n      BROTLI_OPERATION_FLUSH: 1,\n      BROTLI_OPERATION_FINISH: 2,\n      BROTLI_OPERATION_EMIT_METADATA: 3,\n      BROTLI_MODE_GENERIC: 0,\n      BROTLI_MODE_TEXT: 1,\n      BROTLI_MODE_FONT: 2,\n      BROTLI_DEFAULT_MODE: 0,\n      BROTLI_MIN_QUALITY: 0,\n      BROTLI_MAX_QUALITY: 11,\n      BROTLI_DEFAULT_QUALITY: 11,\n      BROTLI_MIN_WINDOW_BITS: 10,\n      BROTLI_MAX_WINDOW_BITS: 24,\n      BROTLI_LARGE_MAX_WINDOW_BITS: 30,\n      BROTLI_DEFAULT_WINDOW: 22,\n      BROTLI_MIN_INPUT_BLOCK_BITS: 16,\n      BROTLI_MAX_INPUT_BLOCK_BITS: 24,\n      BROTLI_PARAM_MODE: 0,\n      BROTLI_PARAM_QUALITY: 1,\n      BROTLI_PARAM_LGWIN: 2,\n      BROTLI_PARAM_LGBLOCK: 3,\n      BROTLI_PARAM_DISABLE_LITERAL_CONTEXT_MODELING: 4,\n      BROTLI_PARAM_SIZE_HINT: 5,\n      BROTLI_PARAM_LARGE_WINDOW: 6,\n      BROTLI_PARAM_NPOSTFIX: 7,\n      BROTLI_PARAM_NDIRECT: 8,\n      BROTLI_DECODER_RESULT_ERROR: 0,\n      BROTLI_DECODER_RESULT_SUCCESS: 1,\n      BROTLI_DECODER_RESULT_NEEDS_MORE_INPUT: 2,\n      BROTLI_DECODER_RESULT_NEEDS_MORE_OUTPUT: 3,\n      BROTLI_DECODER_PARAM_DISABLE_RING_BUFFER_REALLOCATION: 0,\n      BROTLI_DECODER_PARAM_LARGE_WINDOW: 1,\n      BROTLI_DECODER_NO_ERROR: 0,\n      BROTLI_DECODER_SUCCESS: 1,\n      BROTLI_DECODER_NEEDS_MORE_INPUT: 2,\n      BROTLI_DECODER_NEEDS_MORE_OUTPUT: 3,\n      BROTLI_DECODER_ERROR_FORMAT_EXUBERANT_NIBBLE: -1,\n      BROTLI_DECODER_ERROR_FORMAT_RESERVED: -2,\n      BROTLI_DECODER_ERROR_FORMAT_EXUBERANT_META_NIBBLE: -3,\n      BROTLI_DECODER_ERROR_FORMAT_SIMPLE_HUFFMAN_ALPHABET: -4,\n      BROTLI_DECODER_ERROR_FORMAT_SIMPLE_HUFFMAN_SAME: -5,\n      BROTLI_DECODER_ERROR_FORMAT_CL_SPACE: -6,\n      BROTLI_DECODER_ERROR_FORMAT_HUFFMAN_SPACE: -7,\n      BROTLI_DECODER_ERROR_FORMAT_CONTEXT_MAP_REPEAT: -8,\n      BROTLI_DECODER_ERROR_FORMAT_BLOCK_LENGTH_1: -9,\n      BROTLI_DECODER_ERROR_FORMAT_BLOCK_LENGTH_2: -10,\n      BROTLI_DECODER_ERROR_FORMAT_TRANSFORM: -11,\n      BROTLI_DECODER_ERROR_FORMAT_DICTIONARY: -12,\n      BROTLI_DECODER_ERROR_FORMAT_WINDOW_BITS: -13,\n      BROTLI_DECODER_ERROR_FORMAT_PADDING_1: -14,\n      BROTLI_DECODER_ERROR_FORMAT_PADDING_2: -15,\n      BROTLI_DECODER_ERROR_FORMAT_DISTANCE: -16,\n      BROTLI_DECODER_ERROR_DICTIONARY_NOT_SET: -19,\n      BROTLI_DECODER_ERROR_INVALID_ARGUMENTS: -20,\n      BROTLI_DECODER_ERROR_ALLOC_CONTEXT_MODES: -21,\n      BROTLI_DECODER_ERROR_ALLOC_TREE_GROUPS: -22,\n      BROTLI_DECODER_ERROR_ALLOC_CONTEXT_MAP: -25,\n      BROTLI_DECODER_ERROR_ALLOC_RING_BUFFER_1: -26,\n      BROTLI_DECODER_ERROR_ALLOC_RING_BUFFER_2: -27,\n      BROTLI_DECODER_ERROR_ALLOC_BLOCK_TYPE_TREES: -30,\n      BROTLI_DECODER_ERROR_UNREACHABLE: -31,\n    },\n    realZlibConstants,\n  ),\n)\n", "import assert from 'assert'\nimport { Buffer } from 'buffer'\nimport { Minipass } from 'minipass'\nimport * as realZlib from 'zlib'\nimport { constants } from './constants.js'\nexport { constants } from './constants.js'\n\nconst OriginalBufferConcat = Buffer.concat\nconst desc = Object.getOwnPropertyDescriptor(Buffer, 'concat')\nconst noop = (args: Buffer[]) => args as unknown as Buffer\nconst passthroughBufferConcat =\n  desc?.writable === true || desc?.set !== undefined\n    ? (makeNoOp: boolean) => {\n        Buffer.concat = makeNoOp ? noop : OriginalBufferConcat\n      }\n    : (_: boolean) => {}\n\nconst _superWrite = Symbol('_superWrite')\n\nexport class ZlibError extends Error {\n  code?: string\n  errno?: number\n  constructor(err: NodeJS.ErrnoException | Error, origin?: Function) {\n    super('zlib: ' + err.message, { cause: err })\n    this.code = (err as NodeJS.ErrnoException).code\n    this.errno = (err as NodeJS.ErrnoException).errno\n    /* c8 ignore next */\n    if (!this.code) this.code = 'ZLIB_ERROR'\n\n    this.message = 'zlib: ' + err.message\n    Error.captureStackTrace(this, origin ?? this.constructor)\n  }\n\n  get name() {\n    return 'ZlibError'\n  }\n}\n\n// the Zlib class they all inherit from\n// This thing manages the queue of requests, and returns\n// true or false if there is anything in the queue when\n// you call the .write() method.\nconst _flushFlag = Symbol('flushFlag')\n\nexport type ChunkWithFlushFlag = Minipass.ContiguousData & {\n  [_flushFlag]?: number\n}\n\nexport type ZlibBaseOptions = Minipass.Options<Minipass.ContiguousData> & {\n  flush?: number\n  finishFlush?: number\n  fullFlushFlag?: number\n}\n\nexport type ZlibHandle =\n  | realZlib.Gzip\n  | realZlib.Gunzip\n  | realZlib.Deflate\n  | realZlib.Inflate\n  | realZlib.DeflateRaw\n  | realZlib.InflateRaw\n  | realZlib.BrotliCompress\n  | realZlib.BrotliDecompress\n  | realZlib.ZstdCompress\n  | realZlib.ZstdDecompress\nexport type ZlibMode =\n  | 'Gzip'\n  | 'Gunzip'\n  | 'Deflate'\n  | 'Inflate'\n  | 'DeflateRaw'\n  | 'InflateRaw'\n  | 'Unzip'\nexport type BrotliMode = 'BrotliCompress' | 'BrotliDecompress'\nexport type ZstdMode = 'ZstdCompress' | 'ZstdDecompress'\n\nabstract class ZlibBase extends Minipass<Buffer, ChunkWithFlushFlag> {\n  #sawError: boolean = false\n  #ended: boolean = false\n  #flushFlag: number\n  #finishFlushFlag: number\n  #fullFlushFlag: number\n  #handle?: ZlibHandle\n  #onError: (err: ZlibError) => any\n\n  get sawError() {\n    return this.#sawError\n  }\n  get handle() {\n    return this.#handle\n  }\n  /* c8 ignore start */\n  get flushFlag() {\n    return this.#flushFlag\n  }\n  /* c8 ignore stop */\n\n  constructor(opts: ZlibBaseOptions, mode: ZlibMode | BrotliMode | ZstdMode) {\n    if (!opts || typeof opts !== 'object')\n      throw new TypeError('invalid options for ZlibBase constructor')\n\n    //@ts-ignore\n    super(opts)\n\n    /* c8 ignore start */\n    this.#flushFlag = opts.flush ?? 0\n    this.#finishFlushFlag = opts.finishFlush ?? 0\n    this.#fullFlushFlag = opts.fullFlushFlag ?? 0\n    /* c8 ignore stop */\n\n    //@ts-ignore\n    if (typeof realZlib[mode] !== 'function') {\n      throw new TypeError('Compression method not supported: ' + mode)\n    }\n\n    // this will throw if any options are invalid for the class selected\n    try {\n      // @types/node doesn't know that it exports the classes, but they're there\n      //@ts-ignore\n      this.#handle = new realZlib[mode](opts)\n    } catch (er) {\n      // make sure that all errors get decorated properly\n      throw new ZlibError(er as NodeJS.ErrnoException, this.constructor)\n    }\n\n    this.#onError = err => {\n      // no sense raising multiple errors, since we abort on the first one.\n      if (this.#sawError) return\n\n      this.#sawError = true\n\n      // there is no way to cleanly recover.\n      // continuing only obscures problems.\n      this.close()\n      this.emit('error', err)\n    }\n\n    this.#handle?.on('error', er => this.#onError(new ZlibError(er)))\n    this.once('end', () => this.close)\n  }\n\n  close() {\n    if (this.#handle) {\n      this.#handle.close()\n      this.#handle = undefined\n      this.emit('close')\n    }\n  }\n\n  reset() {\n    if (!this.#sawError) {\n      assert(this.#handle, 'zlib binding closed')\n      //@ts-ignore\n      return this.#handle.reset?.()\n    }\n  }\n\n  flush(flushFlag?: number) {\n    if (this.ended) return\n\n    if (typeof flushFlag !== 'number') flushFlag = this.#fullFlushFlag\n\n    this.write(Object.assign(Buffer.alloc(0), { [_flushFlag]: flushFlag }))\n  }\n\n  end(cb?: () => void): this\n  end(chunk: ChunkWithFlushFlag, cb?: () => void): this\n  end(\n    chunk: ChunkWithFlushFlag,\n    encoding?: Minipass.Encoding,\n    cb?: () => void,\n  ): this\n  end(\n    chunk?: ChunkWithFlushFlag | (() => void),\n    encoding?: Minipass.Encoding | (() => void),\n    cb?: () => void,\n  ) {\n    /* c8 ignore start */\n    if (typeof chunk === 'function') {\n      cb = chunk\n      encoding = undefined\n      chunk = undefined\n    }\n    if (typeof encoding === 'function') {\n      cb = encoding\n      encoding = undefined\n    }\n    /* c8 ignore stop */\n    if (chunk) {\n      if (encoding) this.write(chunk, encoding)\n      else this.write(chunk)\n    }\n    this.flush(this.#finishFlushFlag)\n    this.#ended = true\n    return super.end(cb)\n  }\n\n  get ended() {\n    return this.#ended\n  }\n\n  // overridden in the gzip classes to do portable writes\n  [_superWrite](data: Buffer & { [_flushFlag]?: number }) {\n    return super.write(data)\n  }\n\n  write(chunk: ChunkWithFlushFlag, cb?: () => void): boolean\n  write(\n    chunk: ChunkWithFlushFlag,\n    encoding?: Minipass.Encoding,\n    cb?: () => void,\n  ): boolean\n  write(\n    chunk: ChunkWithFlushFlag,\n    encoding?: Minipass.Encoding | (() => void),\n    cb?: () => void,\n  ) {\n    // process the chunk using the sync process\n    // then super.write() all the outputted chunks\n    if (typeof encoding === 'function')\n      (cb = encoding), (encoding = 'utf8')\n\n    if (typeof chunk === 'string')\n      chunk = Buffer.from(chunk as string, encoding as BufferEncoding)\n\n    if (this.#sawError) return\n    assert(this.#handle, 'zlib binding closed')\n\n    // _processChunk tries to .close() the native handle after it's done, so we\n    // intercept that by temporarily making it a no-op.\n    // diving into the node:zlib internals a bit here\n    const nativeHandle = (this.#handle as unknown as { _handle: any })\n      ._handle\n    const originalNativeClose = nativeHandle.close\n    nativeHandle.close = () => {}\n    const originalClose = this.#handle.close\n    this.#handle.close = () => {}\n    // It also calls `Buffer.concat()` at the end, which may be convenient\n    // for some, but which we are not interested in as it slows us down.\n    passthroughBufferConcat(true)\n    let result: undefined | Buffer | Buffer[] = undefined\n    try {\n      const flushFlag =\n        typeof chunk[_flushFlag] === 'number'\n          ? chunk[_flushFlag]\n          : this.#flushFlag\n      result = (\n        this.#handle as unknown as {\n          _processChunk: (chunk: Buffer, flushFlag: number) => Buffer[]\n        }\n      )._processChunk(chunk as Buffer, flushFlag)\n      // if we don't throw, reset it back how it was\n      passthroughBufferConcat(false)\n    } catch (err) {\n      // or if we do, put Buffer.concat() back before we emit error\n      // Error events call into user code, which may call Buffer.concat()\n      passthroughBufferConcat(false)\n      this.#onError(new ZlibError(err as NodeJS.ErrnoException, this.write))\n    } finally {\n      if (this.#handle) {\n        // Core zlib resets `_handle` to null after attempting to close the\n        // native handle. Our no-op handler prevented actual closure, but we\n        // need to restore the `._handle` property.\n        ;(this.#handle as unknown as { _handle: any })._handle =\n          nativeHandle\n        nativeHandle.close = originalNativeClose\n        this.#handle.close = originalClose\n        // `_processChunk()` adds an 'error' listener. If we don't remove it\n        // after each call, these handlers start piling up.\n        this.#handle.removeAllListeners('error')\n        // make sure OUR error listener is still attached tho\n      }\n    }\n\n    if (this.#handle)\n      this.#handle.on('error', er => this.#onError(new ZlibError(er, this.write)))\n\n    let writeReturn\n    if (result) {\n      if (Array.isArray(result) && result.length > 0) {\n        const r = result[0]\n        // The first buffer is always `handle._outBuffer`, which would be\n        // re-used for later invocations; so, we always have to copy that one.\n        writeReturn = this[_superWrite](Buffer.from(r as Buffer))\n        for (let i = 1; i < result.length; i++) {\n          writeReturn = this[_superWrite](result[i] as Buffer)\n        }\n      } else {\n        // either a single Buffer or an empty array\n        writeReturn = this[_superWrite](Buffer.from(result as Buffer | []))\n      }\n    }\n\n    if (cb) cb()\n    return writeReturn\n  }\n}\n\nexport type ZlibOptions = ZlibBaseOptions & {\n  level?: number\n  strategy?: number\n}\n\nexport class Zlib extends ZlibBase {\n  #level?: number\n  #strategy?: number\n\n  constructor(opts: ZlibOptions, mode: ZlibMode) {\n    opts = opts || {}\n\n    opts.flush = opts.flush || constants.Z_NO_FLUSH\n    opts.finishFlush = opts.finishFlush || constants.Z_FINISH\n    opts.fullFlushFlag = constants.Z_FULL_FLUSH\n    super(opts, mode)\n\n    this.#level = opts.level\n    this.#strategy = opts.strategy\n  }\n\n  params(level: number, strategy: number) {\n    if (this.sawError) return\n\n    if (!this.handle)\n      throw new Error('cannot switch params when binding is closed')\n\n    // no way to test this without also not supporting params at all\n    /* c8 ignore start */\n    if (!(this.handle as { params?: any }).params)\n      throw new Error('not supported in this implementation')\n    /* c8 ignore stop */\n\n    if (this.#level !== level || this.#strategy !== strategy) {\n      this.flush(constants.Z_SYNC_FLUSH)\n      assert(this.handle, 'zlib binding closed')\n      // .params() calls .flush(), but the latter is always async in the\n      // core zlib. We override .flush() temporarily to intercept that and\n      // flush synchronously.\n      const origFlush = this.handle.flush\n      this.handle.flush = (\n        flushFlag?: (() => void) | number,\n        cb?: () => void,\n      ) => {\n        /* c8 ignore start */\n        if (typeof flushFlag === 'function') {\n          cb = flushFlag\n          flushFlag = this.flushFlag\n        }\n        /* c8 ignore stop */\n        this.flush(flushFlag)\n        cb?.()\n      }\n      try {\n        ;(\n          this.handle as unknown as {\n            params: (level?: number, strategy?: number) => void\n          }\n        ).params(level, strategy)\n      } finally {\n        this.handle.flush = origFlush\n      }\n      /* c8 ignore start */\n      if (this.handle) {\n        this.#level = level\n        this.#strategy = strategy\n      }\n      /* c8 ignore stop */\n    }\n  }\n}\n\n// minimal 2-byte header\nexport class Deflate extends Zlib {\n  constructor(opts: ZlibOptions) {\n    super(opts, 'Deflate')\n  }\n}\n\nexport class Inflate extends Zlib {\n  constructor(opts: ZlibOptions) {\n    super(opts, 'Inflate')\n  }\n}\n\n// gzip - bigger header, same deflate compression\nexport type GzipOptions = ZlibOptions & { portable?: boolean }\nexport class Gzip extends Zlib {\n  #portable: boolean\n  constructor(opts: GzipOptions) {\n    super(opts, 'Gzip')\n    this.#portable = opts && !!opts.portable\n  }\n\n  [_superWrite](data: Buffer & { [_flushFlag]?: number }) {\n    if (!this.#portable) return super[_superWrite](data)\n\n    // we'll always get the header emitted in one first chunk\n    // overwrite the OS indicator byte with 0xFF\n    this.#portable = false\n    data[9] = 255\n    return super[_superWrite](data)\n  }\n}\n\nexport class Gunzip extends Zlib {\n  constructor(opts: ZlibOptions) {\n    super(opts, 'Gunzip')\n  }\n}\n\n// raw - no header\nexport class DeflateRaw extends Zlib {\n  constructor(opts: ZlibOptions) {\n    super(opts, 'DeflateRaw')\n  }\n}\n\nexport class InflateRaw extends Zlib {\n  constructor(opts: ZlibOptions) {\n    super(opts, 'InflateRaw')\n  }\n}\n\n// auto-detect header.\nexport class Unzip extends Zlib {\n  constructor(opts: ZlibOptions) {\n    super(opts, 'Unzip')\n  }\n}\n\nclass Brotli extends ZlibBase {\n  constructor(opts: ZlibOptions, mode: BrotliMode) {\n    opts = opts || {}\n\n    opts.flush = opts.flush || constants.BROTLI_OPERATION_PROCESS\n    opts.finishFlush =\n      opts.finishFlush || constants.BROTLI_OPERATION_FINISH\n    opts.fullFlushFlag = constants.BROTLI_OPERATION_FLUSH\n    super(opts, mode)\n  }\n}\n\nexport class BrotliCompress extends Brotli {\n  constructor(opts: ZlibOptions) {\n    super(opts, 'BrotliCompress')\n  }\n}\n\nexport class BrotliDecompress extends Brotli {\n  constructor(opts: ZlibOptions) {\n    super(opts, 'BrotliDecompress')\n  }\n}\n\nclass Zstd extends ZlibBase {\n  constructor(opts: ZlibOptions, mode: ZstdMode) {\n    opts = opts || {}\n\n    opts.flush = opts.flush || constants.ZSTD_e_continue\n    opts.finishFlush = opts.finishFlush || constants.ZSTD_e_end\n    opts.fullFlushFlag = constants.ZSTD_e_flush\n    super(opts, mode)\n  }\n}\n\nexport class ZstdCompress extends Zstd {\n  constructor(opts: ZlibOptions) {\n    super(opts, 'ZstdCompress')\n  }\n}\n\nexport class ZstdDecompress extends Zstd {\n  constructor(opts: ZlibOptions) {\n    super(opts, 'ZstdDecompress')\n  }\n}\n", "// Tar can encode large and negative numbers using a leading byte of\n// 0xff for negative, and 0x80 for positive.\n\nexport const encode = (num: number, buf: Buffer) => {\n  if (!Number.isSafeInteger(num)) {\n    // The number is so large that javascript cannot represent it with integer\n    // precision.\n    throw Error(\n      'cannot encode number outside of javascript safe integer range',\n    )\n  } else if (num < 0) {\n    encodeNegative(num, buf)\n  } else {\n    encodePositive(num, buf)\n  }\n  return buf\n}\n\nconst encodePositive = (num: number, buf: Buffer) => {\n  buf[0] = 0x80\n\n  for (var i = buf.length; i > 1; i--) {\n    buf[i - 1] = num & 0xff\n    num = Math.floor(num / 0x100)\n  }\n}\n\nconst encodeNegative = (num: number, buf: Buffer) => {\n  buf[0] = 0xff\n  var flipped = false\n  num = num * -1\n  for (var i = buf.length; i > 1; i--) {\n    var byte = num & 0xff\n    num = Math.floor(num / 0x100)\n    if (flipped) {\n      buf[i - 1] = onesComp(byte)\n    } else if (byte === 0) {\n      buf[i - 1] = 0\n    } else {\n      flipped = true\n      buf[i - 1] = twosComp(byte)\n    }\n  }\n}\n\nexport const parse = (buf: Buffer) => {\n  const pre = buf[0]\n  const value =\n    pre === 0x80 ? pos(buf.subarray(1, buf.length))\n    : pre === 0xff ? twos(buf)\n    : null\n  if (value === null) {\n    throw Error('invalid base256 encoding')\n  }\n\n  if (!Number.isSafeInteger(value)) {\n    // The number is so large that javascript cannot represent it with integer\n    // precision.\n    throw Error(\n      'parsed number outside of javascript safe integer range',\n    )\n  }\n\n  return value\n}\n\nconst twos = (buf: Buffer) => {\n  var len = buf.length\n  var sum = 0\n  var flipped = false\n  for (var i = len - 1; i > -1; i--) {\n    var byte = Number(buf[i])\n    var f\n    if (flipped) {\n      f = onesComp(byte)\n    } else if (byte === 0) {\n      f = byte\n    } else {\n      flipped = true\n      f = twosComp(byte)\n    }\n    if (f !== 0) {\n      sum -= f * Math.pow(256, len - i - 1)\n    }\n  }\n  return sum\n}\n\nconst pos = (buf: Buffer) => {\n  var len = buf.length\n  var sum = 0\n  for (var i = len - 1; i > -1; i--) {\n    var byte = Number(buf[i])\n    if (byte !== 0) {\n      sum += byte * Math.pow(256, len - i - 1)\n    }\n  }\n  return sum\n}\n\nconst onesComp = (byte: number) => (0xff ^ byte) & 0xff\n\nconst twosComp = (byte: number) => ((0xff ^ byte) + 1) & 0xff\n", "export const isCode = (c: string): c is EntryTypeCode =>\n  name.has(c as EntryTypeCode)\n\nexport const isName = (c: string): c is EntryTypeName =>\n  code.has(c as EntryTypeName)\n\nexport type EntryTypeCode =\n  | '0'\n  | ''\n  | '1'\n  | '2'\n  | '3'\n  | '4'\n  | '5'\n  | '6'\n  | '7'\n  | 'g'\n  | 'x'\n  | 'A'\n  | 'D'\n  | 'I'\n  | 'K'\n  | 'L'\n  | 'M'\n  | 'N'\n  | 'S'\n  | 'V'\n  | 'X'\n\nexport type EntryTypeName =\n  | 'File'\n  | 'OldFile'\n  | 'Link'\n  | 'SymbolicLink'\n  | 'CharacterDevice'\n  | 'BlockDevice'\n  | 'Directory'\n  | 'FIFO'\n  | 'ContiguousFile'\n  | 'GlobalExtendedHeader'\n  | 'ExtendedHeader'\n  | 'SolarisACL'\n  | 'GNUDumpDir'\n  | 'Inode'\n  | 'NextFileHasLongLinkpath'\n  | 'NextFileHasLongPath'\n  | 'ContinuationFile'\n  | 'OldGnuLongPath'\n  | 'SparseFile'\n  | 'TapeVolumeHeader'\n  | 'OldExtendedHeader'\n  | 'Unsupported'\n\n// map types from key to human-friendly name\nexport const name = new Map<EntryTypeCode, EntryTypeName>([\n  ['0', 'File'],\n  // same as File\n  ['', 'OldFile'],\n  ['1', 'Link'],\n  ['2', 'SymbolicLink'],\n  // Devices and FIFOs aren't fully supported\n  // they are parsed, but skipped when unpacking\n  ['3', 'CharacterDevice'],\n  ['4', 'BlockDevice'],\n  ['5', 'Directory'],\n  ['6', 'FIFO'],\n  // same as File\n  ['7', 'ContiguousFile'],\n  // pax headers\n  ['g', 'GlobalExtendedHeader'],\n  ['x', 'ExtendedHeader'],\n  // vendor-specific stuff\n  // skip\n  ['A', 'SolarisACL'],\n  // like 5, but with data, which should be skipped\n  ['D', 'GNUDumpDir'],\n  // metadata only, skip\n  ['I', 'Inode'],\n  // data = link path of next file\n  ['K', 'NextFileHasLongLinkpath'],\n  // data = path of next file\n  ['L', 'NextFileHasLongPath'],\n  // skip\n  ['M', 'ContinuationFile'],\n  // like L\n  ['N', 'OldGnuLongPath'],\n  // skip\n  ['S', 'SparseFile'],\n  // skip\n  ['V', 'TapeVolumeHeader'],\n  // like x\n  ['X', 'OldExtendedHeader'],\n])\n\n// map the other direction\nexport const code = new Map<EntryTypeName, EntryTypeCode>(\n  Array.from(name).map(kv => [kv[1], kv[0]]),\n)\n", "// parse a 512-byte header block to a data object, or vice-versa\n// encode returns `true` if a pax extended header is needed, because\n// the data could not be faithfully encoded in a simple header.\n// (Also, check header.needPax to see if it needs a pax header.)\n\nimport { posix as pathModule } from 'node:path'\nimport * as large from './large-numbers.js'\nimport type { EntryTypeCode, EntryTypeName } from './types.js'\nimport * as types from './types.js'\n\nexport type HeaderData = {\n  path?: string\n  mode?: number\n  uid?: number\n  gid?: number\n  size?: number\n  cksum?: number\n  type?: EntryTypeName | 'Unsupported'\n  linkpath?: string\n  uname?: string\n  gname?: string\n  devmaj?: number\n  devmin?: number\n  atime?: Date\n  ctime?: Date\n  mtime?: Date\n\n  // fields that are common in extended PAX headers, but not in the\n  // \"standard\" tar header block\n  charset?: string\n  comment?: string\n  dev?: number\n  ino?: number\n  nlink?: number\n}\n\nexport class Header implements HeaderData {\n  cksumValid: boolean = false\n  needPax: boolean = false\n  nullBlock: boolean = false\n\n  block?: Buffer\n  path?: string\n  mode?: number\n  uid?: number\n  gid?: number\n  size?: number\n  cksum?: number\n  #type: EntryTypeCode | 'Unsupported' = 'Unsupported'\n  linkpath?: string\n  uname?: string\n  gname?: string\n  devmaj: number = 0\n  devmin: number = 0\n  atime?: Date\n  ctime?: Date\n  mtime?: Date\n\n  charset?: string\n  comment?: string\n\n  constructor(\n    data?: Buffer | HeaderData,\n    off: number = 0,\n    ex?: HeaderData,\n    gex?: HeaderData,\n  ) {\n    if (Buffer.isBuffer(data)) {\n      this.decode(data, off || 0, ex, gex)\n    } else if (data) {\n      this.#slurp(data)\n    }\n  }\n\n  decode(\n    buf: Buffer,\n    off: number,\n    ex?: HeaderData,\n    gex?: HeaderData,\n  ) {\n    if (!off) {\n      off = 0\n    }\n\n    if (!buf || !(buf.length >= off + 512)) {\n      throw new Error('need 512 bytes for header')\n    }\n\n    this.path = ex?.path ?? decString(buf, off, 100)\n    this.mode = ex?.mode ?? gex?.mode ?? decNumber(buf, off + 100, 8)\n    this.uid = ex?.uid ?? gex?.uid ?? decNumber(buf, off + 108, 8)\n    this.gid = ex?.gid ?? gex?.gid ?? decNumber(buf, off + 116, 8)\n    this.size = ex?.size ?? gex?.size ?? decNumber(buf, off + 124, 12)\n    this.mtime =\n      ex?.mtime ?? gex?.mtime ?? decDate(buf, off + 136, 12)\n    this.cksum = decNumber(buf, off + 148, 12)\n\n    // if we have extended or global extended headers, apply them now\n    // See https://github.com/npm/node-tar/pull/187\n    // Apply global before local, so it overrides\n    if (gex) this.#slurp(gex, true)\n    if (ex) this.#slurp(ex)\n\n    // old tar versions marked dirs as a file with a trailing /\n    const t = decString(buf, off + 156, 1)\n    if (types.isCode(t)) {\n      this.#type = t || '0'\n    }\n    if (this.#type === '0' && this.path.slice(-1) === '/') {\n      this.#type = '5'\n    }\n\n    // tar implementations sometimes incorrectly put the stat(dir).size\n    // as the size in the tarball, even though Directory entries are\n    // not able to have any body at all.  In the very rare chance that\n    // it actually DOES have a body, we weren't going to do anything with\n    // it anyway, and it'll just be a warning about an invalid header.\n    if (this.#type === '5') {\n      this.size = 0\n    }\n\n    this.linkpath = decString(buf, off + 157, 100)\n    if (\n      buf.subarray(off + 257, off + 265).toString() ===\n      'ustar\\u000000'\n    ) {\n      /* c8 ignore start */\n      this.uname =\n        ex?.uname ?? gex?.uname ?? decString(buf, off + 265, 32)\n      this.gname =\n        ex?.gname ?? gex?.gname ?? decString(buf, off + 297, 32)\n      this.devmaj =\n        ex?.devmaj ?? gex?.devmaj ?? decNumber(buf, off + 329, 8) ?? 0\n      this.devmin =\n        ex?.devmin ?? gex?.devmin ?? decNumber(buf, off + 337, 8) ?? 0\n      /* c8 ignore stop */\n      if (buf[off + 475] !== 0) {\n        // definitely a prefix, definitely >130 chars.\n        const prefix = decString(buf, off + 345, 155)\n        this.path = prefix + '/' + this.path\n      } else {\n        const prefix = decString(buf, off + 345, 130)\n        if (prefix) {\n          this.path = prefix + '/' + this.path\n        }\n        /* c8 ignore start */\n        this.atime =\n          ex?.atime ?? gex?.atime ?? decDate(buf, off + 476, 12)\n        this.ctime =\n          ex?.ctime ?? gex?.ctime ?? decDate(buf, off + 488, 12)\n        /* c8 ignore stop */\n      }\n    }\n\n    let sum = 8 * 0x20\n    for (let i = off; i < off + 148; i++) {\n      sum += buf[i] as number\n    }\n\n    for (let i = off + 156; i < off + 512; i++) {\n      sum += buf[i] as number\n    }\n\n    this.cksumValid = sum === this.cksum\n    if (this.cksum === undefined && sum === 8 * 0x20) {\n      this.nullBlock = true\n    }\n  }\n\n  #slurp(ex: HeaderData, gex: boolean = false) {\n    Object.assign(\n      this,\n      Object.fromEntries(\n        Object.entries(ex).filter(([k, v]) => {\n          // we slurp in everything except for the path attribute in\n          // a global extended header, because that's weird. Also, any\n          // null/undefined values are ignored.\n          return !(\n            v === null ||\n            v === undefined ||\n            (k === 'path' && gex) ||\n            (k === 'linkpath' && gex) ||\n            k === 'global'\n          )\n        }),\n      ),\n    )\n  }\n\n  encode(buf?: Buffer, off: number = 0) {\n    if (!buf) {\n      buf = this.block = Buffer.alloc(512)\n    }\n\n    if (this.#type === 'Unsupported') {\n      this.#type = '0'\n    }\n\n    if (!(buf.length >= off + 512)) {\n      throw new Error('need 512 bytes for header')\n    }\n\n    const prefixSize = this.ctime || this.atime ? 130 : 155\n    const split = splitPrefix(this.path || '', prefixSize)\n    const path = split[0]\n    const prefix = split[1]\n    this.needPax = !!split[2]\n\n    this.needPax = encString(buf, off, 100, path) || this.needPax\n    this.needPax =\n      encNumber(buf, off + 100, 8, this.mode) || this.needPax\n    this.needPax =\n      encNumber(buf, off + 108, 8, this.uid) || this.needPax\n    this.needPax =\n      encNumber(buf, off + 116, 8, this.gid) || this.needPax\n    this.needPax =\n      encNumber(buf, off + 124, 12, this.size) || this.needPax\n    this.needPax =\n      encDate(buf, off + 136, 12, this.mtime) || this.needPax\n    buf[off + 156] = this.#type.charCodeAt(0)\n    this.needPax =\n      encString(buf, off + 157, 100, this.linkpath) || this.needPax\n    buf.write('ustar\\u000000', off + 257, 8)\n    this.needPax =\n      encString(buf, off + 265, 32, this.uname) || this.needPax\n    this.needPax =\n      encString(buf, off + 297, 32, this.gname) || this.needPax\n    this.needPax =\n      encNumber(buf, off + 329, 8, this.devmaj) || this.needPax\n    this.needPax =\n      encNumber(buf, off + 337, 8, this.devmin) || this.needPax\n    this.needPax =\n      encString(buf, off + 345, prefixSize, prefix) || this.needPax\n    if (buf[off + 475] !== 0) {\n      this.needPax =\n        encString(buf, off + 345, 155, prefix) || this.needPax\n    } else {\n      this.needPax =\n        encString(buf, off + 345, 130, prefix) || this.needPax\n      this.needPax =\n        encDate(buf, off + 476, 12, this.atime) || this.needPax\n      this.needPax =\n        encDate(buf, off + 488, 12, this.ctime) || this.needPax\n    }\n\n    let sum = 8 * 0x20\n    for (let i = off; i < off + 148; i++) {\n      sum += buf[i] as number\n    }\n\n    for (let i = off + 156; i < off + 512; i++) {\n      sum += buf[i] as number\n    }\n\n    this.cksum = sum\n    encNumber(buf, off + 148, 8, this.cksum)\n    this.cksumValid = true\n\n    return this.needPax\n  }\n\n  get type(): EntryTypeName {\n    return (\n      this.#type === 'Unsupported' ?\n        this.#type\n      : types.name.get(this.#type)) as EntryTypeName\n  }\n\n  get typeKey(): EntryTypeCode | 'Unsupported' {\n    return this.#type\n  }\n\n  set type(type: EntryTypeCode | EntryTypeName | 'Unsupported') {\n    const c = String(types.code.get(type as EntryTypeName))\n    if (types.isCode(c) || c === 'Unsupported') {\n      this.#type = c\n    } else if (types.isCode(type)) {\n      this.#type = type\n    } else {\n      throw new TypeError('invalid entry type: ' + type)\n    }\n  }\n}\n\nconst splitPrefix = (\n  p: string,\n  prefixSize: number,\n): [string, string, boolean] => {\n  const pathSize = 100\n  let pp = p\n  let prefix = ''\n  let ret: undefined | [string, string, boolean] = undefined\n  const root = pathModule.parse(p).root || '.'\n\n  if (Buffer.byteLength(pp) < pathSize) {\n    ret = [pp, prefix, false]\n  } else {\n    // first set prefix to the dir, and path to the base\n    prefix = pathModule.dirname(pp)\n    pp = pathModule.basename(pp)\n\n    do {\n      if (\n        Buffer.byteLength(pp) <= pathSize &&\n        Buffer.byteLength(prefix) <= prefixSize\n      ) {\n        // both fit!\n        ret = [pp, prefix, false]\n      } else if (\n        Buffer.byteLength(pp) > pathSize &&\n        Buffer.byteLength(prefix) <= prefixSize\n      ) {\n        // prefix fits in prefix, but path doesn't fit in path\n        ret = [pp.slice(0, pathSize - 1), prefix, true]\n      } else {\n        // make path take a bit from prefix\n        pp = pathModule.join(pathModule.basename(prefix), pp)\n        prefix = pathModule.dirname(prefix)\n      }\n    } while (prefix !== root && ret === undefined)\n\n    // at this point, found no resolution, just truncate\n    if (!ret) {\n      ret = [p.slice(0, pathSize - 1), '', true]\n    }\n  }\n  return ret\n}\n\nconst decString = (buf: Buffer, off: number, size: number) =>\n  buf\n    .subarray(off, off + size)\n    .toString('utf8')\n    .replace(/\\0.*/, '')\n\nconst decDate = (buf: Buffer, off: number, size: number) =>\n  numToDate(decNumber(buf, off, size))\n\nconst numToDate = (num?: number) =>\n  num === undefined ? undefined : new Date(num * 1000)\n\nconst decNumber = (buf: Buffer, off: number, size: number) =>\n  Number(buf[off]) & 0x80 ?\n    large.parse(buf.subarray(off, off + size))\n  : decSmallNumber(buf, off, size)\n\nconst nanUndef = (value: number) => (isNaN(value) ? undefined : value)\n\nconst decSmallNumber = (buf: Buffer, off: number, size: number) =>\n  nanUndef(\n    parseInt(\n      buf\n        .subarray(off, off + size)\n        .toString('utf8')\n        .replace(/\\0.*$/, '')\n        .trim(),\n      8,\n    ),\n  )\n\n// the maximum encodable as a null-terminated octal, by field size\nconst MAXNUM = {\n  12: 0o77777777777,\n  8: 0o7777777,\n}\n\nconst encNumber = (\n  buf: Buffer,\n  off: number,\n  size: 12 | 8,\n  num?: number,\n) =>\n  num === undefined ? false\n  : num > MAXNUM[size] || num < 0 ?\n    (large.encode(num, buf.subarray(off, off + size)), true)\n  : (encSmallNumber(buf, off, size, num), false)\n\nconst encSmallNumber = (\n  buf: Buffer,\n  off: number,\n  size: number,\n  num: number,\n) => buf.write(octalString(num, size), off, size, 'ascii')\n\nconst octalString = (num: number, size: number) =>\n  padOctal(Math.floor(num).toString(8), size)\n\nconst padOctal = (str: string, size: number) =>\n  (str.length === size - 1 ?\n    str\n  : new Array(size - str.length - 1).join('0') + str + ' ') + '\\0'\n\nconst encDate = (\n  buf: Buffer,\n  off: number,\n  size: 8 | 12,\n  date?: Date,\n) =>\n  date === undefined ? false : (\n    encNumber(buf, off, size, date.getTime() / 1000)\n  )\n\n// enough to fill the longest string we've got\nconst NULLS = new Array(156).join('\\0')\n// pad with nulls, return true if it's longer or non-ascii\nconst encString = (\n  buf: Buffer,\n  off: number,\n  size: number,\n  str?: string,\n) =>\n  str === undefined ? false : (\n    (buf.write(str + NULLS, off, size, 'utf8'),\n    str.length !== Buffer.byteLength(str) || str.length > size)\n  )\n", "import { basename } from 'node:path'\nimport { Header, HeaderData } from './header.js'\n\nexport class Pax implements HeaderData {\n  atime?: Date\n  mtime?: Date\n  ctime?: Date\n\n  charset?: string\n  comment?: string\n\n  gid?: number\n  uid?: number\n\n  gname?: string\n  uname?: string\n  linkpath?: string\n  dev?: number\n  ino?: number\n  nlink?: number\n  path?: string\n  size?: number\n  mode?: number\n\n  global: boolean\n\n  constructor(obj: HeaderData, global: boolean = false) {\n    this.atime = obj.atime\n    this.charset = obj.charset\n    this.comment = obj.comment\n    this.ctime = obj.ctime\n    this.dev = obj.dev\n    this.gid = obj.gid\n    this.global = global\n    this.gname = obj.gname\n    this.ino = obj.ino\n    this.linkpath = obj.linkpath\n    this.mtime = obj.mtime\n    this.nlink = obj.nlink\n    this.path = obj.path\n    this.size = obj.size\n    this.uid = obj.uid\n    this.uname = obj.uname\n  }\n\n  encode() {\n    const body = this.encodeBody()\n    if (body === '') {\n      return Buffer.allocUnsafe(0)\n    }\n\n    const bodyLen = Buffer.byteLength(body)\n    // round up to 512 bytes\n    // add 512 for header\n    const bufLen = 512 * Math.ceil(1 + bodyLen / 512)\n    const buf = Buffer.allocUnsafe(bufLen)\n\n    // 0-fill the header section, it might not hit every field\n    for (let i = 0; i < 512; i++) {\n      buf[i] = 0\n    }\n\n    new Header({\n      // XXX split the path\n      // then the path should be PaxHeader + basename, but less than 99,\n      // prepend with the dirname\n      /* c8 ignore start */\n      path: ('PaxHeader/' + basename(this.path ?? '')).slice(0, 99),\n      /* c8 ignore stop */\n      mode: this.mode || 0o644,\n      uid: this.uid,\n      gid: this.gid,\n      size: bodyLen,\n      mtime: this.mtime,\n      type: this.global ? 'GlobalExtendedHeader' : 'ExtendedHeader',\n      linkpath: '',\n      uname: this.uname || '',\n      gname: this.gname || '',\n      devmaj: 0,\n      devmin: 0,\n      atime: this.atime,\n      ctime: this.ctime,\n    }).encode(buf)\n\n    buf.write(body, 512, bodyLen, 'utf8')\n\n    // null pad after the body\n    for (let i = bodyLen + 512; i < buf.length; i++) {\n      buf[i] = 0\n    }\n\n    return buf\n  }\n\n  encodeBody() {\n    return (\n      this.encodeField('path') +\n      this.encodeField('ctime') +\n      this.encodeField('atime') +\n      this.encodeField('dev') +\n      this.encodeField('ino') +\n      this.encodeField('nlink') +\n      this.encodeField('charset') +\n      this.encodeField('comment') +\n      this.encodeField('gid') +\n      this.encodeField('gname') +\n      this.encodeField('linkpath') +\n      this.encodeField('mtime') +\n      this.encodeField('size') +\n      this.encodeField('uid') +\n      this.encodeField('uname')\n    )\n  }\n\n  encodeField(field: keyof Pax): string {\n    if (this[field] === undefined) {\n      return ''\n    }\n    const r = this[field]\n    const v = r instanceof Date ? r.getTime() / 1000 : r\n    const s =\n      ' ' +\n      (field === 'dev' || field === 'ino' || field === 'nlink' ?\n        'SCHILY.'\n      : '') +\n      field +\n      '=' +\n      v +\n      '\\n'\n    const byteLen = Buffer.byteLength(s)\n    // the digits includes the length of the digits in ascii base-10\n    // so if it's 9 characters, then adding 1 for the 9 makes it 10\n    // which makes it 11 chars.\n    let digits = Math.floor(Math.log(byteLen) / Math.log(10)) + 1\n    if (byteLen + digits >= Math.pow(10, digits)) {\n      digits += 1\n    }\n    const len = digits + byteLen\n    return len + s\n  }\n\n  static parse(str: string, ex?: HeaderData, g: boolean = false) {\n    return new Pax(merge(parseKV(str), ex), g)\n  }\n}\n\nconst merge = (a: HeaderData, b?: HeaderData) =>\n  b ? Object.assign({}, b, a) : a\n\nconst parseKV = (str: string) =>\n  str\n    .replace(/\\n$/, '')\n    .split('\\n')\n    .reduce(parseKVLine, Object.create(null))\n\nconst parseKVLine = (set: Record<string, any>, line: string) => {\n  const n = parseInt(line, 10)\n\n  // XXX Values with \\n in them will fail this.\n  // Refactor to not be a naive line-by-line parse.\n  if (n !== Buffer.byteLength(line) + 1) {\n    return set\n  }\n\n  line = line.slice((n + ' ').length)\n  const kv = line.split('=')\n  const r = kv.shift()\n\n  if (!r) {\n    return set\n  }\n\n  const k = r.replace(/^SCHILY\\.(dev|ino|nlink)/, '$1')\n\n  const v = kv.join('=')\n  set[k] =\n    /^([A-Z]+\\.)?([mac]|birth|creation)time$/.test(k) ?\n      new Date(Number(v) * 1000)\n    : /^[0-9]+$/.test(v) ? +v\n    : v\n  return set\n}\n", "// on windows, either \\ or / are valid directory separators.\n// on unix, \\ is a valid character in filenames.\n// so, on windows, and only on windows, we replace all \\ chars with /,\n// so that we can use / as our one and only directory separator char.\n\nconst platform =\n  process.env.TESTING_TAR_FAKE_PLATFORM || process.platform\n\nexport const normalizeWindowsPath =\n  platform !== 'win32' ?\n    (p: string) => p\n  : (p: string) => p && p.replace(/\\\\/g, '/')\n", "import { Minipass } from 'minipass'\nimport { Header } from './header.js'\nimport { normalizeWindowsPath } from './normalize-windows-path.js'\nimport { Pax } from './pax.js'\nimport { EntryTypeName } from './types.js'\n\nexport class ReadEntry extends Minipass<Buffer, Buffer> {\n  extended?: Pax\n  globalExtended?: Pax\n  header: Header\n  startBlockSize: number\n  blockRemain: number\n  remain: number\n  type: EntryTypeName\n  meta: boolean = false\n  ignore: boolean = false\n  path: string\n  mode?: number\n  uid?: number\n  gid?: number\n  uname?: string\n  gname?: string\n  size: number = 0\n  mtime?: Date\n  atime?: Date\n  ctime?: Date\n  linkpath?: string\n\n  dev?: number\n  ino?: number\n  nlink?: number\n  invalid: boolean = false\n  absolute?: string\n  unsupported: boolean = false\n\n  constructor(header: Header, ex?: Pax, gex?: Pax) {\n    super({})\n    // read entries always start life paused.  this is to avoid the\n    // situation where Minipass's auto-ending empty streams results\n    // in an entry ending before we're ready for it.\n    this.pause()\n    this.extended = ex\n    this.globalExtended = gex\n    this.header = header\n    /* c8 ignore start */\n    this.remain = header.size ?? 0\n    /* c8 ignore stop */\n    this.startBlockSize = 512 * Math.ceil(this.remain / 512)\n    this.blockRemain = this.startBlockSize\n    this.type = header.type\n    switch (this.type) {\n      case 'File':\n      case 'OldFile':\n      case 'Link':\n      case 'SymbolicLink':\n      case 'CharacterDevice':\n      case 'BlockDevice':\n      case 'Directory':\n      case 'FIFO':\n      case 'ContiguousFile':\n      case 'GNUDumpDir':\n        break\n\n      case 'NextFileHasLongLinkpath':\n      case 'NextFileHasLongPath':\n      case 'OldGnuLongPath':\n      case 'GlobalExtendedHeader':\n      case 'ExtendedHeader':\n      case 'OldExtendedHeader':\n        this.meta = true\n        break\n\n      // NOTE: gnutar and bsdtar treat unrecognized types as 'File'\n      // it may be worth doing the same, but with a warning.\n      default:\n        this.ignore = true\n    }\n\n    /* c8 ignore start */\n    if (!header.path) {\n      throw new Error('no path provided for tar.ReadEntry')\n    }\n    /* c8 ignore stop */\n\n    this.path = normalizeWindowsPath(header.path) as string\n    this.mode = header.mode\n    if (this.mode) {\n      this.mode = this.mode & 0o7777\n    }\n    this.uid = header.uid\n    this.gid = header.gid\n    this.uname = header.uname\n    this.gname = header.gname\n    this.size = this.remain\n    this.mtime = header.mtime\n    this.atime = header.atime\n    this.ctime = header.ctime\n    /* c8 ignore start */\n    this.linkpath =\n      header.linkpath ?\n        normalizeWindowsPath(header.linkpath)\n      : undefined\n    /* c8 ignore stop */\n    this.uname = header.uname\n    this.gname = header.gname\n\n    if (ex) {\n      this.#slurp(ex)\n    }\n    if (gex) {\n      this.#slurp(gex, true)\n    }\n  }\n\n  write(data: Buffer) {\n    const writeLen = data.length\n    if (writeLen > this.blockRemain) {\n      throw new Error('writing more to entry than is appropriate')\n    }\n\n    const r = this.remain\n    const br = this.blockRemain\n    this.remain = Math.max(0, r - writeLen)\n    this.blockRemain = Math.max(0, br - writeLen)\n    if (this.ignore) {\n      return true\n    }\n\n    if (r >= writeLen) {\n      return super.write(data)\n    }\n\n    // r < writeLen\n    return super.write(data.subarray(0, r))\n  }\n\n  #slurp(ex: Pax, gex: boolean = false) {\n    if (ex.path) ex.path = normalizeWindowsPath(ex.path)\n    if (ex.linkpath) ex.linkpath = normalizeWindowsPath(ex.linkpath)\n    Object.assign(\n      this,\n      Object.fromEntries(\n        Object.entries(ex).filter(([k, v]) => {\n          // we slurp in everything except for the path attribute in\n          // a global extended header, because that's weird. Also, any\n          // null/undefined values are ignored.\n          return !(\n            v === null ||\n            v === undefined ||\n            (k === 'path' && gex)\n          )\n        }),\n      ),\n    )\n  }\n}\n", "import { type Minipass } from 'minipass'\n\n/** has a warn method */\nexport type Warner = {\n  warn(code: string, message: string | Error, data: any): void\n  file?: string\n  cwd?: string\n  strict?: boolean\n\n  emit(\n    event: 'warn',\n    code: string,\n    message: string,\n    data?: WarnData,\n  ): void\n  emit(event: 'error', error: TarError): void\n}\n\nexport type WarnEvent<T = Buffer> = Minipass.Events<T> & {\n  warn: [code: string, message: string, data: WarnData]\n}\n\nexport type WarnData = {\n  file?: string\n  cwd?: string\n  code?: string\n  tarCode?: string\n  recoverable?: boolean\n  [k: string]: any\n}\n\nexport type TarError = Error & WarnData\n\nexport const warnMethod = (\n  self: Warner,\n  code: string,\n  message: string | Error,\n  data: WarnData = {},\n) => {\n  if (self.file) {\n    data.file = self.file\n  }\n  if (self.cwd) {\n    data.cwd = self.cwd\n  }\n  data.code =\n    (message instanceof Error &&\n      (message as NodeJS.ErrnoException).code) ||\n    code\n  data.tarCode = code\n  if (!self.strict && data.recoverable !== false) {\n    if (message instanceof Error) {\n      data = Object.assign(message, data)\n      message = message.message\n    }\n    self.emit('warn', code, message, data)\n  } else if (message instanceof Error) {\n    self.emit('error', Object.assign(message, data))\n  } else {\n    self.emit(\n      'error',\n      Object.assign(new Error(`${code}: ${message}`), data),\n    )\n  }\n}\n", "// this[BUFFER] is the remainder of a chunk if we're waiting for\n// the full 512 bytes of a header to come in.  We will Buffer.concat()\n// it to the next write(), which is a mem copy, but a small one.\n//\n// this[QUEUE] is a list of entries that haven't been emitted\n// yet this can only get filled up if the user keeps write()ing after\n// a write() returns false, or does a write() with more than one entry\n//\n// We don't buffer chunks, we always parse them and either create an\n// entry, or push it into the active entry.  The ReadEntry class knows\n// to throw data away if .ignore=true\n//\n// Shift entry off the buffer when it emits 'end', and emit 'entry' for\n// the next one in the list.\n//\n// At any time, we're pushing body chunks into the entry at WRITEENTRY,\n// and waiting for 'end' on the entry at READENTRY\n//\n// ignored entries get .resume() called on them straight away\n\nimport { EventEmitter as EE } from 'events'\nimport { BrotliDecompress, Unzip, ZstdDecompress } from 'minizlib'\nimport { Header } from './header.js'\nimport { TarOptions } from './options.js'\nimport { Pax } from './pax.js'\nimport { ReadEntry } from './read-entry.js'\nimport {\n  warnMethod,\n  type WarnData,\n  type Warner,\n} from './warn-method.js'\n\nconst maxMetaEntrySize = 1024 * 1024\nconst gzipHeader = Buffer.from([0x1f, 0x8b])\nconst zstdHeader = Buffer.from([0x28, 0xb5, 0x2f, 0xfd])\nconst ZIP_HEADER_LEN = Math.max(gzipHeader.length, zstdHeader.length)\n\nconst STATE = Symbol('state')\nconst WRITEENTRY = Symbol('writeEntry')\nconst READENTRY = Symbol('readEntry')\nconst NEXTENTRY = Symbol('nextEntry')\nconst PROCESSENTRY = Symbol('processEntry')\nconst EX = Symbol('extendedHeader')\nconst GEX = Symbol('globalExtendedHeader')\nconst META = Symbol('meta')\nconst EMITMETA = Symbol('emitMeta')\nconst BUFFER = Symbol('buffer')\nconst QUEUE = Symbol('queue')\nconst ENDED = Symbol('ended')\nconst EMITTEDEND = Symbol('emittedEnd')\nconst EMIT = Symbol('emit')\nconst UNZIP = Symbol('unzip')\nconst CONSUMECHUNK = Symbol('consumeChunk')\nconst CONSUMECHUNKSUB = Symbol('consumeChunkSub')\nconst CONSUMEBODY = Symbol('consumeBody')\nconst CONSUMEMETA = Symbol('consumeMeta')\nconst CONSUMEHEADER = Symbol('consumeHeader')\nconst CONSUMING = Symbol('consuming')\nconst BUFFERCONCAT = Symbol('bufferConcat')\nconst MAYBEEND = Symbol('maybeEnd')\nconst WRITING = Symbol('writing')\nconst ABORTED = Symbol('aborted')\nconst DONE = Symbol('onDone')\nconst SAW_VALID_ENTRY = Symbol('sawValidEntry')\nconst SAW_NULL_BLOCK = Symbol('sawNullBlock')\nconst SAW_EOF = Symbol('sawEOF')\nconst CLOSESTREAM = Symbol('closeStream')\n\nconst noop = () => true\n\nexport type State = 'begin' | 'header' | 'ignore' | 'meta' | 'body'\n\nexport class Parser extends EE implements Warner {\n  file: string\n  strict: boolean\n  maxMetaEntrySize: number\n  filter: Exclude<TarOptions['filter'], undefined>\n  brotli?: TarOptions['brotli']\n  zstd?: TarOptions['zstd']\n\n  writable: true = true\n  readable: false = false;\n\n  [QUEUE]: (ReadEntry | [string | symbol, any, any])[] = [];\n  [BUFFER]?: Buffer;\n  [READENTRY]?: ReadEntry;\n  [WRITEENTRY]?: ReadEntry;\n  [STATE]: State = 'begin';\n  [META]: string = '';\n  [EX]?: Pax;\n  [GEX]?: Pax;\n  [ENDED]: boolean = false;\n  [UNZIP]?: false | Unzip | BrotliDecompress | ZstdDecompress;\n  [ABORTED]: boolean = false;\n  [SAW_VALID_ENTRY]?: boolean;\n  [SAW_NULL_BLOCK]: boolean = false;\n  [SAW_EOF]: boolean = false;\n  [WRITING]: boolean = false;\n  [CONSUMING]: boolean = false;\n  [EMITTEDEND]: boolean = false\n\n  constructor(opt: TarOptions = {}) {\n    super()\n\n    this.file = opt.file || ''\n\n    // these BADARCHIVE errors can't be detected early. listen on DONE.\n    this.on(DONE, () => {\n      if (\n        this[STATE] === 'begin' ||\n        this[SAW_VALID_ENTRY] === false\n      ) {\n        // either less than 1 block of data, or all entries were invalid.\n        // Either way, probably not even a tarball.\n        this.warn('TAR_BAD_ARCHIVE', 'Unrecognized archive format')\n      }\n    })\n\n    if (opt.ondone) {\n      this.on(DONE, opt.ondone)\n    } else {\n      this.on(DONE, () => {\n        this.emit('prefinish')\n        this.emit('finish')\n        this.emit('end')\n      })\n    }\n\n    this.strict = !!opt.strict\n    this.maxMetaEntrySize = opt.maxMetaEntrySize || maxMetaEntrySize\n    this.filter = typeof opt.filter === 'function' ? opt.filter : noop\n    // Unlike gzip, brotli doesn't have any magic bytes to identify it\n    // Users need to explicitly tell us they're extracting a brotli file\n    // Or we infer from the file extension\n    const isTBR =\n      opt.file &&\n      (opt.file.endsWith('.tar.br') || opt.file.endsWith('.tbr'))\n    // if it's a tbr file it MIGHT be brotli, but we don't know until\n    // we look at it and verify it's not a valid tar file.\n    this.brotli =\n      !(opt.gzip || opt.zstd) && opt.brotli !== undefined ? opt.brotli\n      : isTBR ? undefined\n      : false\n\n    // zstd has magic bytes to identify it, but we also support explicit options\n    // and file extension detection\n    const isTZST =\n      opt.file &&\n      (opt.file.endsWith('.tar.zst') || opt.file.endsWith('.tzst'))\n    this.zstd =\n      !(opt.gzip || opt.brotli) && opt.zstd !== undefined ? opt.zstd\n      : isTZST ? true\n      : undefined\n\n    // have to set this so that streams are ok piping into it\n    this.on('end', () => this[CLOSESTREAM]())\n\n    if (typeof opt.onwarn === 'function') {\n      this.on('warn', opt.onwarn)\n    }\n    if (typeof opt.onReadEntry === 'function') {\n      this.on('entry', opt.onReadEntry)\n    }\n  }\n\n  warn(\n    code: string,\n    message: string | Error,\n    data: WarnData = {},\n  ): void {\n    warnMethod(this, code, message, data)\n  }\n\n  [CONSUMEHEADER](chunk: Buffer, position: number) {\n    if (this[SAW_VALID_ENTRY] === undefined) {\n      this[SAW_VALID_ENTRY] = false\n    }\n    let header\n    try {\n      header = new Header(chunk, position, this[EX], this[GEX])\n    } catch (er) {\n      return this.warn('TAR_ENTRY_INVALID', er as Error)\n    }\n\n    if (header.nullBlock) {\n      if (this[SAW_NULL_BLOCK]) {\n        this[SAW_EOF] = true\n        // ending an archive with no entries.  pointless, but legal.\n        if (this[STATE] === 'begin') {\n          this[STATE] = 'header'\n        }\n        this[EMIT]('eof')\n      } else {\n        this[SAW_NULL_BLOCK] = true\n        this[EMIT]('nullBlock')\n      }\n    } else {\n      this[SAW_NULL_BLOCK] = false\n      if (!header.cksumValid) {\n        this.warn('TAR_ENTRY_INVALID', 'checksum failure', { header })\n      } else if (!header.path) {\n        this.warn('TAR_ENTRY_INVALID', 'path is required', { header })\n      } else {\n        const type = header.type\n        if (/^(Symbolic)?Link$/.test(type) && !header.linkpath) {\n          this.warn('TAR_ENTRY_INVALID', 'linkpath required', {\n            header,\n          })\n        } else if (\n          !/^(Symbolic)?Link$/.test(type) &&\n          !/^(Global)?ExtendedHeader$/.test(type) &&\n          header.linkpath\n        ) {\n          this.warn('TAR_ENTRY_INVALID', 'linkpath forbidden', {\n            header,\n          })\n        } else {\n          const entry = (this[WRITEENTRY] = new ReadEntry(\n            header,\n            this[EX],\n            this[GEX],\n          ))\n\n          // we do this for meta & ignored entries as well, because they\n          // are still valid tar, or else we wouldn't know to ignore them\n          if (!this[SAW_VALID_ENTRY]) {\n            if (entry.remain) {\n              // this might be the one!\n              const onend = () => {\n                if (!entry.invalid) {\n                  this[SAW_VALID_ENTRY] = true\n                }\n              }\n              entry.on('end', onend)\n            } else {\n              this[SAW_VALID_ENTRY] = true\n            }\n          }\n\n          if (entry.meta) {\n            if (entry.size > this.maxMetaEntrySize) {\n              entry.ignore = true\n              this[EMIT]('ignoredEntry', entry)\n              this[STATE] = 'ignore'\n              entry.resume()\n            } else if (entry.size > 0) {\n              this[META] = ''\n              entry.on('data', c => (this[META] += c))\n              this[STATE] = 'meta'\n            }\n          } else {\n            this[EX] = undefined\n            entry.ignore =\n              entry.ignore || !this.filter(entry.path, entry)\n\n            if (entry.ignore) {\n              // probably valid, just not something we care about\n              this[EMIT]('ignoredEntry', entry)\n              this[STATE] = entry.remain ? 'ignore' : 'header'\n              entry.resume()\n            } else {\n              if (entry.remain) {\n                this[STATE] = 'body'\n              } else {\n                this[STATE] = 'header'\n                entry.end()\n              }\n\n              if (!this[READENTRY]) {\n                this[QUEUE].push(entry)\n                this[NEXTENTRY]()\n              } else {\n                this[QUEUE].push(entry)\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n\n  [CLOSESTREAM]() {\n    queueMicrotask(() => this.emit('close'))\n  }\n\n  [PROCESSENTRY](entry?: ReadEntry | [string | symbol, any, any]) {\n    let go = true\n\n    if (!entry) {\n      this[READENTRY] = undefined\n      go = false\n    } else if (Array.isArray(entry)) {\n      const [ev, ...args]: [string | symbol, any, any] = entry\n      this.emit(ev, ...args)\n    } else {\n      this[READENTRY] = entry\n      this.emit('entry', entry)\n      if (!entry.emittedEnd) {\n        entry.on('end', () => this[NEXTENTRY]())\n        go = false\n      }\n    }\n\n    return go\n  }\n\n  [NEXTENTRY]() {\n    do {} while (this[PROCESSENTRY](this[QUEUE].shift()))\n\n    if (!this[QUEUE].length) {\n      // At this point, there's nothing in the queue, but we may have an\n      // entry which is being consumed (readEntry).\n      // If we don't, then we definitely can handle more data.\n      // If we do, and either it's flowing, or it has never had any data\n      // written to it, then it needs more.\n      // The only other possibility is that it has returned false from a\n      // write() call, so we wait for the next drain to continue.\n      const re = this[READENTRY]\n      const drainNow = !re || re.flowing || re.size === re.remain\n      if (drainNow) {\n        if (!this[WRITING]) {\n          this.emit('drain')\n        }\n      } else {\n        re.once('drain', () => this.emit('drain'))\n      }\n    }\n  }\n\n  [CONSUMEBODY](chunk: Buffer, position: number) {\n    // write up to but no  more than writeEntry.blockRemain\n    const entry = this[WRITEENTRY]\n    /* c8 ignore start */\n    if (!entry) {\n      throw new Error('attempt to consume body without entry??')\n    }\n    const br = entry.blockRemain ?? 0\n    /* c8 ignore stop */\n    const c =\n      br >= chunk.length && position === 0 ?\n        chunk\n      : chunk.subarray(position, position + br)\n\n    entry.write(c)\n\n    if (!entry.blockRemain) {\n      this[STATE] = 'header'\n      this[WRITEENTRY] = undefined\n      entry.end()\n    }\n\n    return c.length\n  }\n\n  [CONSUMEMETA](chunk: Buffer, position: number) {\n    const entry = this[WRITEENTRY]\n    const ret = this[CONSUMEBODY](chunk, position)\n\n    // if we finished, then the entry is reset\n    if (!this[WRITEENTRY] && entry) {\n      this[EMITMETA](entry)\n    }\n\n    return ret\n  }\n\n  [EMIT](ev: string | symbol, data?: any, extra?: any) {\n    if (!this[QUEUE].length && !this[READENTRY]) {\n      this.emit(ev, data, extra)\n    } else {\n      this[QUEUE].push([ev, data, extra])\n    }\n  }\n\n  [EMITMETA](entry: ReadEntry) {\n    this[EMIT]('meta', this[META])\n    switch (entry.type) {\n      case 'ExtendedHeader':\n      case 'OldExtendedHeader':\n        this[EX] = Pax.parse(this[META], this[EX], false)\n        break\n\n      case 'GlobalExtendedHeader':\n        this[GEX] = Pax.parse(this[META], this[GEX], true)\n        break\n\n      case 'NextFileHasLongPath':\n      case 'OldGnuLongPath': {\n        const ex = this[EX] ?? Object.create(null)\n        this[EX] = ex\n        ex.path = this[META].replace(/\\0.*/, '')\n        break\n      }\n\n      case 'NextFileHasLongLinkpath': {\n        const ex = this[EX] || Object.create(null)\n        this[EX] = ex\n        ex.linkpath = this[META].replace(/\\0.*/, '')\n        break\n      }\n\n      /* c8 ignore start */\n      default:\n        throw new Error('unknown meta: ' + entry.type)\n      /* c8 ignore stop */\n    }\n  }\n\n  abort(error: Error) {\n    this[ABORTED] = true\n    this.emit('abort', error)\n    // always throws, even in non-strict mode\n    this.warn('TAR_ABORT', error, { recoverable: false })\n  }\n\n  write(\n    buffer: Uint8Array | string,\n    cb?: (err?: Error | null) => void,\n  ): boolean\n  write(\n    str: string,\n    encoding?: BufferEncoding,\n    cb?: (err?: Error | null) => void,\n  ): boolean\n  write(\n    chunk: Buffer | string,\n    encoding?: BufferEncoding | (() => any),\n    cb?: () => any,\n  ): boolean {\n    if (typeof encoding === 'function') {\n      cb = encoding\n      encoding = undefined\n    }\n    if (typeof chunk === 'string') {\n      chunk = Buffer.from(\n        chunk,\n        /* c8 ignore next */\n        typeof encoding === 'string' ? encoding : 'utf8',\n      )\n    }\n    if (this[ABORTED]) {\n      /* c8 ignore next */\n      cb?.()\n      return false\n    }\n\n    // first write, might be gzipped, zstd, or brotli compressed\n    const needSniff =\n      this[UNZIP] === undefined ||\n      (this.brotli === undefined && this[UNZIP] === false)\n    if (needSniff && chunk) {\n      if (this[BUFFER]) {\n        chunk = Buffer.concat([this[BUFFER], chunk])\n        this[BUFFER] = undefined\n      }\n      if (chunk.length < ZIP_HEADER_LEN) {\n        this[BUFFER] = chunk\n        /* c8 ignore next */\n        cb?.()\n        return true\n      }\n\n      // look for gzip header\n      for (\n        let i = 0;\n        this[UNZIP] === undefined && i < gzipHeader.length;\n        i++\n      ) {\n        if (chunk[i] !== gzipHeader[i]) {\n          this[UNZIP] = false\n        }\n      }\n\n      // look for zstd header if gzip header not found\n      let isZstd = false\n      if (this[UNZIP] === false && this.zstd !== false) {\n        isZstd = true\n        for (let i = 0; i < zstdHeader.length; i++) {\n          if (chunk[i] !== zstdHeader[i]) {\n            isZstd = false\n            break\n          }\n        }\n      }\n\n      const maybeBrotli = this.brotli === undefined && !isZstd\n      if (this[UNZIP] === false && maybeBrotli) {\n        // read the first header to see if it's a valid tar file. If so,\n        // we can safely assume that it's not actually brotli, despite the\n        // .tbr or .tar.br file extension.\n        // if we ended before getting a full chunk, yes, def brotli\n        if (chunk.length < 512) {\n          if (this[ENDED]) {\n            this.brotli = true\n          } else {\n            this[BUFFER] = chunk\n            /* c8 ignore next */\n            cb?.()\n            return true\n          }\n        } else {\n          // if it's tar, it's pretty reliably not brotli, chances of\n          // that happening are astronomical.\n          try {\n            new Header(chunk.subarray(0, 512))\n            this.brotli = false\n          } catch (_) {\n            this.brotli = true\n          }\n        }\n      }\n\n      if (\n        this[UNZIP] === undefined ||\n        (this[UNZIP] === false && (this.brotli || isZstd))\n      ) {\n        const ended = this[ENDED]\n        this[ENDED] = false\n        this[UNZIP] =\n          this[UNZIP] === undefined ? new Unzip({})\n          : isZstd ? new ZstdDecompress({})\n          : new BrotliDecompress({})\n        this[UNZIP].on('data', chunk => this[CONSUMECHUNK](chunk))\n        this[UNZIP].on('error', er => this.abort(er as Error))\n        this[UNZIP].on('end', () => {\n          this[ENDED] = true\n          this[CONSUMECHUNK]()\n        })\n        this[WRITING] = true\n        const ret = !!this[UNZIP][ended ? 'end' : 'write'](chunk)\n        this[WRITING] = false\n        cb?.()\n        return ret\n      }\n    }\n\n    this[WRITING] = true\n    if (this[UNZIP]) {\n      this[UNZIP].write(chunk)\n    } else {\n      this[CONSUMECHUNK](chunk)\n    }\n    this[WRITING] = false\n\n    // return false if there's a queue, or if the current entry isn't flowing\n    const ret =\n      this[QUEUE].length ? false\n      : this[READENTRY] ? this[READENTRY].flowing\n      : true\n\n    // if we have no queue, then that means a clogged READENTRY\n    if (!ret && !this[QUEUE].length) {\n      this[READENTRY]?.once('drain', () => this.emit('drain'))\n    }\n\n    /* c8 ignore next */\n    cb?.()\n    return ret\n  }\n\n  [BUFFERCONCAT](c: Buffer) {\n    if (c && !this[ABORTED]) {\n      this[BUFFER] =\n        this[BUFFER] ? Buffer.concat([this[BUFFER], c]) : c\n    }\n  }\n\n  [MAYBEEND]() {\n    if (\n      this[ENDED] &&\n      !this[EMITTEDEND] &&\n      !this[ABORTED] &&\n      !this[CONSUMING]\n    ) {\n      this[EMITTEDEND] = true\n      const entry = this[WRITEENTRY]\n      if (entry && entry.blockRemain) {\n        // truncated, likely a damaged file\n        const have = this[BUFFER] ? this[BUFFER].length : 0\n        this.warn(\n          'TAR_BAD_ARCHIVE',\n          `Truncated input (needed ${entry.blockRemain} more bytes, only ${have} available)`,\n          { entry },\n        )\n        if (this[BUFFER]) {\n          entry.write(this[BUFFER])\n        }\n        entry.end()\n      }\n      this[EMIT](DONE)\n    }\n  }\n\n  [CONSUMECHUNK](chunk?: Buffer) {\n    if (this[CONSUMING] && chunk) {\n      this[BUFFERCONCAT](chunk)\n    } else if (!chunk && !this[BUFFER]) {\n      this[MAYBEEND]()\n    } else if (chunk) {\n      this[CONSUMING] = true\n      if (this[BUFFER]) {\n        this[BUFFERCONCAT](chunk)\n        const c = this[BUFFER]\n        this[BUFFER] = undefined\n        this[CONSUMECHUNKSUB](c)\n      } else {\n        this[CONSUMECHUNKSUB](chunk)\n      }\n\n      while (\n        this[BUFFER] &&\n        (this[BUFFER] as Buffer)?.length >= 512 &&\n        !this[ABORTED] &&\n        !this[SAW_EOF]\n      ) {\n        const c = this[BUFFER]\n        this[BUFFER] = undefined\n        this[CONSUMECHUNKSUB](c)\n      }\n      this[CONSUMING] = false\n    }\n\n    if (!this[BUFFER] || this[ENDED]) {\n      this[MAYBEEND]()\n    }\n  }\n\n  [CONSUMECHUNKSUB](chunk: Buffer) {\n    // we know that we are in CONSUMING mode, so anything written goes into\n    // the buffer.  Advance the position and put any remainder in the buffer.\n    let position = 0\n    const length = chunk.length\n    while (\n      position + 512 <= length &&\n      !this[ABORTED] &&\n      !this[SAW_EOF]\n    ) {\n      switch (this[STATE]) {\n        case 'begin':\n        case 'header':\n          this[CONSUMEHEADER](chunk, position)\n          position += 512\n          break\n\n        case 'ignore':\n        case 'body':\n          position += this[CONSUMEBODY](chunk, position)\n          break\n\n        case 'meta':\n          position += this[CONSUMEMETA](chunk, position)\n          break\n\n        /* c8 ignore start */\n        default:\n          throw new Error('invalid state: ' + this[STATE])\n        /* c8 ignore stop */\n      }\n    }\n\n    if (position < length) {\n      if (this[BUFFER]) {\n        this[BUFFER] = Buffer.concat([\n          chunk.subarray(position),\n          this[BUFFER],\n        ])\n      } else {\n        this[BUFFER] = chunk.subarray(position)\n      }\n    }\n  }\n\n  end(cb?: () => void): this\n  end(data: string | Buffer, cb?: () => void): this\n  end(str: string, encoding?: BufferEncoding, cb?: () => void): this\n  end(\n    chunk?: string | Buffer | (() => void),\n    encoding?: BufferEncoding | (() => void),\n    cb?: () => void,\n  ) {\n    if (typeof chunk === 'function') {\n      cb = chunk\n      encoding = undefined\n      chunk = undefined\n    }\n    if (typeof encoding === 'function') {\n      cb = encoding\n      encoding = undefined\n    }\n    if (typeof chunk === 'string') {\n      chunk = Buffer.from(chunk, encoding)\n    }\n    if (cb) this.once('finish', cb)\n    if (!this[ABORTED]) {\n      if (this[UNZIP]) {\n        /* c8 ignore start */\n        if (chunk) this[UNZIP].write(chunk)\n        /* c8 ignore stop */\n        this[UNZIP].end()\n      } else {\n        this[ENDED] = true\n        if (this.brotli === undefined || this.zstd === undefined)\n          chunk = chunk || Buffer.alloc(0)\n        if (chunk) this.write(chunk)\n        this[MAYBEEND]()\n      }\n    }\n    return this\n  }\n}\n", "// warning: extremely hot code path.\n// This has been meticulously optimized for use\n// within npm install on large package trees.\n// Do not edit without careful benchmarking.\nexport const stripTrailingSlashes = (str: string) => {\n  let i = str.length - 1\n  let slashesStart = -1\n  while (i > -1 && str.charAt(i) === '/') {\n    slashesStart = i\n    i--\n  }\n  return slashesStart === -1 ? str : str.slice(0, slashesStart)\n}\n", "// tar -t\nimport * as fsm from '@isaacs/fs-minipass'\nimport fs from 'node:fs'\nimport { dirname, parse } from 'path'\nimport { makeCommand } from './make-command.js'\nimport {\n  TarOptions,\n  TarOptionsFile,\n  TarOptionsSyncFile,\n} from './options.js'\nimport { Parser } from './parse.js'\nimport { stripTrailingSlashes } from './strip-trailing-slashes.js'\n\nconst onReadEntryFunction = (opt: TarOptions) => {\n  const onReadEntry = opt.onReadEntry\n  opt.onReadEntry =\n    onReadEntry ?\n      e => {\n        onReadEntry(e)\n        e.resume()\n      }\n    : e => e.resume()\n}\n\n// construct a filter that limits the file entries listed\n// include child entries if a dir is included\nexport const filesFilter = (opt: TarOptions, files: string[]) => {\n  const map = new Map<string, boolean>(\n    files.map(f => [stripTrailingSlashes(f), true]),\n  )\n  const filter = opt.filter\n\n  const mapHas = (file: string, r: string = ''): boolean => {\n    const root = r || parse(file).root || '.'\n    let ret: boolean\n    if (file === root) ret = false\n    else {\n      const m = map.get(file)\n      if (m !== undefined) {\n        ret = m\n      } else {\n        ret = mapHas(dirname(file), root)\n      }\n    }\n\n    map.set(file, ret)\n    return ret\n  }\n\n  opt.filter =\n    filter ?\n      (file, entry) =>\n        filter(file, entry) && mapHas(stripTrailingSlashes(file))\n    : file => mapHas(stripTrailingSlashes(file))\n}\n\nconst listFileSync = (opt: TarOptionsSyncFile) => {\n  const p = new Parser(opt)\n  const file = opt.file\n  let fd: number | undefined\n  try {\n    fd = fs.openSync(file, 'r')\n    const stat: fs.Stats = fs.fstatSync(fd)\n    const readSize: number = opt.maxReadSize || 16 * 1024 * 1024\n    if (stat.size < readSize) {\n      const buf = Buffer.allocUnsafe(stat.size)\n      const read = fs.readSync(fd, buf, 0, stat.size, 0)\n      p.end(read === buf.byteLength ? buf : buf.subarray(0, read))\n    } else {\n      let pos = 0\n      const buf = Buffer.allocUnsafe(readSize)\n      while (pos < stat.size) {\n        const bytesRead = fs.readSync(fd, buf, 0, readSize, pos)\n        if (bytesRead === 0) break\n        pos += bytesRead\n        p.write(buf.subarray(0, bytesRead))\n      }\n      p.end()\n    }\n  } finally {\n    if (typeof fd === 'number') {\n      try {\n        fs.closeSync(fd)\n        /* c8 ignore next */\n      } catch (er) {}\n    }\n  }\n}\n\nconst listFile = (\n  opt: TarOptionsFile,\n  _files: string[],\n): Promise<void> => {\n  const parse = new Parser(opt)\n  const readSize = opt.maxReadSize || 16 * 1024 * 1024\n\n  const file = opt.file\n  const p = new Promise<void>((resolve, reject) => {\n    parse.on('error', reject)\n    parse.on('end', resolve)\n\n    fs.stat(file, (er, stat) => {\n      if (er) {\n        reject(er)\n      } else {\n        const stream = new fsm.ReadStream(file, {\n          readSize: readSize,\n          size: stat.size,\n        })\n        stream.on('error', reject)\n        stream.pipe(parse)\n      }\n    })\n  })\n  return p\n}\n\nexport const list = makeCommand(\n  listFileSync,\n  listFile,\n  opt => new Parser(opt) as Parser & { sync: true },\n  opt => new Parser(opt),\n  (opt, files) => {\n    if (files?.length) filesFilter(opt, files)\n    if (!opt.noResume) onReadEntryFunction(opt)\n  },\n)\n", "export const modeFix = (\n  mode: number,\n  isDir: boolean,\n  portable: boolean,\n) => {\n  mode &= 0o7777\n\n  // in portable mode, use the minimum reasonable umask\n  // if this system creates files with 0o664 by default\n  // (as some linux distros do), then we'll write the\n  // archive with 0o644 instead.  Also, don't ever create\n  // a file that is not readable/writable by the owner.\n  if (portable) {\n    mode = (mode | 0o600) & ~0o22\n  }\n\n  // if dirs are readable, then they should be listable\n  if (isDir) {\n    if (mode & 0o400) {\n      mode |= 0o100\n    }\n    if (mode & 0o40) {\n      mode |= 0o10\n    }\n    if (mode & 0o4) {\n      mode |= 0o1\n    }\n  }\n  return mode\n}\n", "// unix absolute paths are also absolute on win32, so we use this for both\nimport { win32 } from 'node:path'\nconst { isAbsolute, parse } = win32\n\n// returns [root, stripped]\n// Note that windows will think that //x/y/z/a has a \"root\" of //x/y, and in\n// those cases, we want to sanitize it to x/y/z/a, not z/a, so we strip /\n// explicitly if it's the first character.\n// drive-specific relative paths on Windows get their root stripped off even\n// though they are not absolute, so `c:../foo` becomes ['c:', '../foo']\nexport const stripAbsolutePath = (path: string) => {\n  let r = ''\n\n  let parsed = parse(path)\n  while (isAbsolute(path) || parsed.root) {\n    // windows will think that //x/y/z has a \"root\" of //x/y/\n    // but strip the //?/C:/ off of //?/C:/path\n    const root =\n      path.charAt(0) === '/' && path.slice(0, 4) !== '//?/' ?\n        '/'\n      : parsed.root\n    path = path.slice(root.length)\n    r += root\n    parsed = parse(path)\n  }\n  return [r, path]\n}\n", "// When writing files on Windows, translate the characters to their\n// 0xf000 higher-encoded versions.\n\nconst raw = ['|', '<', '>', '?', ':']\n\nconst win = raw.map(char =>\n  String.fromCharCode(0xf000 + char.charCodeAt(0)),\n)\n\nconst toWin = new Map(raw.map((char, i) => [char, win[i]]))\nconst toRaw = new Map(win.map((char, i) => [char, raw[i]]))\n\nexport const encode = (s: string) =>\n  raw.reduce((s, c) => s.split(c).join(toWin.get(c)), s)\nexport const decode = (s: string) =>\n  win.reduce((s, c) => s.split(c).join(toRaw.get(c)), s)\n", "import fs, { type Stats } from 'fs'\nimport { Minipass } from 'minipass'\nimport path from 'path'\nimport { Header } from './header.js'\nimport { modeFix } from './mode-fix.js'\nimport { normalizeWindowsPath } from './normalize-windows-path.js'\nimport {\n  dealias,\n  LinkCacheKey,\n  TarOptions,\n  TarOptionsWithAliases,\n} from './options.js'\nimport { Pax } from './pax.js'\nimport { ReadEntry } from './read-entry.js'\nimport { stripAbsolutePath } from './strip-absolute-path.js'\nimport { stripTrailingSlashes } from './strip-trailing-slashes.js'\nimport { EntryTypeName } from './types.js'\nimport {\n  WarnData,\n  Warner,\n  WarnEvent,\n  warnMethod,\n} from './warn-method.js'\nimport * as winchars from './winchars.js'\n\nconst prefixPath = (path: string, prefix?: string) => {\n  if (!prefix) {\n    return normalizeWindowsPath(path)\n  }\n  path = normalizeWindowsPath(path).replace(/^\\.(\\/|$)/, '')\n  return stripTrailingSlashes(prefix) + '/' + path\n}\n\nconst maxReadSize = 16 * 1024 * 1024\n\nconst PROCESS = Symbol('process')\nconst FILE = Symbol('file')\nconst DIRECTORY = Symbol('directory')\nconst SYMLINK = Symbol('symlink')\nconst HARDLINK = Symbol('hardlink')\nconst HEADER = Symbol('header')\nconst READ = Symbol('read')\nconst LSTAT = Symbol('lstat')\nconst ONLSTAT = Symbol('onlstat')\nconst ONREAD = Symbol('onread')\nconst ONREADLINK = Symbol('onreadlink')\nconst OPENFILE = Symbol('openfile')\nconst ONOPENFILE = Symbol('onopenfile')\nconst CLOSE = Symbol('close')\nconst MODE = Symbol('mode')\nconst AWAITDRAIN = Symbol('awaitDrain')\nconst ONDRAIN = Symbol('ondrain')\nconst PREFIX = Symbol('prefix')\n\nexport class WriteEntry\n  extends Minipass<Buffer, Minipass.ContiguousData, WarnEvent>\n  implements Warner\n{\n  path: string\n  portable: boolean\n  myuid: number = (process.getuid && process.getuid()) || 0\n  // until node has builtin pwnam functions, this'll have to do\n  myuser: string = process.env.USER || ''\n  maxReadSize: number\n  linkCache: Exclude<TarOptions['linkCache'], undefined>\n  statCache: Exclude<TarOptions['statCache'], undefined>\n  preservePaths: boolean\n  cwd: string\n  strict: boolean\n  mtime?: Date\n  noPax: boolean\n  noMtime: boolean\n  prefix?: string\n  fd?: number\n\n  blockLen: number = 0\n  blockRemain: number = 0\n  buf?: Buffer\n  pos: number = 0\n  remain: number = 0\n  length: number = 0\n  offset: number = 0\n\n  win32: boolean\n  absolute: string\n\n  header?: Header\n  type?: EntryTypeName | 'Unsupported'\n  linkpath?: string\n  stat?: Stats\n  onWriteEntry?: (entry: WriteEntry) => any\n\n  #hadError: boolean = false\n\n  constructor(p: string, opt_: TarOptionsWithAliases = {}) {\n    const opt = dealias(opt_)\n    super()\n    this.path = normalizeWindowsPath(p)\n    // suppress atime, ctime, uid, gid, uname, gname\n    this.portable = !!opt.portable\n    this.maxReadSize = opt.maxReadSize || maxReadSize\n    this.linkCache = opt.linkCache || new Map()\n    this.statCache = opt.statCache || new Map()\n    this.preservePaths = !!opt.preservePaths\n    this.cwd = normalizeWindowsPath(opt.cwd || process.cwd())\n    this.strict = !!opt.strict\n    this.noPax = !!opt.noPax\n    this.noMtime = !!opt.noMtime\n    this.mtime = opt.mtime\n    this.prefix =\n      opt.prefix ? normalizeWindowsPath(opt.prefix) : undefined\n    this.onWriteEntry = opt.onWriteEntry\n\n    if (typeof opt.onwarn === 'function') {\n      this.on('warn', opt.onwarn)\n    }\n\n    let pathWarn: string | boolean = false\n    if (!this.preservePaths) {\n      const [root, stripped] = stripAbsolutePath(this.path)\n      if (root && typeof stripped === 'string') {\n        this.path = stripped\n        pathWarn = root\n      }\n    }\n\n    this.win32 = !!opt.win32 || process.platform === 'win32'\n    if (this.win32) {\n      // force the \\ to / normalization, since we might not *actually*\n      // be on windows, but want \\ to be considered a path separator.\n      this.path = winchars.decode(this.path.replace(/\\\\/g, '/'))\n      p = p.replace(/\\\\/g, '/')\n    }\n\n    this.absolute = normalizeWindowsPath(\n      opt.absolute || path.resolve(this.cwd, p),\n    )\n\n    if (this.path === '') {\n      this.path = './'\n    }\n\n    if (pathWarn) {\n      this.warn(\n        'TAR_ENTRY_INFO',\n        `stripping ${pathWarn} from absolute path`,\n        {\n          entry: this,\n          path: pathWarn + this.path,\n        },\n      )\n    }\n\n    const cs = this.statCache.get(this.absolute)\n    if (cs) {\n      this[ONLSTAT](cs)\n    } else {\n      this[LSTAT]()\n    }\n  }\n\n  warn(code: string, message: string | Error, data: WarnData = {}) {\n    return warnMethod(this, code, message, data)\n  }\n\n  emit(ev: keyof WarnEvent, ...data: any[]) {\n    if (ev === 'error') {\n      this.#hadError = true\n    }\n    return super.emit(ev, ...data)\n  }\n\n  [LSTAT]() {\n    fs.lstat(this.absolute, (er, stat) => {\n      if (er) {\n        return this.emit('error', er)\n      }\n      this[ONLSTAT](stat)\n    })\n  }\n\n  [ONLSTAT](stat: Stats) {\n    this.statCache.set(this.absolute, stat)\n    this.stat = stat\n    if (!stat.isFile()) {\n      stat.size = 0\n    }\n    this.type = getType(stat)\n    this.emit('stat', stat)\n    this[PROCESS]()\n  }\n\n  [PROCESS]() {\n    switch (this.type) {\n      case 'File':\n        return this[FILE]()\n      case 'Directory':\n        return this[DIRECTORY]()\n      case 'SymbolicLink':\n        return this[SYMLINK]()\n      // unsupported types are ignored.\n      default:\n        return this.end()\n    }\n  }\n\n  [MODE](mode: number) {\n    return modeFix(mode, this.type === 'Directory', this.portable)\n  }\n\n  [PREFIX](path: string) {\n    return prefixPath(path, this.prefix)\n  }\n\n  [HEADER]() {\n    /* c8 ignore start */\n    if (!this.stat) {\n      throw new Error('cannot write header before stat')\n    }\n    /* c8 ignore stop */\n\n    if (this.type === 'Directory' && this.portable) {\n      this.noMtime = true\n    }\n\n    this.onWriteEntry?.(this)\n    this.header = new Header({\n      path: this[PREFIX](this.path),\n      // only apply the prefix to hard links.\n      linkpath:\n        this.type === 'Link' && this.linkpath !== undefined ?\n          this[PREFIX](this.linkpath)\n        : this.linkpath,\n      // only the permissions and setuid/setgid/sticky bitflags\n      // not the higher-order bits that specify file type\n      mode: this[MODE](this.stat.mode),\n      uid: this.portable ? undefined : this.stat.uid,\n      gid: this.portable ? undefined : this.stat.gid,\n      size: this.stat.size,\n      mtime: this.noMtime ? undefined : this.mtime || this.stat.mtime,\n      /* c8 ignore next */\n      type: this.type === 'Unsupported' ? undefined : this.type,\n      uname:\n        this.portable ? undefined\n        : this.stat.uid === this.myuid ? this.myuser\n        : '',\n      atime: this.portable ? undefined : this.stat.atime,\n      ctime: this.portable ? undefined : this.stat.ctime,\n    })\n\n    if (this.header.encode() && !this.noPax) {\n      super.write(\n        new Pax({\n          atime: this.portable ? undefined : this.header.atime,\n          ctime: this.portable ? undefined : this.header.ctime,\n          gid: this.portable ? undefined : this.header.gid,\n          mtime:\n            this.noMtime ? undefined : (\n              this.mtime || this.header.mtime\n            ),\n          path: this[PREFIX](this.path),\n          linkpath:\n            this.type === 'Link' && this.linkpath !== undefined ?\n              this[PREFIX](this.linkpath)\n            : this.linkpath,\n          size: this.header.size,\n          uid: this.portable ? undefined : this.header.uid,\n          uname: this.portable ? undefined : this.header.uname,\n          dev: this.portable ? undefined : this.stat.dev,\n          ino: this.portable ? undefined : this.stat.ino,\n          nlink: this.portable ? undefined : this.stat.nlink,\n        }).encode(),\n      )\n    }\n    const block = this.header?.block\n    /* c8 ignore start */\n    if (!block) {\n      throw new Error('failed to encode header')\n    }\n    /* c8 ignore stop */\n    super.write(block)\n  }\n\n  [DIRECTORY]() {\n    /* c8 ignore start */\n    if (!this.stat) {\n      throw new Error('cannot create directory entry without stat')\n    }\n    /* c8 ignore stop */\n    if (this.path.slice(-1) !== '/') {\n      this.path += '/'\n    }\n    this.stat.size = 0\n    this[HEADER]()\n    this.end()\n  }\n\n  [SYMLINK]() {\n    fs.readlink(this.absolute, (er, linkpath) => {\n      if (er) {\n        return this.emit('error', er)\n      }\n      this[ONREADLINK](linkpath)\n    })\n  }\n\n  [ONREADLINK](linkpath: string) {\n    this.linkpath = normalizeWindowsPath(linkpath)\n    this[HEADER]()\n    this.end()\n  }\n\n  [HARDLINK](linkpath: string) {\n    /* c8 ignore start */\n    if (!this.stat) {\n      throw new Error('cannot create link entry without stat')\n    }\n    /* c8 ignore stop */\n    this.type = 'Link'\n    this.linkpath = normalizeWindowsPath(\n      path.relative(this.cwd, linkpath),\n    )\n    this.stat.size = 0\n    this[HEADER]()\n    this.end()\n  }\n\n  [FILE]() {\n    /* c8 ignore start */\n    if (!this.stat) {\n      throw new Error('cannot create file entry without stat')\n    }\n    /* c8 ignore stop */\n    if (this.stat.nlink > 1) {\n      const linkKey =\n        `${this.stat.dev}:${this.stat.ino}` as LinkCacheKey\n      const linkpath = this.linkCache.get(linkKey)\n      if (linkpath?.indexOf(this.cwd) === 0) {\n        return this[HARDLINK](linkpath)\n      }\n      this.linkCache.set(linkKey, this.absolute)\n    }\n\n    this[HEADER]()\n    if (this.stat.size === 0) {\n      return this.end()\n    }\n\n    this[OPENFILE]()\n  }\n\n  [OPENFILE]() {\n    fs.open(this.absolute, 'r', (er, fd) => {\n      if (er) {\n        return this.emit('error', er)\n      }\n      this[ONOPENFILE](fd)\n    })\n  }\n\n  [ONOPENFILE](fd: number) {\n    this.fd = fd\n    if (this.#hadError) {\n      return this[CLOSE]()\n    }\n    /* c8 ignore start */\n    if (!this.stat) {\n      throw new Error('should stat before calling onopenfile')\n    }\n    /* c8 ignore start */\n\n    this.blockLen = 512 * Math.ceil(this.stat.size / 512)\n    this.blockRemain = this.blockLen\n    const bufLen = Math.min(this.blockLen, this.maxReadSize)\n    this.buf = Buffer.allocUnsafe(bufLen)\n    this.offset = 0\n    this.pos = 0\n    this.remain = this.stat.size\n    this.length = this.buf.length\n    this[READ]()\n  }\n\n  [READ]() {\n    const { fd, buf, offset, length, pos } = this\n    if (fd === undefined || buf === undefined) {\n      throw new Error('cannot read file without first opening')\n    }\n    fs.read(fd, buf, offset, length, pos, (er, bytesRead) => {\n      if (er) {\n        // ignoring the error from close(2) is a bad practice, but at\n        // this point we already have an error, don't need another one\n        return this[CLOSE](() => this.emit('error', er))\n      }\n      this[ONREAD](bytesRead)\n    })\n  }\n\n  /* c8 ignore start */\n  [CLOSE](\n    cb: (er?: null | Error | NodeJS.ErrnoException) => any = () => {},\n  ) {\n    /* c8 ignore stop */\n    if (this.fd !== undefined) fs.close(this.fd, cb)\n  }\n\n  [ONREAD](bytesRead: number) {\n    if (bytesRead <= 0 && this.remain > 0) {\n      const er = Object.assign(\n        new Error('encountered unexpected EOF'),\n        {\n          path: this.absolute,\n          syscall: 'read',\n          code: 'EOF',\n        },\n      )\n      return this[CLOSE](() => this.emit('error', er))\n    }\n\n    if (bytesRead > this.remain) {\n      const er = Object.assign(\n        new Error('did not encounter expected EOF'),\n        {\n          path: this.absolute,\n          syscall: 'read',\n          code: 'EOF',\n        },\n      )\n      return this[CLOSE](() => this.emit('error', er))\n    }\n\n    /* c8 ignore start */\n    if (!this.buf) {\n      throw new Error('should have created buffer prior to reading')\n    }\n    /* c8 ignore stop */\n\n    // null out the rest of the buffer, if we could fit the block padding\n    // at the end of this loop, we've incremented bytesRead and this.remain\n    // to be incremented up to the blockRemain level, as if we had expected\n    // to get a null-padded file, and read it until the end.  then we will\n    // decrement both remain and blockRemain by bytesRead, and know that we\n    // reached the expected EOF, without any null buffer to append.\n    if (bytesRead === this.remain) {\n      for (\n        let i = bytesRead;\n        i < this.length && bytesRead < this.blockRemain;\n        i++\n      ) {\n        this.buf[i + this.offset] = 0\n        bytesRead++\n        this.remain++\n      }\n    }\n\n    const chunk =\n      this.offset === 0 && bytesRead === this.buf.length ?\n        this.buf\n      : this.buf.subarray(this.offset, this.offset + bytesRead)\n\n    const flushed = this.write(chunk)\n    if (!flushed) {\n      this[AWAITDRAIN](() => this[ONDRAIN]())\n    } else {\n      this[ONDRAIN]()\n    }\n  }\n\n  [AWAITDRAIN](cb: () => any) {\n    this.once('drain', cb)\n  }\n\n  write(buffer: Buffer | string, cb?: () => void): boolean\n  write(\n    str: Buffer | string,\n    encoding?: BufferEncoding | null,\n    cb?: () => void,\n  ): boolean\n  write(\n    chunk: Buffer | string,\n    encoding?: BufferEncoding | (() => any) | null,\n    cb?: () => any,\n  ): boolean {\n    /* c8 ignore start - just junk to comply with NodeJS.WritableStream */\n    if (typeof encoding === 'function') {\n      cb = encoding\n      encoding = undefined\n    }\n    if (typeof chunk === 'string') {\n      chunk = Buffer.from(\n        chunk,\n        typeof encoding === 'string' ? encoding : 'utf8',\n      )\n    }\n    /* c8 ignore stop */\n\n    if (this.blockRemain < chunk.length) {\n      const er = Object.assign(\n        new Error('writing more data than expected'),\n        {\n          path: this.absolute,\n        },\n      )\n      return this.emit('error', er)\n    }\n    this.remain -= chunk.length\n    this.blockRemain -= chunk.length\n    this.pos += chunk.length\n    this.offset += chunk.length\n    return super.write(chunk, null, cb)\n  }\n\n  [ONDRAIN]() {\n    if (!this.remain) {\n      if (this.blockRemain) {\n        super.write(Buffer.alloc(this.blockRemain))\n      }\n      return this[CLOSE](er =>\n        er ? this.emit('error', er) : this.end(),\n      )\n    }\n\n    /* c8 ignore start */\n    if (!this.buf) {\n      throw new Error('buffer lost somehow in ONDRAIN')\n    }\n    /* c8 ignore stop */\n\n    if (this.offset >= this.length) {\n      // if we only have a smaller bit left to read, alloc a smaller buffer\n      // otherwise, keep it the same length it was before.\n      this.buf = Buffer.allocUnsafe(\n        Math.min(this.blockRemain, this.buf.length),\n      )\n      this.offset = 0\n    }\n    this.length = this.buf.length - this.offset\n    this[READ]()\n  }\n}\n\nexport class WriteEntrySync extends WriteEntry implements Warner {\n  sync: true = true;\n\n  [LSTAT]() {\n    this[ONLSTAT](fs.lstatSync(this.absolute))\n  }\n\n  [SYMLINK]() {\n    this[ONREADLINK](fs.readlinkSync(this.absolute))\n  }\n\n  [OPENFILE]() {\n    this[ONOPENFILE](fs.openSync(this.absolute, 'r'))\n  }\n\n  [READ]() {\n    let threw = true\n    try {\n      const { fd, buf, offset, length, pos } = this\n      /* c8 ignore start */\n      if (fd === undefined || buf === undefined) {\n        throw new Error('fd and buf must be set in READ method')\n      }\n      /* c8 ignore stop */\n      const bytesRead = fs.readSync(fd, buf, offset, length, pos)\n      this[ONREAD](bytesRead)\n      threw = false\n    } finally {\n      // ignoring the error from close(2) is a bad practice, but at\n      // this point we already have an error, don't need another one\n      if (threw) {\n        try {\n          this[CLOSE](() => {})\n        } catch (er) {}\n      }\n    }\n  }\n\n  [AWAITDRAIN](cb: () => any) {\n    cb()\n  }\n\n  /* c8 ignore start */\n  [CLOSE](\n    cb: (er?: null | Error | NodeJS.ErrnoException) => any = () => {},\n  ) {\n    /* c8 ignore stop */\n    if (this.fd !== undefined) fs.closeSync(this.fd)\n    cb()\n  }\n}\n\nexport class WriteEntryTar\n  extends Minipass<Buffer, Buffer | string, WarnEvent>\n  implements Warner\n{\n  blockLen: number = 0\n  blockRemain: number = 0\n  buf: number = 0\n  pos: number = 0\n  remain: number = 0\n  length: number = 0\n  preservePaths: boolean\n  portable: boolean\n  strict: boolean\n  noPax: boolean\n  noMtime: boolean\n  readEntry: ReadEntry\n  type: EntryTypeName\n  prefix?: string\n  path: string\n  mode?: number\n  uid?: number\n  gid?: number\n  uname?: string\n  gname?: string\n  header?: Header\n  mtime?: Date\n  atime?: Date\n  ctime?: Date\n  linkpath?: string\n  size: number\n  onWriteEntry?: (entry: WriteEntry) => any\n\n  warn(code: string, message: string | Error, data: WarnData = {}) {\n    return warnMethod(this, code, message, data)\n  }\n\n  constructor(\n    readEntry: ReadEntry,\n    opt_: TarOptionsWithAliases = {},\n  ) {\n    const opt = dealias(opt_)\n    super()\n    this.preservePaths = !!opt.preservePaths\n    this.portable = !!opt.portable\n    this.strict = !!opt.strict\n    this.noPax = !!opt.noPax\n    this.noMtime = !!opt.noMtime\n    this.onWriteEntry = opt.onWriteEntry\n\n    this.readEntry = readEntry\n    const { type } = readEntry\n    /* c8 ignore start */\n    if (type === 'Unsupported') {\n      throw new Error('writing entry that should be ignored')\n    }\n    /* c8 ignore stop */\n    this.type = type\n    if (this.type === 'Directory' && this.portable) {\n      this.noMtime = true\n    }\n\n    this.prefix = opt.prefix\n\n    this.path = normalizeWindowsPath(readEntry.path)\n    this.mode =\n      readEntry.mode !== undefined ?\n        this[MODE](readEntry.mode)\n      : undefined\n    this.uid = this.portable ? undefined : readEntry.uid\n    this.gid = this.portable ? undefined : readEntry.gid\n    this.uname = this.portable ? undefined : readEntry.uname\n    this.gname = this.portable ? undefined : readEntry.gname\n    this.size = readEntry.size\n    this.mtime =\n      this.noMtime ? undefined : opt.mtime || readEntry.mtime\n    this.atime = this.portable ? undefined : readEntry.atime\n    this.ctime = this.portable ? undefined : readEntry.ctime\n    this.linkpath =\n      readEntry.linkpath !== undefined ?\n        normalizeWindowsPath(readEntry.linkpath)\n      : undefined\n\n    if (typeof opt.onwarn === 'function') {\n      this.on('warn', opt.onwarn)\n    }\n\n    let pathWarn: false | string = false\n    if (!this.preservePaths) {\n      const [root, stripped] = stripAbsolutePath(this.path)\n      if (root && typeof stripped === 'string') {\n        this.path = stripped\n        pathWarn = root\n      }\n    }\n\n    this.remain = readEntry.size\n    this.blockRemain = readEntry.startBlockSize\n\n    this.onWriteEntry?.(this as unknown as WriteEntry)\n    this.header = new Header({\n      path: this[PREFIX](this.path),\n      linkpath:\n        this.type === 'Link' && this.linkpath !== undefined ?\n          this[PREFIX](this.linkpath)\n        : this.linkpath,\n      // only the permissions and setuid/setgid/sticky bitflags\n      // not the higher-order bits that specify file type\n      mode: this.mode,\n      uid: this.portable ? undefined : this.uid,\n      gid: this.portable ? undefined : this.gid,\n      size: this.size,\n      mtime: this.noMtime ? undefined : this.mtime,\n      type: this.type,\n      uname: this.portable ? undefined : this.uname,\n      atime: this.portable ? undefined : this.atime,\n      ctime: this.portable ? undefined : this.ctime,\n    })\n\n    if (pathWarn) {\n      this.warn(\n        'TAR_ENTRY_INFO',\n        `stripping ${pathWarn} from absolute path`,\n        {\n          entry: this,\n          path: pathWarn + this.path,\n        },\n      )\n    }\n\n    if (this.header.encode() && !this.noPax) {\n      super.write(\n        new Pax({\n          atime: this.portable ? undefined : this.atime,\n          ctime: this.portable ? undefined : this.ctime,\n          gid: this.portable ? undefined : this.gid,\n          mtime: this.noMtime ? undefined : this.mtime,\n          path: this[PREFIX](this.path),\n          linkpath:\n            this.type === 'Link' && this.linkpath !== undefined ?\n              this[PREFIX](this.linkpath)\n            : this.linkpath,\n          size: this.size,\n          uid: this.portable ? undefined : this.uid,\n          uname: this.portable ? undefined : this.uname,\n          dev: this.portable ? undefined : this.readEntry.dev,\n          ino: this.portable ? undefined : this.readEntry.ino,\n          nlink: this.portable ? undefined : this.readEntry.nlink,\n        }).encode(),\n      )\n    }\n\n    const b = this.header?.block\n    /* c8 ignore start */\n    if (!b) throw new Error('failed to encode header')\n    /* c8 ignore stop */\n    super.write(b)\n    readEntry.pipe(this)\n  }\n\n  [PREFIX](path: string) {\n    return prefixPath(path, this.prefix)\n  }\n\n  [MODE](mode: number) {\n    return modeFix(mode, this.type === 'Directory', this.portable)\n  }\n\n  write(buffer: Buffer | string, cb?: () => void): boolean\n  write(\n    str: Buffer | string,\n    encoding?: BufferEncoding | null,\n    cb?: () => void,\n  ): boolean\n  write(\n    chunk: Buffer | string,\n    encoding?: BufferEncoding | (() => any) | null,\n    cb?: () => any,\n  ): boolean {\n    /* c8 ignore start - just junk to comply with NodeJS.WritableStream */\n    if (typeof encoding === 'function') {\n      cb = encoding\n      encoding = undefined\n    }\n    if (typeof chunk === 'string') {\n      chunk = Buffer.from(\n        chunk,\n        typeof encoding === 'string' ? encoding : 'utf8',\n      )\n    }\n    /* c8 ignore stop */\n    const writeLen = chunk.length\n    if (writeLen > this.blockRemain) {\n      throw new Error('writing more to entry than is appropriate')\n    }\n    this.blockRemain -= writeLen\n    return super.write(chunk, cb)\n  }\n\n  end(cb?: () => void): this\n  end(chunk: Buffer | string, cb?: () => void): this\n  end(\n    chunk: Buffer | string,\n    encoding?: BufferEncoding,\n    cb?: () => void,\n  ): this\n  end(\n    chunk?: Buffer | string | (() => void),\n    encoding?: BufferEncoding | (() => void),\n    cb?: () => void,\n  ): this {\n    if (this.blockRemain) {\n      super.write(Buffer.alloc(this.blockRemain))\n    }\n    /* c8 ignore start - just junk to comply with NodeJS.WritableStream */\n    if (typeof chunk === 'function') {\n      cb = chunk\n      encoding = undefined\n      chunk = undefined\n    }\n    if (typeof encoding === 'function') {\n      cb = encoding\n      encoding = undefined\n    }\n    if (typeof chunk === 'string') {\n      chunk = Buffer.from(chunk, encoding ?? 'utf8')\n    }\n    if (cb) this.once('finish', cb)\n    chunk ? super.end(chunk, cb) : super.end(cb)\n    /* c8 ignore stop */\n    return this\n  }\n}\n\nconst getType = (stat: Stats): EntryTypeName | 'Unsupported' =>\n  stat.isFile() ? 'File'\n  : stat.isDirectory() ? 'Directory'\n  : stat.isSymbolicLink() ? 'SymbolicLink'\n  : 'Unsupported'\n", "export class Yallist<T = unknown> {\n  tail?: Node<T>\n  head?: Node<T>\n  length: number = 0\n\n  static create<T = unknown>(list: Iterable<T> = []) {\n    return new Yallist(list)\n  }\n\n  constructor(list: Iterable<T> = []) {\n    for (const item of list) {\n      this.push(item)\n    }\n  }\n\n  *[Symbol.iterator]() {\n    for (let walker = this.head; walker; walker = walker.next) {\n      yield walker.value\n    }\n  }\n\n  removeNode(node: Node<T>) {\n    if (node.list !== this) {\n      throw new Error(\n        'removing node which does not belong to this list',\n      )\n    }\n\n    const next = node.next\n    const prev = node.prev\n\n    if (next) {\n      next.prev = prev\n    }\n\n    if (prev) {\n      prev.next = next\n    }\n\n    if (node === this.head) {\n      this.head = next\n    }\n    if (node === this.tail) {\n      this.tail = prev\n    }\n\n    this.length--\n    node.next = undefined\n    node.prev = undefined\n    node.list = undefined\n\n    return next\n  }\n\n  unshiftNode(node: Node<T>) {\n    if (node === this.head) {\n      return\n    }\n\n    if (node.list) {\n      node.list.removeNode(node)\n    }\n\n    const head = this.head\n    node.list = this\n    node.next = head\n    if (head) {\n      head.prev = node\n    }\n\n    this.head = node\n    if (!this.tail) {\n      this.tail = node\n    }\n    this.length++\n  }\n\n  pushNode(node: Node<T>) {\n    if (node === this.tail) {\n      return\n    }\n\n    if (node.list) {\n      node.list.removeNode(node)\n    }\n\n    const tail = this.tail\n    node.list = this\n    node.prev = tail\n    if (tail) {\n      tail.next = node\n    }\n\n    this.tail = node\n    if (!this.head) {\n      this.head = node\n    }\n    this.length++\n  }\n\n  push(...args: T[]) {\n    for (let i = 0, l = args.length; i < l; i++) {\n      push(this, args[i])\n    }\n    return this.length\n  }\n\n  unshift(...args: T[]) {\n    for (var i = 0, l = args.length; i < l; i++) {\n      unshift(this, args[i])\n    }\n    return this.length\n  }\n\n  pop() {\n    if (!this.tail) {\n      return undefined\n    }\n\n    const res = this.tail.value\n    const t = this.tail\n    this.tail = this.tail.prev\n    if (this.tail) {\n      this.tail.next = undefined\n    } else {\n      this.head = undefined\n    }\n    t.list = undefined\n    this.length--\n    return res\n  }\n\n  shift() {\n    if (!this.head) {\n      return undefined\n    }\n\n    const res = this.head.value\n    const h = this.head\n    this.head = this.head.next\n    if (this.head) {\n      this.head.prev = undefined\n    } else {\n      this.tail = undefined\n    }\n    h.list = undefined\n    this.length--\n    return res\n  }\n\n  forEach(\n    fn: (value: T, i: number, list: Yallist<T>) => any,\n    thisp?: any,\n  ) {\n    thisp = thisp || this\n    for (let walker = this.head, i = 0; !!walker; i++) {\n      fn.call(thisp, walker.value, i, this)\n      walker = walker.next\n    }\n  }\n\n  forEachReverse(\n    fn: (value: T, i: number, list: Yallist<T>) => any,\n    thisp?: any,\n  ) {\n    thisp = thisp || this\n    for (let walker = this.tail, i = this.length - 1; !!walker; i--) {\n      fn.call(thisp, walker.value, i, this)\n      walker = walker.prev\n    }\n  }\n\n  get(n: number) {\n    let i = 0\n    let walker = this.head\n    for (; !!walker && i < n; i++) {\n      walker = walker.next\n    }\n    if (i === n && !!walker) {\n      return walker.value\n    }\n  }\n\n  getReverse(n: number) {\n    let i = 0\n    let walker = this.tail\n    for (; !!walker && i < n; i++) {\n      // abort out of the list early if we hit a cycle\n      walker = walker.prev\n    }\n    if (i === n && !!walker) {\n      return walker.value\n    }\n  }\n\n  map<R = any>(\n    fn: (value: T, list: Yallist<T>) => R,\n    thisp?: any,\n  ): Yallist<R> {\n    thisp = thisp || this\n    const res = new Yallist<R>()\n    for (let walker = this.head; !!walker; ) {\n      res.push(fn.call(thisp, walker.value, this))\n      walker = walker.next\n    }\n    return res\n  }\n\n  mapReverse<R = any>(\n    fn: (value: T, list: Yallist<T>) => R,\n    thisp?: any,\n  ): Yallist<R> {\n    thisp = thisp || this\n    var res = new Yallist<R>()\n    for (let walker = this.tail; !!walker; ) {\n      res.push(fn.call(thisp, walker.value, this))\n      walker = walker.prev\n    }\n    return res\n  }\n\n  reduce(fn: (left: T, right: T, i: number) => T): T\n  reduce<R = any>(\n    fn: (acc: R, next: T, i: number) => R,\n    initial: R,\n  ): R\n  reduce<R = any>(\n    fn: (acc: R, next: T, i: number) => R,\n    initial?: R,\n  ): R {\n    let acc: R | T\n    let walker = this.head\n    if (arguments.length > 1) {\n      acc = initial as R\n    } else if (this.head) {\n      walker = this.head.next\n      acc = this.head.value\n    } else {\n      throw new TypeError(\n        'Reduce of empty list with no initial value',\n      )\n    }\n\n    for (var i = 0; !!walker; i++) {\n      acc = fn(acc as R, walker.value, i)\n      walker = walker.next\n    }\n\n    return acc as R\n  }\n\n  reduceReverse(fn: (left: T, right: T, i: number) => T): T\n  reduceReverse<R = any>(\n    fn: (acc: R, next: T, i: number) => R,\n    initial: R,\n  ): R\n  reduceReverse<R = any>(\n    fn: (acc: R, next: T, i: number) => R,\n    initial?: R,\n  ): R {\n    let acc: R | T\n    let walker = this.tail\n    if (arguments.length > 1) {\n      acc = initial as R\n    } else if (this.tail) {\n      walker = this.tail.prev\n      acc = this.tail.value\n    } else {\n      throw new TypeError(\n        'Reduce of empty list with no initial value',\n      )\n    }\n\n    for (let i = this.length - 1; !!walker; i--) {\n      acc = fn(acc as R, walker.value, i)\n      walker = walker.prev\n    }\n\n    return acc as R\n  }\n\n  toArray() {\n    const arr = new Array(this.length)\n    for (let i = 0, walker = this.head; !!walker; i++) {\n      arr[i] = walker.value\n      walker = walker.next\n    }\n    return arr\n  }\n\n  toArrayReverse() {\n    const arr = new Array(this.length)\n    for (let i = 0, walker = this.tail; !!walker; i++) {\n      arr[i] = walker.value\n      walker = walker.prev\n    }\n    return arr\n  }\n\n  slice(from: number = 0, to: number = this.length) {\n    if (to < 0) {\n      to += this.length\n    }\n    if (from < 0) {\n      from += this.length\n    }\n    const ret = new Yallist()\n    if (to < from || to < 0) {\n      return ret\n    }\n    if (from < 0) {\n      from = 0\n    }\n    if (to > this.length) {\n      to = this.length\n    }\n    let walker = this.head\n    let i = 0\n    for (i = 0; !!walker && i < from; i++) {\n      walker = walker.next\n    }\n    for (; !!walker && i < to; i++, walker = walker.next) {\n      ret.push(walker.value)\n    }\n    return ret\n  }\n\n  sliceReverse(from: number = 0, to: number = this.length) {\n    if (to < 0) {\n      to += this.length\n    }\n    if (from < 0) {\n      from += this.length\n    }\n    const ret = new Yallist()\n    if (to < from || to < 0) {\n      return ret\n    }\n    if (from < 0) {\n      from = 0\n    }\n    if (to > this.length) {\n      to = this.length\n    }\n    let i = this.length\n    let walker = this.tail\n    for (; !!walker && i > to; i--) {\n      walker = walker.prev\n    }\n    for (; !!walker && i > from; i--, walker = walker.prev) {\n      ret.push(walker.value)\n    }\n    return ret\n  }\n\n  splice(start: number, deleteCount: number = 0, ...nodes: T[]) {\n    if (start > this.length) {\n      start = this.length - 1\n    }\n    if (start < 0) {\n      start = this.length + start\n    }\n\n    let walker = this.head\n\n    for (let i = 0; !!walker && i < start; i++) {\n      walker = walker.next\n    }\n\n    const ret: T[] = []\n    for (let i = 0; !!walker && i < deleteCount; i++) {\n      ret.push(walker.value)\n      walker = this.removeNode(walker)\n    }\n    if (!walker) {\n      walker = this.tail\n    } else if (walker !== this.tail) {\n      walker = walker.prev\n    }\n\n    for (const v of nodes) {\n      walker = insertAfter<T>(this, walker, v)\n    }\n\n    return ret\n  }\n\n  reverse() {\n    const head = this.head\n    const tail = this.tail\n    for (let walker = head; !!walker; walker = walker.prev) {\n      const p = walker.prev\n      walker.prev = walker.next\n      walker.next = p\n    }\n    this.head = tail\n    this.tail = head\n    return this\n  }\n}\n\n// insertAfter undefined means \"make the node the new head of list\"\nfunction insertAfter<T>(\n  self: Yallist<T>,\n  node: Node<T> | undefined,\n  value: T,\n) {\n  const prev = node\n  const next = node ? node.next : self.head\n  const inserted = new Node<T>(value, prev, next, self)\n\n  if (inserted.next === undefined) {\n    self.tail = inserted\n  }\n  if (inserted.prev === undefined) {\n    self.head = inserted\n  }\n\n  self.length++\n\n  return inserted\n}\n\nfunction push<T>(self: Yallist<T>, item: T) {\n  self.tail = new Node<T>(item, self.tail, undefined, self)\n  if (!self.head) {\n    self.head = self.tail\n  }\n  self.length++\n}\n\nfunction unshift<T>(self: Yallist<T>, item: T) {\n  self.head = new Node<T>(item, undefined, self.head, self)\n  if (!self.tail) {\n    self.tail = self.head\n  }\n  self.length++\n}\n\nexport class Node<T = unknown> {\n  list?: Yallist<T>\n  next?: Node<T>\n  prev?: Node<T>\n  value: T\n\n  constructor(\n    value: T,\n    prev?: Node<T> | undefined,\n    next?: Node<T> | undefined,\n    list?: Yallist<T> | undefined,\n  ) {\n    this.list = list\n    this.value = value\n\n    if (prev) {\n      prev.next = this\n      this.prev = prev\n    } else {\n      this.prev = undefined\n    }\n\n    if (next) {\n      next.prev = this\n      this.next = next\n    } else {\n      this.next = undefined\n    }\n  }\n}\n", "// A readable tar stream creator\n// Technically, this is a transform stream that you write paths into,\n// and tar format comes out of.\n// The `add()` method is like `write()` but returns this,\n// and end() return `this` as well, so you can\n// do `new Pack(opt).add('files').add('dir').end().pipe(output)\n// You could also do something like:\n// streamOfPaths().pipe(new Pack()).pipe(new fs.WriteStream('out.tar'))\n\nimport fs, { type Stats } from 'fs'\nimport {\n  WriteEntry,\n  WriteEntrySync,\n  WriteEntryTar,\n} from './write-entry.js'\n\nexport class PackJob {\n  path: string\n  absolute: string\n  entry?: WriteEntry | WriteEntryTar\n  stat?: Stats\n  readdir?: string[]\n  pending: boolean = false\n  ignore: boolean = false\n  piped: boolean = false\n  constructor(path: string, absolute: string) {\n    this.path = path || './'\n    this.absolute = absolute\n  }\n}\n\nimport { Minipass } from 'minipass'\nimport * as zlib from 'minizlib'\nimport { Yallist } from 'yallist'\nimport { ReadEntry } from './read-entry.js'\nimport {\n  WarnEvent,\n  warnMethod,\n  type WarnData,\n  type Warner,\n} from './warn-method.js'\n\nconst EOF = Buffer.alloc(1024)\nconst ONSTAT = Symbol('onStat')\nconst ENDED = Symbol('ended')\nconst QUEUE = Symbol('queue')\nconst CURRENT = Symbol('current')\nconst PROCESS = Symbol('process')\nconst PROCESSING = Symbol('processing')\nconst PROCESSJOB = Symbol('processJob')\nconst JOBS = Symbol('jobs')\nconst JOBDONE = Symbol('jobDone')\nconst ADDFSENTRY = Symbol('addFSEntry')\nconst ADDTARENTRY = Symbol('addTarEntry')\nconst STAT = Symbol('stat')\nconst READDIR = Symbol('readdir')\nconst ONREADDIR = Symbol('onreaddir')\nconst PIPE = Symbol('pipe')\nconst ENTRY = Symbol('entry')\nconst ENTRYOPT = Symbol('entryOpt')\nconst WRITEENTRYCLASS = Symbol('writeEntryClass')\nconst WRITE = Symbol('write')\nconst ONDRAIN = Symbol('ondrain')\n\nimport path from 'path'\nimport { normalizeWindowsPath } from './normalize-windows-path.js'\nimport { TarOptions } from './options.js'\n\nexport class Pack\n  extends Minipass<Buffer, ReadEntry | string, WarnEvent<Buffer>>\n  implements Warner\n{\n  sync: boolean = false\n  opt: TarOptions\n  cwd: string\n  maxReadSize?: number\n  preservePaths: boolean\n  strict: boolean\n  noPax: boolean\n  prefix: string\n  linkCache: Exclude<TarOptions['linkCache'], undefined>\n  statCache: Exclude<TarOptions['statCache'], undefined>\n  file: string\n  portable: boolean\n  zip?: zlib.BrotliCompress | zlib.Gzip | zlib.ZstdCompress\n  readdirCache: Exclude<TarOptions['readdirCache'], undefined>\n  noDirRecurse: boolean\n  follow: boolean\n  noMtime: boolean\n  mtime?: Date\n  filter: Exclude<TarOptions['filter'], undefined>\n  jobs: number;\n\n  [WRITEENTRYCLASS]: typeof WriteEntry | typeof WriteEntrySync\n  onWriteEntry?: (entry: WriteEntry) => void;\n  // Note: we actually DO need a linked list here, because we\n  // shift() to update the head of the list where we start, but still\n  // while that happens, need to know what the next item in the queue\n  // will be. Since we do multiple jobs in parallel, it's not as simple\n  // as just an Array.shift(), since that would lose the information about\n  // the next job in the list. We could add a .next field on the PackJob\n  // class, but then we'd have to be tracking the tail of the queue the\n  // whole time, and Yallist just does that for us anyway.\n  [QUEUE]: Yallist<PackJob>;\n  [JOBS]: number = 0;\n  [PROCESSING]: boolean = false;\n  [ENDED]: boolean = false\n\n  constructor(opt: TarOptions = {}) {\n    //@ts-ignore\n    super()\n    this.opt = opt\n    this.file = opt.file || ''\n    this.cwd = opt.cwd || process.cwd()\n    this.maxReadSize = opt.maxReadSize\n    this.preservePaths = !!opt.preservePaths\n    this.strict = !!opt.strict\n    this.noPax = !!opt.noPax\n    this.prefix = normalizeWindowsPath(opt.prefix || '')\n    this.linkCache = opt.linkCache || new Map()\n    this.statCache = opt.statCache || new Map()\n    this.readdirCache = opt.readdirCache || new Map()\n    this.onWriteEntry = opt.onWriteEntry\n\n    this[WRITEENTRYCLASS] = WriteEntry\n    if (typeof opt.onwarn === 'function') {\n      this.on('warn', opt.onwarn)\n    }\n\n    this.portable = !!opt.portable\n\n    if (opt.gzip || opt.brotli || opt.zstd) {\n      if (\n        (opt.gzip ? 1 : 0) +\n          (opt.brotli ? 1 : 0) +\n          (opt.zstd ? 1 : 0) >\n        1\n      ) {\n        throw new TypeError(\n          'gzip, brotli, zstd are mutually exclusive',\n        )\n      }\n      if (opt.gzip) {\n        if (typeof opt.gzip !== 'object') {\n          opt.gzip = {}\n        }\n        if (this.portable) {\n          opt.gzip.portable = true\n        }\n        this.zip = new zlib.Gzip(opt.gzip)\n      }\n      if (opt.brotli) {\n        if (typeof opt.brotli !== 'object') {\n          opt.brotli = {}\n        }\n        this.zip = new zlib.BrotliCompress(opt.brotli)\n      }\n      if (opt.zstd) {\n        if (typeof opt.zstd !== 'object') {\n          opt.zstd = {}\n        }\n        this.zip = new zlib.ZstdCompress(opt.zstd)\n      }\n      /* c8 ignore next */\n      if (!this.zip) throw new Error('impossible')\n      const zip = this.zip\n      zip.on('data', chunk => super.write(chunk as unknown as string))\n      zip.on('end', () => super.end())\n      zip.on('drain', () => this[ONDRAIN]())\n      this.on('resume', () => zip.resume())\n    } else {\n      this.on('drain', this[ONDRAIN])\n    }\n\n    this.noDirRecurse = !!opt.noDirRecurse\n    this.follow = !!opt.follow\n    this.noMtime = !!opt.noMtime\n    if (opt.mtime) this.mtime = opt.mtime\n\n    this.filter =\n      typeof opt.filter === 'function' ? opt.filter : () => true\n\n    this[QUEUE] = new Yallist<PackJob>()\n    this[JOBS] = 0\n    this.jobs = Number(opt.jobs) || 4\n    this[PROCESSING] = false\n    this[ENDED] = false\n  }\n\n  [WRITE](chunk: Buffer) {\n    return super.write(chunk as unknown as string)\n  }\n\n  add(path: string | ReadEntry) {\n    this.write(path)\n    return this\n  }\n\n  end(cb?: () => void): this\n  end(path: string | ReadEntry, cb?: () => void): this\n  end(\n    path: string | ReadEntry,\n    encoding?: Minipass.Encoding,\n    cb?: () => void,\n  ): this\n  end(\n    path?: string | ReadEntry | (() => void),\n    encoding?: Minipass.Encoding | (() => void),\n    cb?: () => void,\n  ) {\n    /* c8 ignore start */\n    if (typeof path === 'function') {\n      cb = path\n      path = undefined\n    }\n    if (typeof encoding === 'function') {\n      cb = encoding\n      encoding = undefined\n    }\n    /* c8 ignore stop */\n    if (path) {\n      this.add(path)\n    }\n    this[ENDED] = true\n    this[PROCESS]()\n    /* c8 ignore next */\n    if (cb) cb()\n    return this\n  }\n\n  write(path: string | ReadEntry) {\n    if (this[ENDED]) {\n      throw new Error('write after end')\n    }\n\n    if (path instanceof ReadEntry) {\n      this[ADDTARENTRY](path)\n    } else {\n      this[ADDFSENTRY](path)\n    }\n    return this.flowing\n  }\n\n  [ADDTARENTRY](p: ReadEntry) {\n    const absolute = normalizeWindowsPath(\n      path.resolve(this.cwd, p.path),\n    )\n    // in this case, we don't have to wait for the stat\n    if (!this.filter(p.path, p)) {\n      p.resume()\n    } else {\n      const job = new PackJob(p.path, absolute)\n      job.entry = new WriteEntryTar(p, this[ENTRYOPT](job))\n      job.entry.on('end', () => this[JOBDONE](job))\n      this[JOBS] += 1\n      this[QUEUE].push(job)\n    }\n\n    this[PROCESS]()\n  }\n\n  [ADDFSENTRY](p: string) {\n    const absolute = normalizeWindowsPath(path.resolve(this.cwd, p))\n    this[QUEUE].push(new PackJob(p, absolute))\n    this[PROCESS]()\n  }\n\n  [STAT](job: PackJob) {\n    job.pending = true\n    this[JOBS] += 1\n    const stat = this.follow ? 'stat' : 'lstat'\n    fs[stat](job.absolute, (er, stat) => {\n      job.pending = false\n      this[JOBS] -= 1\n      if (er) {\n        this.emit('error', er)\n      } else {\n        this[ONSTAT](job, stat)\n      }\n    })\n  }\n\n  [ONSTAT](job: PackJob, stat: Stats) {\n    this.statCache.set(job.absolute, stat)\n    job.stat = stat\n\n    // now we have the stat, we can filter it.\n    if (!this.filter(job.path, stat)) {\n      job.ignore = true\n    } else if (\n      stat.isFile() &&\n      stat.nlink > 1 &&\n      job === this[CURRENT] &&\n      !this.linkCache.get(`${stat.dev}:${stat.ino}`) &&\n      !this.sync\n    ) {\n      // if it's not filtered, and it's a new File entry,\n      // jump the queue in case any pending Link entries are about\n      // to try to link to it. This prevents a hardlink from coming ahead\n      // of its target in the archive.\n      this[PROCESSJOB](job)\n    }\n\n    this[PROCESS]()\n  }\n\n  [READDIR](job: PackJob) {\n    job.pending = true\n    this[JOBS] += 1\n    fs.readdir(job.absolute, (er, entries) => {\n      job.pending = false\n      this[JOBS] -= 1\n      if (er) {\n        return this.emit('error', er)\n      }\n      this[ONREADDIR](job, entries)\n    })\n  }\n\n  [ONREADDIR](job: PackJob, entries: string[]) {\n    this.readdirCache.set(job.absolute, entries)\n    job.readdir = entries\n    this[PROCESS]()\n  }\n\n  [PROCESS]() {\n    if (this[PROCESSING]) {\n      return\n    }\n\n    this[PROCESSING] = true\n    for (\n      let w = this[QUEUE].head;\n      !!w && this[JOBS] < this.jobs;\n      w = w.next\n    ) {\n      this[PROCESSJOB](w.value)\n      if (w.value.ignore) {\n        const p = w.next\n        this[QUEUE].removeNode(w)\n        w.next = p\n      }\n    }\n\n    this[PROCESSING] = false\n\n    if (this[ENDED] && !this[QUEUE].length && this[JOBS] === 0) {\n      if (this.zip) {\n        this.zip.end(EOF)\n      } else {\n        super.write(EOF as unknown as string)\n        super.end()\n      }\n    }\n  }\n\n  get [CURRENT]() {\n    return this[QUEUE] && this[QUEUE].head && this[QUEUE].head.value\n  }\n\n  [JOBDONE](_job: PackJob) {\n    this[QUEUE].shift()\n    this[JOBS] -= 1\n    this[PROCESS]()\n  }\n\n  [PROCESSJOB](job: PackJob) {\n    if (job.pending) {\n      return\n    }\n\n    if (job.entry) {\n      if (job === this[CURRENT] && !job.piped) {\n        this[PIPE](job)\n      }\n      return\n    }\n\n    if (!job.stat) {\n      const sc = this.statCache.get(job.absolute)\n      if (sc) {\n        this[ONSTAT](job, sc)\n      } else {\n        this[STAT](job)\n      }\n    }\n    if (!job.stat) {\n      return\n    }\n\n    // filtered out!\n    if (job.ignore) {\n      return\n    }\n\n    if (\n      !this.noDirRecurse &&\n      job.stat.isDirectory() &&\n      !job.readdir\n    ) {\n      const rc = this.readdirCache.get(job.absolute)\n      if (rc) {\n        this[ONREADDIR](job, rc)\n      } else {\n        this[READDIR](job)\n      }\n      if (!job.readdir) {\n        return\n      }\n    }\n\n    // we know it doesn't have an entry, because that got checked above\n    job.entry = this[ENTRY](job)\n    if (!job.entry) {\n      job.ignore = true\n      return\n    }\n\n    if (job === this[CURRENT] && !job.piped) {\n      this[PIPE](job)\n    }\n  }\n\n  [ENTRYOPT](job: PackJob): TarOptions {\n    return {\n      onwarn: (code, msg, data) => this.warn(code, msg, data),\n      noPax: this.noPax,\n      cwd: this.cwd,\n      absolute: job.absolute,\n      preservePaths: this.preservePaths,\n      maxReadSize: this.maxReadSize,\n      strict: this.strict,\n      portable: this.portable,\n      linkCache: this.linkCache,\n      statCache: this.statCache,\n      noMtime: this.noMtime,\n      mtime: this.mtime,\n      prefix: this.prefix,\n      onWriteEntry: this.onWriteEntry,\n    }\n  }\n\n  [ENTRY](job: PackJob) {\n    this[JOBS] += 1\n    try {\n      const e = new this[WRITEENTRYCLASS](\n        job.path,\n        this[ENTRYOPT](job),\n      )\n      return e\n        .on('end', () => this[JOBDONE](job))\n        .on('error', er => this.emit('error', er))\n    } catch (er) {\n      this.emit('error', er)\n    }\n  }\n\n  [ONDRAIN]() {\n    if (this[CURRENT] && this[CURRENT].entry) {\n      this[CURRENT].entry.resume()\n    }\n  }\n\n  // like .pipe() but using super, because our write() is special\n  [PIPE](job: PackJob) {\n    job.piped = true\n\n    if (job.readdir) {\n      job.readdir.forEach(entry => {\n        const p = job.path\n        const base = p === './' ? '' : p.replace(/\\/*$/, '/')\n        this[ADDFSENTRY](base + entry)\n      })\n    }\n\n    const source = job.entry\n    const zip = this.zip\n    /* c8 ignore start */\n    if (!source) throw new Error('cannot pipe without source')\n    /* c8 ignore stop */\n\n    if (zip) {\n      source.on('data', chunk => {\n        if (!zip.write(chunk)) {\n          source.pause()\n        }\n      })\n    } else {\n      source.on('data', chunk => {\n        if (!super.write(chunk as unknown as string)) {\n          source.pause()\n        }\n      })\n    }\n  }\n\n  pause() {\n    if (this.zip) {\n      this.zip.pause()\n    }\n    return super.pause()\n  }\n  warn(\n    code: string,\n    message: string | Error,\n    data: WarnData = {},\n  ): void {\n    warnMethod(this, code, message, data)\n  }\n}\n\nexport class PackSync extends Pack {\n  sync: true = true\n  constructor(opt: TarOptions) {\n    super(opt)\n    this[WRITEENTRYCLASS] = WriteEntrySync\n  }\n\n  // pause/resume are no-ops in sync streams.\n  pause() {}\n  resume() {}\n\n  [STAT](job: PackJob) {\n    const stat = this.follow ? 'statSync' : 'lstatSync'\n    this[ONSTAT](job, fs[stat](job.absolute))\n  }\n\n  [READDIR](job: PackJob) {\n    this[ONREADDIR](job, fs.readdirSync(job.absolute))\n  }\n\n  // gotta get it all in this tick\n  [PIPE](job: PackJob) {\n    const source = job.entry\n    const zip = this.zip\n\n    if (job.readdir) {\n      job.readdir.forEach(entry => {\n        const p = job.path\n        const base = p === './' ? '' : p.replace(/\\/*$/, '/')\n        this[ADDFSENTRY](base + entry)\n      })\n    }\n\n    /* c8 ignore start */\n    if (!source) throw new Error('Cannot pipe without source')\n    /* c8 ignore stop */\n\n    if (zip) {\n      source.on('data', chunk => {\n        zip.write(chunk)\n      })\n    } else {\n      source.on('data', chunk => {\n        super[WRITE](chunk)\n      })\n    }\n  }\n}\n", "import { WriteStream, WriteStreamSync } from '@isaacs/fs-minipass'\nimport { Minipass } from 'minipass'\nimport path from 'node:path'\nimport { list } from './list.js'\nimport { makeCommand } from './make-command.js'\nimport {\n  TarOptions,\n  TarOptionsFile,\n  TarOptionsSync,\n  TarOptionsSyncFile,\n} from './options.js'\nimport { Pack, PackSync } from './pack.js'\n\nconst createFileSync = (opt: TarOptionsSyncFile, files: string[]) => {\n  const p = new PackSync(opt)\n  const stream = new WriteStreamSync(opt.file, {\n    mode: opt.mode || 0o666,\n  })\n  p.pipe(stream as unknown as Minipass.Writable)\n  addFilesSync(p, files)\n}\n\nconst createFile = (opt: TarOptionsFile, files: string[]) => {\n  const p = new Pack(opt)\n  const stream = new WriteStream(opt.file, {\n    mode: opt.mode || 0o666,\n  })\n  p.pipe(stream as unknown as Minipass.Writable)\n\n  const promise = new Promise<void>((res, rej) => {\n    stream.on('error', rej)\n    stream.on('close', res)\n    p.on('error', rej)\n  })\n\n  addFilesAsync(p, files)\n\n  return promise\n}\n\nconst addFilesSync = (p: PackSync, files: string[]) => {\n  files.forEach(file => {\n    if (file.charAt(0) === '@') {\n      list({\n        file: path.resolve(p.cwd, file.slice(1)),\n        sync: true,\n        noResume: true,\n        onReadEntry: entry => p.add(entry),\n      })\n    } else {\n      p.add(file)\n    }\n  })\n  p.end()\n}\n\nconst addFilesAsync = async (\n  p: Pack,\n  files: string[],\n): Promise<void> => {\n  for (let i = 0; i < files.length; i++) {\n    const file = String(files[i])\n    if (file.charAt(0) === '@') {\n      await list({\n        file: path.resolve(String(p.cwd), file.slice(1)),\n        noResume: true,\n        onReadEntry: entry => {\n          p.add(entry)\n        },\n      })\n    } else {\n      p.add(file)\n    }\n  }\n  p.end()\n}\n\nconst createSync = (opt: TarOptionsSync, files: string[]) => {\n  const p = new PackSync(opt)\n  addFilesSync(p, files)\n  return p\n}\n\nconst createAsync = (opt: TarOptions, files: string[]) => {\n  const p = new Pack(opt)\n  addFilesAsync(p, files)\n  return p\n}\n\nexport const create = makeCommand(\n  createFileSync,\n  createFile,\n  createSync,\n  createAsync,\n  (_opt, files) => {\n    if (!files?.length) {\n      throw new TypeError('no paths specified to add to archive')\n    }\n  },\n)\n", "// Get the appropriate flag to use for creating files\n// We use fmap on Windows platforms for files less than\n// 512kb.  This is a fairly low limit, but avoids making\n// things slower in some cases.  Since most of what this\n// library is used for is extracting tarballs of many\n// relatively small files in npm packages and the like,\n// it can be a big boost on Windows platforms.\n\nimport fs from 'fs'\n\nconst platform = process.env.__FAKE_PLATFORM__ || process.platform\nconst isWindows = platform === 'win32'\n\n/* c8 ignore start */\nconst { O_CREAT, O_TRUNC, O_WRONLY } = fs.constants\nconst UV_FS_O_FILEMAP =\n  Number(process.env.__FAKE_FS_O_FILENAME__) ||\n  fs.constants.UV_FS_O_FILEMAP ||\n  0\n/* c8 ignore stop */\n\nconst fMapEnabled = isWindows && !!UV_FS_O_FILEMAP\nconst fMapLimit = 512 * 1024\nconst fMapFlag = UV_FS_O_FILEMAP | O_TRUNC | O_CREAT | O_WRONLY\nexport const getWriteFlag =\n  !fMapEnabled ?\n    () => 'w'\n  : (size: number) => (size < fMapLimit ? fMapFlag : 'w')\n", "import fs, { type Dirent } from 'node:fs'\nimport path from 'node:path'\n\nconst lchownSync = (path: string, uid: number, gid: number) => {\n  try {\n    return fs.lchownSync(path, uid, gid)\n  } catch (er) {\n    if ((er as NodeJS.ErrnoException)?.code !== 'ENOENT') throw er\n  }\n}\n\nconst chown = (\n  cpath: string,\n  uid: number,\n  gid: number,\n  cb: (er?: unknown) => any,\n) => {\n  fs.lchown(cpath, uid, gid, er => {\n    // Skip ENOENT error\n    cb(er && (er as NodeJS.ErrnoException)?.code !== 'ENOENT' ? er : null)\n  })\n}\n\nconst chownrKid = (\n  p: string,\n  child: Dirent,\n  uid: number,\n  gid: number,\n  cb: (er?: unknown) => any,\n) => {\n  if (child.isDirectory()) {\n    chownr(path.resolve(p, child.name), uid, gid, (er: unknown) => {\n      if (er) return cb(er)\n      const cpath = path.resolve(p, child.name)\n      chown(cpath, uid, gid, cb)\n    })\n  } else {\n    const cpath = path.resolve(p, child.name)\n    chown(cpath, uid, gid, cb)\n  }\n}\n\nexport const chownr = (\n  p: string,\n  uid: number,\n  gid: number,\n  cb: (er?: unknown) => any,\n) => {\n  fs.readdir(p, { withFileTypes: true }, (er, children) => {\n    // any error other than ENOTDIR or ENOTSUP means it's not readable,\n    // or doesn't exist.  give up.\n    if (er) {\n      if (er.code === 'ENOENT') return cb()\n      else if (er.code !== 'ENOTDIR' && er.code !== 'ENOTSUP')\n        return cb(er)\n    }\n    if (er || !children.length) return chown(p, uid, gid, cb)\n\n    let len = children.length\n    let errState: null | NodeJS.ErrnoException = null\n    const then = (er?: unknown) => {\n      /* c8 ignore start */\n      if (errState) return\n      /* c8 ignore stop */\n      if (er) return cb((errState = er as NodeJS.ErrnoException))\n      if (--len === 0) return chown(p, uid, gid, cb)\n    }\n\n    for (const child of children) {\n      chownrKid(p, child, uid, gid, then)\n    }\n  })\n}\n\nconst chownrKidSync = (\n  p: string,\n  child: Dirent,\n  uid: number,\n  gid: number,\n) => {\n  if (child.isDirectory())\n    chownrSync(path.resolve(p, child.name), uid, gid)\n\n  lchownSync(path.resolve(p, child.name), uid, gid)\n}\n\nexport const chownrSync = (p: string, uid: number, gid: number) => {\n  let children: Dirent[]\n  try {\n    children = fs.readdirSync(p, { withFileTypes: true })\n  } catch (er) {\n    const e = er as NodeJS.ErrnoException\n    if (e?.code === 'ENOENT') return\n    else if (e?.code === 'ENOTDIR' || e?.code === 'ENOTSUP')\n      return lchownSync(p, uid, gid)\n    else throw e\n  }\n\n  for (const child of children) {\n    chownrKidSync(p, child, uid, gid)\n  }\n\n  return lchownSync(p, uid, gid)\n}\n", "export class CwdError extends Error {\n  path: string\n  code: string\n  syscall: 'chdir' = 'chdir'\n\n  constructor(path: string, code: string) {\n    super(`${code}: Cannot cd into '${path}'`)\n    this.path = path\n    this.code = code\n  }\n\n  get name() {\n    return 'CwdError'\n  }\n}\n", "export class SymlinkError extends Error {\n  path: string\n  symlink: string\n  syscall: 'symlink' = 'symlink'\n  code: 'TAR_SYMLINK_ERROR' = 'TAR_SYMLINK_ERROR'\n  constructor(symlink: string, path: string) {\n    super('TAR_SYMLINK_ERROR: Cannot extract through symbolic link')\n    this.symlink = symlink\n    this.path = path\n  }\n  get name() {\n    return 'SymlinkError'\n  }\n}\n", "import { chownr, chownrSync } from 'chownr'\nimport fs from 'node:fs'\nimport fsp from 'node:fs/promises'\nimport path from 'node:path'\nimport { CwdError } from './cwd-error.js'\nimport { normalizeWindowsPath } from './normalize-windows-path.js'\nimport { SymlinkError } from './symlink-error.js'\n\nexport type MkdirOptions = {\n  uid?: number\n  gid?: number\n  processUid?: number\n  processGid?: number\n  umask?: number\n  preserve: boolean\n  unlink: boolean\n  cwd: string\n  mode: number\n}\n\nexport type MkdirError =\n  | NodeJS.ErrnoException\n  | CwdError\n  | SymlinkError\n\nconst checkCwd = (\n  dir: string,\n  cb: (er?: null | MkdirError) => any,\n) => {\n  fs.stat(dir, (er, st) => {\n    if (er || !st.isDirectory()) {\n      er = new CwdError(\n        dir,\n        (er as NodeJS.ErrnoException)?.code || 'ENOTDIR',\n      )\n    }\n    cb(er)\n  })\n}\n\n/**\n * Wrapper around fs/promises.mkdir for tar's needs.\n *\n * The main purpose is to avoid creating directories if we know that\n * they already exist (and track which ones exist for this purpose),\n * and prevent entries from being extracted into symlinked folders,\n * if `preservePaths` is not set.\n */\nexport const mkdir = (\n  dir: string,\n  opt: MkdirOptions,\n  cb: (er?: null | MkdirError, made?: string) => void,\n) => {\n  dir = normalizeWindowsPath(dir)\n\n  // if there's any overlap between mask and mode,\n  // then we'll need an explicit chmod\n  /* c8 ignore next */\n  const umask = opt.umask ?? 0o22\n  const mode = opt.mode | 0o0700\n  const needChmod = (mode & umask) !== 0\n\n  const uid = opt.uid\n  const gid = opt.gid\n  const doChown =\n    typeof uid === 'number' &&\n    typeof gid === 'number' &&\n    (uid !== opt.processUid || gid !== opt.processGid)\n\n  const preserve = opt.preserve\n  const unlink = opt.unlink\n  const cwd = normalizeWindowsPath(opt.cwd)\n\n  const done = (er?: null | MkdirError, created?: string) => {\n    if (er) {\n      cb(er)\n    } else {\n      if (created && doChown) {\n        chownr(created, uid, gid, er =>\n          done(er as NodeJS.ErrnoException),\n        )\n      } else if (needChmod) {\n        fs.chmod(dir, mode, cb)\n      } else {\n        cb()\n      }\n    }\n  }\n\n  if (dir === cwd) {\n    return checkCwd(dir, done)\n  }\n\n  if (preserve) {\n    return fsp.mkdir(dir, { mode, recursive: true }).then(\n      made => done(null, made ?? undefined), // oh, ts\n      done,\n    )\n  }\n\n  const sub = normalizeWindowsPath(path.relative(